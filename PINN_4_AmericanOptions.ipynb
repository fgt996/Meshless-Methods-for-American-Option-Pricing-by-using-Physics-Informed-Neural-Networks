{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWWrd1AgaYRb"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "!pip install wandb -qqq\n",
        "!pip install pyDOE\n",
        "!pip install SobolSequence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev0OiDCqbgVw"
      },
      "source": [
        "# 1D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyK1jCBGqL0F"
      },
      "source": [
        "### Define problem, initial and boundary conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JoX0frJqL0H"
      },
      "outputs": [],
      "source": [
        "\n",
        "import time\n",
        "import wandb\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from math import pi, exp\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n_dim = 1\n",
        "\n",
        "#Define global variables\n",
        "r = 0.01\n",
        "T = 3\n",
        "K = 10\n",
        "sigma = 0.05\n",
        "DTYPE = 'float32'\n",
        "\n",
        "#Fix seeds\n",
        "random_seed = 2\n",
        "np.random.seed(random_seed)\n",
        "tf.random.set_seed(random_seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIumHTSYqL0I"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Sampler_IBC():\n",
        "    def __init__(self, lb, ub, cond=None, N_points=100,\n",
        "                 method='uniform', grid=None, split=False, DTYPE='float64'):\n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "        self.cond = cond\n",
        "        self.DTYPE = DTYPE\n",
        "        self.sample(N_points, method, grid, split)\n",
        "\n",
        "    def sample(self, N_points, method, grid, split):\n",
        "        if method == 'uniform':\n",
        "            x_ibc = np.random.uniform(0, 1, size=(N_points, self.ub.shape[0]))\n",
        "            x_ibc = self.lb + (self.ub - self.lb)*x_ibc\n",
        "        elif method == 'latin':\n",
        "            from pyDOE import lhs\n",
        "            x_ibc = self.lb + (self.ub - self.lb)*lhs(self.ub.shape[0],N_points)\n",
        "        elif method == 'sobol':\n",
        "            import sobol\n",
        "            x_ibc = sobol.sample(dimension=self.ub.shape[0], n_points=N_points)\n",
        "            x_ibc = self.lb + (self.ub - self.lb)*x_ibc\n",
        "        elif method == 'equi':\n",
        "            x_ibc = np.linspace(self.lb, self.ub, N_points)\n",
        "        elif method == 'grid':\n",
        "            x_ibc = np.linspace(self.lb, self.ub, N_points).T\n",
        "            temp_final = list()\n",
        "            for val in x_ibc[0]:\n",
        "                temp_final.append( [val] )\n",
        "            dim = 1\n",
        "            while dim < x_ibc.shape[0]:\n",
        "                temp = list()\n",
        "                for t1 in range(x_ibc.shape[1]):\n",
        "                    for t2 in range(len(temp_final)):\n",
        "                        temp_val = temp_final[t2].copy()\n",
        "                        temp_val.append( x_ibc[dim, t1] )\n",
        "                        temp.append( temp_val )\n",
        "                temp_final = temp\n",
        "                dim += 1\n",
        "            x_ibc = np.array(temp_final)\n",
        "        elif method == 'grid_old':\n",
        "            idx = np.random.choice(range(grid.shape[0]),N_points,replace=False)\n",
        "            x_ibc = grid[idx]\n",
        "        if self.cond != None:\n",
        "            y_ibc = self.cond(x_ibc)\n",
        "            self.y = tf.cast(tf.Variable(y_ibc, trainable=False ),\n",
        "                             self.DTYPE)\n",
        "        if split:\n",
        "            x_ibc, t_ibc = x_ibc[:, :-1], x_ibc[:, -1:]\n",
        "            self.t = tf.cast(tf.Variable(t_ibc, trainable=False ),\n",
        "                             self.DTYPE)\n",
        "        self.x = tf.cast(tf.Variable(x_ibc, trainable=False ),\n",
        "                         self.DTYPE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK9vZ8_2qL0J"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Define domain boundaries\n",
        "lb = np.array([0., 0.])\n",
        "ub = np.array([3.*K, T])\n",
        "\n",
        "#Define conditions\n",
        "def h_1(inp):\n",
        "    res = list()\n",
        "    for inp_val in inp:\n",
        "        res.append( K - inp_val[0] )\n",
        "    return np.array(res)\n",
        "\n",
        "def h_2(inp):\n",
        "    res = - np.ones( inp.shape[0] )\n",
        "    return res\n",
        "\n",
        "def g(inp):\n",
        "    res = np.zeros( inp.shape[0] )\n",
        "    return np.array(res)\n",
        "\n",
        "def u_0(inp):\n",
        "    res = list()\n",
        "    for inp_val in inp:\n",
        "        x, t = inp_val\n",
        "        res.append( np.max([0, K-x]) )\n",
        "    return np.array(res)\n",
        "\n",
        "def s_0(inp):\n",
        "    res = np.ones( inp.shape[0] ) * K\n",
        "    return np.array(res)\n",
        "\n",
        "#Point sampling\n",
        "N_to_sample = 300\n",
        "\n",
        "#---------------- PDE Conditions\n",
        "print('PDE COnditions\\n')\n",
        "#Initial\n",
        "init_sampler = Sampler_IBC(np.array([0., T]),\n",
        "                           np.array([3.*K, T]),\n",
        "                           u_0, N_to_sample, DTYPE=DTYPE, method='sobol' )\n",
        "print( f'x: {init_sampler.x.shape}     y: {init_sampler.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "dir_sampler = Sampler_IBC(np.array([3.*K, 0.]), np.array([3.*K, T]),\n",
        "                          g, N_to_sample, DTYPE=DTYPE, method='sobol' )\n",
        "print( f'x: {dir_sampler.x.shape}     y: {dir_sampler.y.shape}' )\n",
        "\n",
        "#Neumann\n",
        "print('No')\n",
        "\n",
        "#---------------- Free Boundary Conditions\n",
        "print('\\nFree Boundary COnditions\\n')\n",
        "#Initial\n",
        "init_fb_sampler = Sampler_IBC(np.array([T]), np.array([T]),\n",
        "                              s_0, 1, DTYPE=DTYPE )\n",
        "print( f't: {init_fb_sampler.x.shape}     y: {init_fb_sampler.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "dir_fb_sampler = Sampler_IBC(np.array([0.]), np.array([T]),\n",
        "                             None, N_to_sample, DTYPE=DTYPE, method='sobol' )\n",
        "print( f't: {dir_fb_sampler.x.shape}' )\n",
        "\n",
        "#Neumann\n",
        "neu_fb_sampler = Sampler_IBC(np.array([0.]), np.array([T]),\n",
        "                             h_2, N_to_sample, DTYPE=DTYPE, method='sobol' )\n",
        "print( f't: {neu_fb_sampler.x.shape}     y: {neu_fb_sampler.y.shape}' )\n",
        "\n",
        "#---------------- Conditions passed to PINN\n",
        "pinn_conditions = {'Initial':init_sampler,\n",
        "                   'Dirichlet':dir_sampler,\n",
        "                   'Neumann':None}\n",
        "fb_conditions = {'Initial':init_fb_sampler,\n",
        "                 'Dirichlet':dir_fb_sampler,\n",
        "                 'Neumann':neu_fb_sampler}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4yWWZnGqL0L"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_sampler = Sampler_IBC(lb, ub, cond=None, DTYPE=DTYPE,\n",
        "                           N_points=1000000, method='uniform', split=True)\n",
        "\n",
        "#Point sampling\n",
        "sample_to_test = 1000\n",
        "\n",
        "#---------------- PDE Conditions\n",
        "print('PDE COnditions\\n')\n",
        "#Initial\n",
        "init_sampler_test = Sampler_IBC(np.array([0., T]),\n",
        "                           np.array([3.*K, T]),\n",
        "                           u_0, sample_to_test, DTYPE=DTYPE, method='uniform' )\n",
        "print( f'x: {init_sampler_test.x.shape}     y: {init_sampler_test.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "dir_sampler_test = Sampler_IBC(np.array([3.*K, 0.]), np.array([3.*K, T]),\n",
        "                          g, sample_to_test, DTYPE=DTYPE, method='uniform' )\n",
        "print( f'x: {dir_sampler_test.x.shape}     y: {dir_sampler_test.y.shape}' )\n",
        "\n",
        "#Neumann\n",
        "print('No')\n",
        "\n",
        "#---------------- Free Boundary Conditions\n",
        "print('\\nFree Boundary COnditions\\n')\n",
        "#Initial\n",
        "init_fb_sampler_test = Sampler_IBC(np.array([T]), np.array([T]),\n",
        "                              s_0, 1, DTYPE=DTYPE, method='uniform' )\n",
        "print( f't: {init_fb_sampler_test.x.shape}     y: {init_fb_sampler_test.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "dir_fb_sampler_test = Sampler_IBC(np.array([0.]), np.array([T]),\n",
        "                             None, sample_to_test, DTYPE=DTYPE, method='uniform' )\n",
        "print( f't: {dir_fb_sampler_test.x.shape}' )\n",
        "\n",
        "#Neumann\n",
        "neu_fb_sampler_test = Sampler_IBC(np.array([0.]), np.array([T]),\n",
        "                             h_2, sample_to_test, DTYPE=DTYPE, method='uniform' )\n",
        "print( f't: {neu_fb_sampler_test.x.shape}     y: {neu_fb_sampler_test.y.shape}' )\n",
        "\n",
        "#---------------- Conditions passed to PINN\n",
        "pinn_cond_test = {'Initial':init_sampler_test,\n",
        "                   'Dirichlet':dir_sampler_test,\n",
        "                   'Neumann':None}\n",
        "fb_cond_test = {'Initial':init_fb_sampler_test,\n",
        "                 'Dirichlet':dir_fb_sampler_test,\n",
        "                 'Neumann':neu_fb_sampler_test}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POwYl-HJxi-D"
      },
      "source": [
        "### PINN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jgPLOUH3A9LG"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Define PDE\n",
        "def pde(tape, xs, ts, u_val, u_x):\n",
        "    u_xx = tape.gradient(u_x, xs)\n",
        "    u_t = tape.gradient(u_val, ts)\n",
        "    del(tape)\n",
        "    u_val = tf.cast(u_val, DTYPE)\n",
        "    f = (r * xs * u_x) + u_t + (sigma**2 * xs**2 * u_xx)/2 - (r * u_val)\n",
        "    return f\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q35ECL1mVJxR"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow import GradientTape as G_Tape\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.initializers import GlorotNormal, GlorotUniform\n",
        "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
        "from tensorflow.keras.metrics import mean_squared_error\n",
        "\n",
        "class FreeBoundary_PINN():\n",
        "\n",
        "    def __init__(self, params, pde, ibc_cond, ibc_fb_cond, lb, ub,\n",
        "                 N_f=10000, N_fb_ibc=150, DTYPE='float64', coll_points=None):\n",
        "        self.params = params\n",
        "        self.DTYPE = DTYPE\n",
        "        self.Default_Params()\n",
        "        #Set seed\n",
        "        if self.params['seed'] != None:\n",
        "            tf.keras.utils.set_random_seed( self.params['seed'] )\n",
        "            tf.config.experimental.enable_op_determinism()\n",
        "        #Define pde\n",
        "        self.pde = pde\n",
        "        #Define intial and boundary conditions\n",
        "        self.ibc_cond = ibc_cond\n",
        "        self.ibc_fb_cond = ibc_fb_cond\n",
        "        #All needed for points sampling\n",
        "        self.lb = tf.Variable(lb, trainable=False)\n",
        "        self.ub = tf.Variable(ub, trainable=False)\n",
        "        if coll_points == None:\n",
        "            self.N_f = N_f\n",
        "            self.Sample_Points()\n",
        "        else:\n",
        "            self.x_f = coll_points[0]\n",
        "            self.t_f = coll_points[1]\n",
        "        self.N_fb_ibc = N_fb_ibc\n",
        "        #Initialize the class: define the network\n",
        "        self.Define_Regularizer()\n",
        "        self.Define_Initializer()\n",
        "        self.Define_Optimizer()\n",
        "        self.Create_Network()\n",
        "        self.Create_FB_Network()\n",
        "\n",
        "    def Default_Params(self):\n",
        "        target = self.params.keys()\n",
        "        if 'seed' not in target:\n",
        "            self.params['seed'] = None\n",
        "        if 'optimizer' not in target:\n",
        "            self.params['optimizer'] = 'Adam'\n",
        "        if 'reg_type' not in target:\n",
        "            self.params['reg_type'] = None\n",
        "        if 'initializer' not in target:\n",
        "            self.params['initializer'] = 'glorot_normal'\n",
        "        if 'activation' not in target:\n",
        "            self.params['activation'] = 'tanh'\n",
        "        if 'output_act' not in target:\n",
        "            self.params['output_act'] = 'linear'\n",
        "        if 'pde_weight' not in target:\n",
        "            self.params['pde_weight'] = 1.\n",
        "        if 'sup_weight' not in target:\n",
        "            self.params['sup_weight'] = [1., 1., 1.]\n",
        "        if 'fb_weight' not in target:\n",
        "            self.params['fb_weight'] = [1., 1., 1.]\n",
        "        if 'patience' not in target:\n",
        "            self.params['patience'] = np.inf\n",
        "        if 'sample_method' not in target:\n",
        "            self.params['sample_method'] = 'uniform'\n",
        "        if 'fb_output_act' not in target:\n",
        "            self.params['fb_output_act'] = 'linear'\n",
        "        if 'fb_activation' not in target:\n",
        "            self.params['fb_activation'] = 'tanh'\n",
        "        if 'verbose' not in target:\n",
        "            self.params['verbose'] = 1\n",
        "        if 'steps_fb_per_pde' not in target:\n",
        "            self.params['steps_fb_per_pde'] = 1\n",
        "        if 'fb_freezing' not in target:\n",
        "            self.params['fb_freezing'] = None\n",
        "\n",
        "    def Define_Regularizer(self):\n",
        "        if self.params['reg_type'] == 'l1':\n",
        "            self.regularizer = l1( self.params['reg'] )\n",
        "        elif self.params['reg_type'] == 'l2':\n",
        "            self.regularizer = l2( self.params['reg'] )\n",
        "        elif self.params['reg_type'] == 'l1_l2':\n",
        "            self.regularizer = l1_l2( self.params['reg'][0],\n",
        "                                      self.params['reg'][1] )\n",
        "        else:\n",
        "            self.regularizer = None\n",
        "\n",
        "    def Define_Initializer(self):\n",
        "        if self.params['initializer'] == 'glorot_normal':\n",
        "            self.initializer = GlorotNormal()\n",
        "        elif self.params['initializer'] == 'glorot_uniform':\n",
        "            self.initializer = GlorotUniform()\n",
        "        else:\n",
        "            self.initializer = None\n",
        "\n",
        "    def Define_Optimizer(self):\n",
        "        temp = self.params['optimizer']\n",
        "        if temp.lower() == 'adam':\n",
        "            self.opt = Adam( self.params['lr'] )\n",
        "        elif temp.lower() == 'rmsprop':\n",
        "            self.opt = RMSprop( self.params['lr'] )\n",
        "        else:\n",
        "            raise ValueError(f\"Optimizer {temp} not recognized\")\n",
        "\n",
        "    def Create_Network(self):\n",
        "        input_layer = Input(shape=self.params['layers'][0],\n",
        "                            name = 'Input')\n",
        "        x = Dense(units=self.params['layers'][1],\n",
        "                  activation=self.params['activation'],\n",
        "                  kernel_initializer=self.initializer,\n",
        "                  kernel_regularizer=self.regularizer,\n",
        "                  name='Dense_1')(input_layer)\n",
        "        for layer in range(2, len(self.params['layers'])-1):\n",
        "            x = Dense(units=self.params['layers'][layer],\n",
        "                      activation=self.params['activation'],\n",
        "                      kernel_initializer=self.initializer,\n",
        "                      kernel_regularizer=self.regularizer,\n",
        "                      name=f'Dense_{layer}')(x)\n",
        "        output = Dense(units=self.params['layers'][-1],\n",
        "                       activation=self.params['output_act'],\n",
        "                       kernel_initializer=self.initializer,\n",
        "                       kernel_regularizer=self.regularizer,\n",
        "                       name='Output')(x)\n",
        "        self.mdl = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "    def Create_FB_Network(self):\n",
        "        input_layer = Input(shape=self.params['fb_layers'][0],\n",
        "                            name = 'Input')\n",
        "        x = Dense(units=self.params['fb_layers'][1],\n",
        "                  activation=self.params['fb_activation'],\n",
        "                  kernel_initializer=self.initializer,\n",
        "                  kernel_regularizer=self.regularizer,\n",
        "                  name='Dense_1')(input_layer)\n",
        "        for layer in range(2, len(self.params['fb_layers'])-1):\n",
        "            x = Dense(units=self.params['fb_layers'][layer],\n",
        "                      activation=self.params['fb_activation'],\n",
        "                      kernel_initializer=self.initializer,\n",
        "                      kernel_regularizer=self.regularizer,\n",
        "                      name=f'Dense_{layer}')(x)\n",
        "        output = Dense(units=self.params['fb_layers'][-1],\n",
        "                       activation=self.params['fb_output_act'],\n",
        "                       kernel_initializer=self.initializer,\n",
        "                       kernel_regularizer=self.regularizer,\n",
        "                       name='Output')(x)\n",
        "        self.fb = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "    def Sample_Points(self):\n",
        "        #According to the selected method, sample collocation points\n",
        "        method = self.params['sample_method']\n",
        "        if method == 'latin':\n",
        "            from pyDOE import lhs\n",
        "            cps = self.lb + (self.ub - self.lb)*lhs(self.ub.shape[0], self.N_f)\n",
        "        elif method == 'uniform':\n",
        "            cps = np.random.uniform(0, 1, size=(self.N_f, self.ub.shape[0]))\n",
        "            cps = self.lb + (self.ub - self.lb)*cps\n",
        "        elif method == 'sobol':\n",
        "            import sobol\n",
        "            cps = sobol.sample(dimension=self.ub.shape[0], n_points=self.N_f)\n",
        "            cps = self.lb + (self.ub - self.lb)*cps\n",
        "        else:\n",
        "            raise ValueError(f'Sampling method {method} not recognized')\n",
        "        #Return collocation points as tf tensors\n",
        "        self.x_f_total = tf.cast(tf.Variable(cps[:, :-1], trainable=False),\n",
        "                                 self.DTYPE)\n",
        "        self.t_f_total = tf.cast(tf.Variable(cps[:, -1:], trainable=False),\n",
        "                                 self.DTYPE)\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self):\n",
        "        with G_Tape(watch_accessed_variables=False) as mdl_tape:\n",
        "            #Watch solution weights\n",
        "            mdl_tape.watch(self.mdl.trainable_variables)\n",
        "            #--------------- Compute Free Boundary losses\n",
        "            with G_Tape(watch_accessed_variables=False) as fb_tape:\n",
        "                #Watch free boundary weights\n",
        "                fb_tape.watch(self.fb.trainable_variables)\n",
        "                #Compute Initial Free Boundary Condition\n",
        "                fb_init = self.fb(self.ibc_fb_cond['Initial'].x,\n",
        "                                      training=True)\n",
        "                fb_init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_init-self.ibc_fb_cond['Initial'].y)\n",
        "                    )\n",
        "                #Compute Dirichlet Free Boundary Condition\n",
        "                if self.ibc_fb_cond['Dirichlet'] != None:\n",
        "                    #Compute Free Boundary values\n",
        "                    s_values = self.fb(self.ibc_fb_cond['Dirichlet'].x,\n",
        "                                       training=True)\n",
        "                    fb_dc = self.mdl(tf.concat(\n",
        "                        [s_values,\n",
        "                         self.ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                         training=True)\n",
        "                    fb_dir_target = tf.nn.relu(\n",
        "                        tf.ones_like(\n",
        "                            s_values\n",
        "                            ) * K - s_values\n",
        "                    )\n",
        "                    fb_dir_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(fb_dc - fb_dir_target)\n",
        "                        )\n",
        "                else:\n",
        "                    fb_dir_loss = 0\n",
        "                #Compute Neumann Free Boundary Condition\n",
        "                if self.ibc_fb_cond['Neumann'] != None:\n",
        "                    s_values = self.fb(self.ibc_fb_cond['Neumann'].x,\n",
        "                                       training=True)\n",
        "                    with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                        neu_fb_tape.watch(s_values)\n",
        "                        pinn_nc_fb = self.mdl(\n",
        "                            tf.concat([s_values,\n",
        "                                       self.ibc_fb_cond['Neumann'].x],\n",
        "                                      axis=1),\n",
        "                                      training=True)\n",
        "                    pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                                      s_values)\n",
        "                    fb_neu_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(pinn_nc_fb-self.ibc_fb_cond['Neumann'].y)\n",
        "                        )\n",
        "                else:\n",
        "                    fb_neu_loss = 0\n",
        "                #Compute final loss\n",
        "                fb_loss = self.params['fb_weight'][0] * fb_init_loss +\\\n",
        "                self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "                self.params['fb_weight'][2] * fb_neu_loss\n",
        "            #Compute gradient and apply optimizers for free boundary\n",
        "            gradient_fb = fb_tape.gradient(fb_loss,\n",
        "                                          self.fb.trainable_variables)\n",
        "            self.opt.apply_gradients( zip(gradient_fb,\n",
        "                                          self.fb.trainable_variables) )\n",
        "\n",
        "            #--------------- Compute PINN losses\n",
        "            #Compute unsupervised loss\n",
        "            s_values = self.fb(tf.concat([self.t_f_total], axis=-1),\n",
        "                               training=False)\n",
        "            x_f = tf.reshape(self.x_f_total[ self.x_f_total > s_values ],\n",
        "                             (-1,1) )\n",
        "            t_f = tf.reshape(self.t_f_total[ self.x_f_total > s_values ],\n",
        "                             (-1,1) )\n",
        "            with G_Tape(persistent=True,\n",
        "                        watch_accessed_variables=False) as pinn_tape:\n",
        "                #Watch independet variables\n",
        "                pinn_tape.watch(x_f)\n",
        "                pinn_tape.watch(t_f)\n",
        "                #Apply u function for unsupervised\n",
        "                u_val = self.mdl(tf.stack([x_f, t_f], axis=1),\n",
        "                                training=True)\n",
        "                u_x = pinn_tape.gradient(u_val, x_f)\n",
        "            unsup_loss = tf.reduce_mean(tf.square(\n",
        "                self.pde(pinn_tape, x_f, t_f, u_val, u_x) ))\n",
        "            #Compute Initial Condition\n",
        "            pinn_init = self.mdl(self.ibc_cond['Initial'].x,\n",
        "                                training=True)\n",
        "            if len(pinn_init.shape) > 1:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  tf.expand_dims(self.ibc_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  self.ibc_cond['Initial'].y )\n",
        "                    )\n",
        "            #Compute Dirichlet Boundary Condition\n",
        "            if self.ibc_cond['Dirichlet'] != None:\n",
        "                pinn_dc = self.mdl(self.ibc_cond['Dirichlet'].x,\n",
        "                                    training=True)\n",
        "                dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_dc-self.ibc_cond['Dirichlet'].y)\n",
        "                    )\n",
        "            else:\n",
        "                dir_loss = 0\n",
        "            #Compute Neumann Boundary Condition\n",
        "            if self.ibc_cond['Neumann'] != None:\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_tape:\n",
        "                    neu_tape.watch(self.ibc_cond['Neumann'].x)\n",
        "                    pinn_nc = self.mdl(tf.concat([self.ibc_cond['Neumann'].x,\n",
        "                                                  self.ibc_cond['Neumann'].t],\n",
        "                                                axis=1),\n",
        "                                        training=True)\n",
        "                pinn_nc = neu_tape.gradient(pinn_nc, self.ibc_cond['Neumann'].x)\n",
        "                neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc-self.ibc_cond['Neumann'].y)\n",
        "                    )\n",
        "            else:\n",
        "                neu_loss = 0\n",
        "\n",
        "            #--------------- Compute total loss\n",
        "            pinn_loss = (self.params['pde_weight'] * unsup_loss) +\\\n",
        "            self.params['sup_weight'][0] * init_loss +\\\n",
        "            self.params['sup_weight'][1] * dir_loss +\\\n",
        "            self.params['sup_weight'][2] * neu_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "        #Compute gradient and apply optimizers\n",
        "        gradient = mdl_tape.gradient(pinn_loss,self.mdl.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(gradient,self.mdl.trainable_variables))\n",
        "        #Return losses\n",
        "        final_fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "        final_pinn_loss = unsup_loss + init_loss +\\\n",
        "         dir_loss + neu_loss + fb_dir_loss + fb_neu_loss\n",
        "        return (unsup_loss, init_loss, dir_loss, neu_loss, fb_init_loss,\n",
        "                fb_dir_loss, fb_neu_loss, final_fb_loss, final_pinn_loss)\n",
        "\n",
        "    @tf.function\n",
        "    def train_mdl_solo(self):\n",
        "        with G_Tape(watch_accessed_variables=False) as mdl_tape:\n",
        "            #Watch solution weights\n",
        "            mdl_tape.watch(self.mdl.trainable_variables)\n",
        "            #--------------- Compute Free Boundary losses\n",
        "            #Compute Initial Free Boundary Condition\n",
        "            fb_init = self.fb(self.ibc_fb_cond['Initial'].x,\n",
        "                                  training=True)\n",
        "            fb_init_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(fb_init-self.ibc_fb_cond['Initial'].y)\n",
        "                )\n",
        "            #Compute Dirichlet Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Dirichlet'] != None:\n",
        "                #Compute Free Boundary values\n",
        "                s_values = self.fb(self.ibc_fb_cond['Dirichlet'].x,\n",
        "                                    training=True)\n",
        "                fb_dc = self.mdl(tf.concat(\n",
        "                    [s_values,\n",
        "                      self.ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                      training=True)\n",
        "                fb_dir_target = tf.nn.relu(\n",
        "                    tf.ones_like(\n",
        "                        s_values\n",
        "                        ) * K - s_values\n",
        "                )\n",
        "                fb_dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_dc - fb_dir_target)\n",
        "                    )\n",
        "            else:\n",
        "                fb_dir_loss = 0\n",
        "            #Compute Neumann Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Neumann'] != None:\n",
        "                s_values = self.fb(self.ibc_fb_cond['Neumann'].x,\n",
        "                                    training=True)\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                    neu_fb_tape.watch(s_values)\n",
        "                    pinn_nc_fb = self.mdl(\n",
        "                        tf.concat([s_values,\n",
        "                                    self.ibc_fb_cond['Neumann'].x],\n",
        "                                  axis=1),\n",
        "                                  training=True)\n",
        "                pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                                  s_values)\n",
        "                fb_neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc_fb-self.ibc_fb_cond['Neumann'].y)\n",
        "                    )\n",
        "            else:\n",
        "                fb_neu_loss = 0\n",
        "            #Compute final loss\n",
        "            fb_loss = self.params['fb_weight'][0] * fb_init_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "\n",
        "            #--------------- Compute PINN losses\n",
        "            #Compute unsupervised loss\n",
        "            s_values = self.fb(tf.concat([self.t_f_total], axis=-1),\n",
        "                               training=False)\n",
        "            x_f = tf.reshape(self.x_f_total[ self.x_f_total > s_values ],\n",
        "                             (-1,1) )\n",
        "            t_f = tf.reshape(self.t_f_total[ self.x_f_total > s_values ],\n",
        "                             (-1,1) )\n",
        "            with G_Tape(persistent=True,\n",
        "                        watch_accessed_variables=False) as pinn_tape:\n",
        "                #Watch independet variables\n",
        "                pinn_tape.watch(x_f)\n",
        "                pinn_tape.watch(t_f)\n",
        "                #Apply u function for unsupervised\n",
        "                u_val = self.mdl(tf.stack([x_f, t_f], axis=1),\n",
        "                                training=True)\n",
        "                u_x = pinn_tape.gradient(u_val, x_f)\n",
        "            unsup_loss = tf.reduce_mean(tf.square(\n",
        "                self.pde(pinn_tape, x_f, t_f, u_val, u_x) ))\n",
        "            #Compute Initial Condition\n",
        "            pinn_init = self.mdl(self.ibc_cond['Initial'].x,\n",
        "                                training=True)\n",
        "            if len(pinn_init.shape) > 1:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  tf.expand_dims(self.ibc_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  self.ibc_cond['Initial'].y )\n",
        "                    )\n",
        "            #Compute Dirichlet Boundary Condition\n",
        "            if self.ibc_cond['Dirichlet'] != None:\n",
        "                pinn_dc = self.mdl(self.ibc_cond['Dirichlet'].x,\n",
        "                                    training=True)\n",
        "                dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_dc-self.ibc_cond['Dirichlet'].y)\n",
        "                    )\n",
        "            else:\n",
        "                dir_loss = 0\n",
        "            #Compute Neumann Boundary Condition\n",
        "            if self.ibc_cond['Neumann'] != None:\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_tape:\n",
        "                    neu_tape.watch(self.ibc_cond['Neumann'].x)\n",
        "                    pinn_nc = self.mdl(tf.concat([self.ibc_cond['Neumann'].x,\n",
        "                                                  self.ibc_cond['Neumann'].t],\n",
        "                                                axis=1),\n",
        "                                        training=True)\n",
        "                pinn_nc = neu_tape.gradient(pinn_nc, self.ibc_cond['Neumann'].x)\n",
        "                neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc-self.ibc_cond['Neumann'].y)\n",
        "                    )\n",
        "            else:\n",
        "                neu_loss = 0\n",
        "\n",
        "            #--------------- Compute total loss\n",
        "            pinn_loss = (self.params['pde_weight'] * unsup_loss) +\\\n",
        "            self.params['sup_weight'][0] * init_loss +\\\n",
        "            self.params['sup_weight'][1] * dir_loss +\\\n",
        "            self.params['sup_weight'][2] * neu_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "        #Compute gradient and apply optimizers\n",
        "        gradient = mdl_tape.gradient(pinn_loss,self.mdl.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(gradient,self.mdl.trainable_variables))\n",
        "        #Return losses\n",
        "        final_fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "        final_pinn_loss = unsup_loss + init_loss +\\\n",
        "         dir_loss + neu_loss + fb_dir_loss + fb_neu_loss\n",
        "        return (unsup_loss, init_loss, dir_loss, neu_loss, fb_init_loss,\n",
        "                fb_dir_loss, fb_neu_loss, final_fb_loss, final_pinn_loss)\n",
        "\n",
        "    @tf.function\n",
        "    def train_fb_solo(self):\n",
        "        #--------------- Compute Free Boundary losses\n",
        "        with G_Tape(watch_accessed_variables=False) as fb_tape:\n",
        "            #Watch free boundary weights\n",
        "            fb_tape.watch(self.fb.trainable_variables)\n",
        "            #Compute Initial Free Boundary Condition\n",
        "            fb_init = self.fb(self.ibc_fb_cond['Initial'].x,\n",
        "                                  training=True)\n",
        "            fb_init_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(fb_init-self.ibc_fb_cond['Initial'].y)\n",
        "                )\n",
        "            #Compute Dirichlet Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Dirichlet'] != None:\n",
        "                #Compute Free Boundary values\n",
        "                s_values = self.fb(self.ibc_fb_cond['Dirichlet'].x,\n",
        "                                    training=True)\n",
        "                fb_dc = self.mdl(tf.concat(\n",
        "                    [s_values,\n",
        "                      self.ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                      training=True)\n",
        "                fb_dir_target = tf.nn.relu(\n",
        "                    tf.ones_like(\n",
        "                        s_values\n",
        "                        ) * K - s_values\n",
        "                )\n",
        "                fb_dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_dc - fb_dir_target)\n",
        "                    )\n",
        "            else:\n",
        "                fb_dir_loss = 0\n",
        "            #Compute Neumann Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Neumann'] != None:\n",
        "                s_values = self.fb(self.ibc_fb_cond['Neumann'].x,\n",
        "                                    training=True)\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                    neu_fb_tape.watch(s_values)\n",
        "                    pinn_nc_fb = self.mdl(\n",
        "                        tf.concat([s_values,\n",
        "                                    self.ibc_fb_cond['Neumann'].x],\n",
        "                                  axis=1),\n",
        "                                  training=True)\n",
        "                pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                                  s_values)\n",
        "                fb_neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc_fb-self.ibc_fb_cond['Neumann'].y)\n",
        "                    )\n",
        "            else:\n",
        "                fb_neu_loss = 0\n",
        "            #Compute final loss\n",
        "            fb_loss = self.params['fb_weight'][0] * fb_init_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "        #Compute gradient and apply optimizers for free boundary\n",
        "        gradient_fb = fb_tape.gradient(fb_loss,\n",
        "                                      self.fb.trainable_variables)\n",
        "        self.opt.apply_gradients( zip(gradient_fb,\n",
        "                                      self.fb.trainable_variables) )\n",
        "        #Return losses\n",
        "        final_fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "        return fb_init_loss, fb_dir_loss, fb_neu_loss, final_fb_loss\n",
        "\n",
        "    def fit(self, wandb_run=None):\n",
        "        #Early warning initialization\n",
        "        self.early_warning = {'Target':np.inf,\n",
        "                              'n_steps':0,\n",
        "                              'top_mdl':None,\n",
        "                              'weights':None}\n",
        "        old_top_mdl, old_weights = None, None\n",
        "        #Training\n",
        "        self.u_losses, self.i_losses = list(), list()\n",
        "        self.d_losses, self.n_losses = list(), list()\n",
        "        self.b_i_losses = list()\n",
        "        self.b_d_losses, self.b_n_losses = list(), list()\n",
        "        self.b_losses, self.p_losses = list(), list()\n",
        "        print_base = \"{:<10}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}\"\n",
        "        if self.params['fb_freezing'] == None:\n",
        "            for epoch in tqdm(range(self.params['epochs']),\n",
        "                              desc='PINNs - Training'):\n",
        "                if epoch == 0:\n",
        "                    print(print_base.format('Epoch', 'Unsupervised', 'Initial',\n",
        "                                            'Dirichlet', 'Neumann',\n",
        "                                            'FB_Init', 'FB_Dir', 'FB_Neu',\n",
        "                                            'Free Boundary', 'Total'))\n",
        "                    print('\\n')\n",
        "                #Case 1: more mdl steps for a single fb step\n",
        "                for _ in range(self.params['steps_fb_per_pde'] -1):\n",
        "                    self.train_mdl_solo();\n",
        "                #Case 2: more fb steps for a single mdl step\n",
        "                for _ in range(0, self.params['steps_fb_per_pde'] +1, -1):\n",
        "                    self.train_fb_solo();\n",
        "                u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l =\\\n",
        "                self.train_step()\n",
        "                u_l, i_l = np.array(u_l), np.array(i_l)\n",
        "                d_l, n_l = np.array(d_l), np.array(n_l)\n",
        "                b_i_l = np.array(b_i_l)\n",
        "                b_d_l, b_n_l = np.array(b_d_l), np.array(b_n_l)\n",
        "                b_l, p_l = np.array(b_l), np.array(p_l)\n",
        "                if epoch % self.params['verbose'] == 0:\n",
        "                    print(print_base.format(epoch+1, format(u_l, '.20f')[:10],\n",
        "                                            format(i_l, '.20f')[:10],\n",
        "                                            format(d_l, '.20f')[:10],\n",
        "                                            format(n_l, '.20f')[:10],\n",
        "                                            format(b_i_l, '.20f')[:10],\n",
        "                                            format(b_d_l, '.20f')[:10],\n",
        "                                            format(b_n_l, '.20f')[:10],\n",
        "                                            format(b_l, '.20f')[:10],\n",
        "                                            format(p_l, '.20f')[:10]))\n",
        "                self.u_losses.append(u_l)\n",
        "                self.i_losses.append(i_l)\n",
        "                self.d_losses.append(d_l)\n",
        "                self.n_losses.append(n_l)\n",
        "                self.b_i_losses.append(b_i_l)\n",
        "                self.b_d_losses.append(b_d_l)\n",
        "                self.b_n_losses.append(b_d_l)\n",
        "                self.b_losses.append(b_l)\n",
        "                self.p_losses.append(p_l)\n",
        "                #Eventually, update wandb\n",
        "                if wandb_run != None:\n",
        "                    wandb_run.log({'Unsupervised':u_l,\n",
        "                                  'Supervised':i_l + d_l + n_l,\n",
        "                                  'Total':p_l,\n",
        "                                   'Free Boundary':b_l,\n",
        "                                   'Initial':i_l,\n",
        "                                   'Dirichlet':d_l,\n",
        "                                   'Neumann':n_l,\n",
        "                                   'FB_Init':b_i_l,\n",
        "                                   'FB_Dir':b_d_l,\n",
        "                                   'FB_Neu':b_n_l})\n",
        "                #Check for the early warning\n",
        "                if p_l <= self.early_warning['Target']:\n",
        "                    self.early_warning['Target'] = p_l\n",
        "                    self.early_warning['n_steps'] = 0\n",
        "                    self.early_warning['top_mdl'] = old_top_mdl\n",
        "                    self.early_warning['weights'] = old_weights\n",
        "                else:\n",
        "                    self.early_warning['n_steps'] += 1\n",
        "                    if self.early_warning['n_steps'] >= self.params['patience']:\n",
        "                        break\n",
        "                #Save model and weights for next step early warning\n",
        "                old_top_mdl = tf.keras.models.clone_model( self.mdl )\n",
        "                old_weights = self.mdl.get_weights()\n",
        "        else:\n",
        "            #Before freezing, both mdl and fb are training\n",
        "            for epoch in tqdm(range(self.params['fb_freezing']),\n",
        "                              desc='PINNs - Training'):\n",
        "                if epoch == 0:\n",
        "                    print(print_base.format('Epoch', 'Unsupervised', 'Initial',\n",
        "                                            'Dirichlet', 'Neumann',\n",
        "                                            'FB_Init', 'FB_Dir', 'FB_Neu',\n",
        "                                            'Free Boundary', 'Total'))\n",
        "                    print('\\n')\n",
        "                #Case 1: more mdl steps for a single fb step\n",
        "                for _ in range(self.params['steps_fb_per_pde'] -1):\n",
        "                    self.train_mdl_solo();\n",
        "                #Case 2: more fb steps for a single mdl step\n",
        "                for _ in range(0, self.params['steps_fb_per_pde'] +1, -1):\n",
        "                    self.train_fb_solo();\n",
        "                u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l =\\\n",
        "                self.train_step()\n",
        "                u_l, i_l = np.array(u_l), np.array(i_l)\n",
        "                d_l, n_l = np.array(d_l), np.array(n_l)\n",
        "                b_i_l = np.array(b_i_l)\n",
        "                b_d_l, b_n_l = np.array(b_d_l), np.array(b_n_l)\n",
        "                b_l, p_l = np.array(b_l), np.array(p_l)\n",
        "                if epoch % self.params['verbose'] == 0:\n",
        "                    print(print_base.format(epoch+1, format(u_l, '.20f')[:10],\n",
        "                                            format(i_l, '.20f')[:10],\n",
        "                                            format(d_l, '.20f')[:10],\n",
        "                                            format(n_l, '.20f')[:10],\n",
        "                                            format(b_i_l, '.20f')[:10],\n",
        "                                            format(b_d_l, '.20f')[:10],\n",
        "                                            format(b_n_l, '.20f')[:10],\n",
        "                                            format(b_l, '.20f')[:10],\n",
        "                                            format(p_l, '.20f')[:10]))\n",
        "                self.u_losses.append(u_l)\n",
        "                self.i_losses.append(i_l)\n",
        "                self.d_losses.append(d_l)\n",
        "                self.n_losses.append(n_l)\n",
        "                self.b_i_losses.append(b_i_l)\n",
        "                self.b_d_losses.append(b_d_l)\n",
        "                self.b_n_losses.append(b_d_l)\n",
        "                self.b_losses.append(b_l)\n",
        "                self.p_losses.append(p_l)\n",
        "                #Eventually, update wandb\n",
        "                if wandb_run != None:\n",
        "                    wandb_run.log({'Unsupervised':u_l,\n",
        "                                  'Supervised':i_l + d_l + n_l,\n",
        "                                  'Total':p_l,\n",
        "                                   'Free Boundary':b_l,\n",
        "                                   'Initial':i_l,\n",
        "                                   'Dirichlet':d_l,\n",
        "                                   'Neumann':n_l,\n",
        "                                   'FB_Init':b_i_l,\n",
        "                                   'FB_Dir':b_d_l,\n",
        "                                   'FB_Neu':b_n_l})\n",
        "                #Check for the early warning\n",
        "                if p_l <= self.early_warning['Target']:\n",
        "                    self.early_warning['Target'] = p_l\n",
        "                    self.early_warning['n_steps'] = 0\n",
        "                    self.early_warning['top_mdl'] = old_top_mdl\n",
        "                    self.early_warning['weights'] = old_weights\n",
        "                else:\n",
        "                    self.early_warning['n_steps'] += 1\n",
        "                    if self.early_warning['n_steps'] >= self.params['patience']:\n",
        "                        break\n",
        "                #Save model and weights for next step early warning\n",
        "                old_top_mdl = tf.keras.models.clone_model( self.mdl )\n",
        "                old_weights = self.mdl.get_weights()\n",
        "            #Now, freeze fb and train only mdl\n",
        "            for epoch in tqdm(range(self.params['fb_freezing'],\n",
        "                                    self.params['epochs']),\n",
        "                              desc='PINNs - Training; Free Boundary Fixed'):\n",
        "                u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l =\\\n",
        "                self.train_mdl_solo()\n",
        "                u_l, i_l = np.array(u_l), np.array(i_l)\n",
        "                d_l, n_l = np.array(d_l), np.array(n_l)\n",
        "                b_i_l = np.array(b_i_l)\n",
        "                b_d_l, b_n_l = np.array(b_d_l), np.array(b_n_l)\n",
        "                b_l, p_l = np.array(b_l), np.array(p_l)\n",
        "                if epoch % self.params['verbose'] == 0:\n",
        "                    print(print_base.format(epoch+1, format(u_l, '.20f')[:10],\n",
        "                                            format(i_l, '.20f')[:10],\n",
        "                                            format(d_l, '.20f')[:10],\n",
        "                                            format(n_l, '.20f')[:10],\n",
        "                                            format(b_i_l, '.20f')[:10],\n",
        "                                            format(b_d_l, '.20f')[:10],\n",
        "                                            format(b_n_l, '.20f')[:10],\n",
        "                                            format(b_l, '.20f')[:10],\n",
        "                                            format(p_l, '.20f')[:10]))\n",
        "                self.u_losses.append(u_l)\n",
        "                self.i_losses.append(i_l)\n",
        "                self.d_losses.append(d_l)\n",
        "                self.n_losses.append(n_l)\n",
        "                self.b_i_losses.append(b_i_l)\n",
        "                self.b_d_losses.append(b_d_l)\n",
        "                self.b_n_losses.append(b_d_l)\n",
        "                self.b_losses.append(b_l)\n",
        "                self.p_losses.append(p_l)\n",
        "                #Eventually, update wandb\n",
        "                if wandb_run != None:\n",
        "                    wandb_run.log({'Unsupervised':u_l,\n",
        "                                  'Supervised':i_l + d_l + n_l,\n",
        "                                  'Total':p_l,\n",
        "                                   'Free Boundary':b_l,\n",
        "                                   'Initial':i_l,\n",
        "                                   'Dirichlet':d_l,\n",
        "                                   'Neumann':n_l,\n",
        "                                   'FB_Init':b_i_l,\n",
        "                                   'FB_Dir':b_d_l,\n",
        "                                   'FB_Neu':b_n_l})\n",
        "                #Check for the early warning\n",
        "                if p_l <= self.early_warning['Target']:\n",
        "                    self.early_warning['Target'] = p_l\n",
        "                    self.early_warning['n_steps'] = 0\n",
        "                    self.early_warning['top_mdl'] = old_top_mdl\n",
        "                    self.early_warning['weights'] = old_weights\n",
        "                else:\n",
        "                    self.early_warning['n_steps'] += 1\n",
        "                    if self.early_warning['n_steps'] >= self.params['patience']:\n",
        "                        break\n",
        "                #Save model and weights for next step early warning\n",
        "                old_top_mdl = tf.keras.models.clone_model( self.mdl )\n",
        "                old_weights = self.mdl.get_weights()\n",
        "        #Recover information about optimal epoch in early warning\n",
        "        self.mdl = tf.keras.models.clone_model( self.early_warning['top_mdl'] )\n",
        "        self.mdl.set_weights(self.early_warning['weights'])\n",
        "        top_epoch = epoch+1 - self.early_warning[\"n_steps\"]\n",
        "        print(f'Best loss achieved at step {top_epoch}')\n",
        "\n",
        "    def plot_losses(self):\n",
        "        plt.figure( figsize=(12,8) )\n",
        "        plt.semilogy(self.u_losses, label='Unsupervised')\n",
        "        plt.semilogy(np.array(self.i_losses) +\\\n",
        "                     np.array(self.d_losses) +\\\n",
        "                     np.array(self.n_losses),\n",
        "                     label='Supervised')\n",
        "        plt.semilogy(self.b_losses, label='Free Boundary')\n",
        "        plt.semilogy(self.p_losses, label='PINN')\n",
        "        plt.legend()\n",
        "        plt.title('Losses')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_unsupervised_test(self, test_sampler, test_ibc_cond,\n",
        "                               test_ibc_fb_cond, to_print=True, output=False):\n",
        "        #--------------- Compute Free Boundary losses\n",
        "        #Compute Initial Free Boundary Condition\n",
        "        if test_ibc_fb_cond['Initial'] != None:\n",
        "            fb_init = self.fb(test_ibc_fb_cond['Initial'].x,\n",
        "                                  training=False)\n",
        "            fb_init_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(fb_init-test_ibc_fb_cond['Initial'].y)\n",
        "                )\n",
        "        else:\n",
        "            fb_init_loss = 0\n",
        "        #Compute Dirichlet Free Boundary Condition\n",
        "        if test_ibc_fb_cond['Dirichlet'] != None:\n",
        "            #Compute Free Boundary values\n",
        "            s_values = self.fb(test_ibc_fb_cond['Dirichlet'].x,\n",
        "                                training=True)\n",
        "            fb_dc = self.mdl(tf.concat(\n",
        "                [s_values,\n",
        "                  test_ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                  training=True)\n",
        "            fb_dir_target = tf.nn.relu(\n",
        "                tf.ones_like(\n",
        "                    s_values\n",
        "                    ) * K - s_values\n",
        "            )\n",
        "            fb_dir_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(fb_dc - fb_dir_target)\n",
        "                )\n",
        "        else:\n",
        "            fb_dir_loss = 0\n",
        "        #Compute Neumann Free Boundary Condition\n",
        "        if test_ibc_fb_cond['Neumann'] != None:\n",
        "            s_values = self.fb(test_ibc_fb_cond['Neumann'].x,\n",
        "                                training=False)\n",
        "            with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                neu_fb_tape.watch(s_values)\n",
        "                pinn_nc_fb = self.mdl(\n",
        "                    tf.concat([s_values,\n",
        "                                test_ibc_fb_cond['Neumann'].x],\n",
        "                              axis=1),\n",
        "                              training=True)\n",
        "            pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                              s_values)\n",
        "            fb_neu_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(pinn_nc_fb-test_ibc_fb_cond['Neumann'].y)\n",
        "                )\n",
        "        else:\n",
        "            fb_neu_loss = 0\n",
        "        #Compute final loss\n",
        "        fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "\n",
        "        #--------------- Compute PINN losses\n",
        "        #Compute unsupervised loss\n",
        "        s_values = self.fb(tf.concat([test_sampler.t], axis=-1),\n",
        "                               training=False)\n",
        "        x_f = tf.reshape(test_sampler.x[ test_sampler.x > s_values ],\n",
        "                          (-1,1) )\n",
        "        t_f = tf.reshape(test_sampler.t[ test_sampler.x > s_values ],\n",
        "                          (-1,1) )\n",
        "        with G_Tape(persistent=True,\n",
        "                    watch_accessed_variables=False) as pinn_tape:\n",
        "            #Watch independet variables\n",
        "            pinn_tape.watch(x_f)\n",
        "            pinn_tape.watch(t_f)\n",
        "            #Apply u function for unsupervised\n",
        "            u_val = self.mdl(tf.stack([x_f, t_f], axis=1),\n",
        "                            training=True)\n",
        "            u_x = pinn_tape.gradient(u_val, x_f)\n",
        "        unsup_loss = tf.reduce_mean(tf.square(\n",
        "            self.pde(pinn_tape, x_f, t_f, u_val, u_x) ))\n",
        "        #Compute Initial Condition\n",
        "        if test_ibc_cond['Initial'] != None:\n",
        "            pinn_init = self.mdl(test_ibc_cond['Initial'].x,\n",
        "                                  training=False)\n",
        "            if len(pinn_init.shape) > 1:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  tf.expand_dims( test_ibc_cond['Initial'].y, axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  test_ibc_cond['Initial'].y )\n",
        "                    )\n",
        "        else:\n",
        "            init_loss = 0\n",
        "        #Compute Dirichlet Boundary Condition\n",
        "        if test_ibc_cond['Dirichlet'] != None:\n",
        "            pinn_dc = self.mdl(test_ibc_cond['Dirichlet'].x,\n",
        "                                training=False)\n",
        "            dir_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(pinn_dc-test_ibc_cond['Dirichlet'].y)\n",
        "                )\n",
        "        else:\n",
        "            dir_loss = 0\n",
        "        #Compute Neumann Boundary Condition\n",
        "        if test_ibc_cond['Neumann'] != None:\n",
        "            with G_Tape(watch_accessed_variables=False) as neu_tape:\n",
        "                neu_tape.watch(test_ibc_cond['Neumann'].x)\n",
        "                pinn_nc = self.mdl(tf.concat([test_ibc_cond['Neumann'].x,\n",
        "                                              test_ibc_cond['Neumann'].t],\n",
        "                                            axis=1),\n",
        "                                    training=False)\n",
        "            pinn_nc = neu_tape.gradient(pinn_nc, test_ibc_cond['Neumann'].x)\n",
        "            neu_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(pinn_nc-test_ibc_cond['Neumann'].y)\n",
        "                )\n",
        "        else:\n",
        "            neu_loss = 0\n",
        "\n",
        "        #--------------- Compute total loss\n",
        "        pinn_loss = unsup_loss + init_loss + dir_loss +\\\n",
        "        neu_loss + fb_dir_loss + fb_neu_loss\n",
        "\n",
        "        u_l, i_l = np.array(unsup_loss), np.array(init_loss)\n",
        "        d_l, n_l = np.array(dir_loss), np.array(neu_loss)\n",
        "        b_i_l = np.array(fb_init_loss)\n",
        "        b_d_l, b_n_l = np.array(fb_dir_loss), np.array(fb_neu_loss)\n",
        "        b_l, p_l = np.array(fb_loss), np.array(pinn_loss)\n",
        "\n",
        "        if to_print:\n",
        "            print_base = \"{:<10}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}\"\n",
        "            print(print_base.format('', 'Unsupervised', 'Initial',\n",
        "                                    'Dirichlet', 'Neumann', 'FB_Init', 'FB_Dir',\n",
        "                                    'FB_Neu', 'Free Boundary', 'Total'))\n",
        "            print(print_base.format('', format(u_l, '.20f')[:10],\n",
        "                                    format(i_l, '.20f')[:10],\n",
        "                                    format(d_l, '.20f')[:10],\n",
        "                                    format(n_l, '.20f')[:10],\n",
        "                                    format(b_i_l, '.20f')[:10],\n",
        "                                    format(b_d_l, '.20f')[:10],\n",
        "                                    format(b_n_l, '.20f')[:10],\n",
        "                                    format(b_l, '.20f')[:10],\n",
        "                                    format(p_l, '.20f')[:10]))\n",
        "        if output:\n",
        "            return u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIJ_9CBWYUQv"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juSGi9JVvD0i"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay as ex_d\n",
        "my_lr = ex_d(1e-2, 500, 0.9, staircase=False)\n",
        "\n",
        "params = {'sample_method':'sobol',\n",
        "          'layers':[n_dim+1, 20, 20, 20, 20, 20, 20, 20, 20, 1],\n",
        "          'activation':'tanh',\n",
        "          'output_act':'linear', 'initializer':'glorot_normal',\n",
        "          'fb_layers':[1, 100, 100, 100, n_dim], 'fb_activation':'tanh',\n",
        "          'fb_output_act':'linear', 'fb_initializer':'glorot_normal',\n",
        "          'lr':my_lr, 'optimizer':'rmsprop',\n",
        "          'fb_lr':my_lr, 'fb_optimizer':'rmsprop', 'steps_fb_per_pde':20,\n",
        "          'pde_weight':1, 'epochs':4000, 'verbose':50}\n",
        "\n",
        "my_pinn = FreeBoundary_PINN(params, pde, pinn_conditions,\n",
        "                            fb_conditions, lb, ub, N_f=30000, DTYPE=DTYPE)\n",
        "\n",
        "START = time.time()\n",
        "my_pinn.fit()\n",
        "train_time = time.time() - START\n",
        "my_pinn.plot_losses()\n",
        "\n",
        "START = time.time()\n",
        "values = my_pinn.plot_unsupervised_test(test_sampler, pinn_cond_test,\n",
        "                                        fb_cond_test, output=True)\n",
        "test_time = time.time() - START\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP66hwUTbvQl"
      },
      "source": [
        "# 2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlpfXeuTXXrl"
      },
      "source": [
        "### Define problem, initial and boundary conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzvWjmZWXXrl"
      },
      "outputs": [],
      "source": [
        "\n",
        "import time\n",
        "import wandb\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from math import pi, exp\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "n_dim = 2\n",
        "\n",
        "#Fix seeds\n",
        "random_seed = 2\n",
        "np.random.seed(random_seed)\n",
        "tf.random.set_seed(random_seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmTvlN8YXXrl"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Sampler_IBC():\n",
        "    def __init__(self, lb, ub, cond=None, N_points=100,\n",
        "                 method='sobol', grid=None, split=False, DTYPE='float64'):\n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "        self.cond = cond\n",
        "        self.DTYPE = DTYPE\n",
        "        self.sample(N_points, method, grid, split)\n",
        "\n",
        "    def sample(self, N_points, method, grid, split):\n",
        "        if method == 'uniform':\n",
        "            x_ibc = np.random.uniform(0, 1, size=(N_points, self.ub.shape[0]))\n",
        "            x_ibc = self.lb + (self.ub - self.lb)*x_ibc\n",
        "        elif method == 'latin':\n",
        "            from pyDOE import lhs\n",
        "            x_ibc = self.lb + (self.ub - self.lb)*lhs(self.ub.shape[0],N_points)\n",
        "        elif method == 'sobol':\n",
        "            import sobol\n",
        "            x_ibc = sobol.sample(dimension=self.ub.shape[0], n_points=N_points)\n",
        "            x_ibc = self.lb + (self.ub - self.lb)*x_ibc\n",
        "        elif method == 'equi':\n",
        "            x_ibc = np.linspace(self.lb, self.ub, N_points)\n",
        "        elif method == 'grid':\n",
        "            x_ibc = np.linspace(self.lb, self.ub, N_points).T\n",
        "            temp_final = list()\n",
        "            for val in x_ibc[0]:\n",
        "                temp_final.append( [val] )\n",
        "            dim = 1\n",
        "            while dim < x_ibc.shape[0]:\n",
        "                temp = list()\n",
        "                for t1 in range(x_ibc.shape[1]):\n",
        "                    for t2 in range(len(temp_final)):\n",
        "                        temp_val = temp_final[t2].copy()\n",
        "                        temp_val.append( x_ibc[dim, t1] )\n",
        "                        temp.append( temp_val )\n",
        "                temp_final = temp\n",
        "                dim += 1\n",
        "            x_ibc = np.array(temp_final)\n",
        "        elif method == 'grid_old':\n",
        "            idx = np.random.choice(range(grid.shape[0]),N_points,replace=False)\n",
        "            x_ibc = grid[idx]\n",
        "        if self.cond != None:\n",
        "            y_ibc = self.cond(x_ibc)\n",
        "            self.y = tf.cast(tf.Variable(y_ibc, trainable=False ),\n",
        "                             self.DTYPE)\n",
        "        if split:\n",
        "            x_ibc, t_ibc = x_ibc[:, :-1], x_ibc[:, -1:]\n",
        "            self.t = tf.cast(tf.Variable(t_ibc, trainable=False ),\n",
        "                             self.DTYPE)\n",
        "        self.x = tf.cast(tf.Variable(x_ibc, trainable=False ),\n",
        "                         self.DTYPE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MDL5UaiXXrm"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Define global variables\n",
        "r = 0.01\n",
        "T = 3\n",
        "K = 10\n",
        "sigma = [[0.05, 0.01], [0.01, 0.06]]\n",
        "sigma = np.array(sigma)\n",
        "if np.linalg.det(sigma) != 0:\n",
        "    if np.allclose(sigma, sigma.T):\n",
        "        if np.all(np.linalg.eigvals(sigma) > 0):\n",
        "            print('No problem with Covariance Matrix')\n",
        "        else:\n",
        "            print('Matrix is not positive semidefinite')\n",
        "    else:\n",
        "        print('Matrix is not symmetric')\n",
        "else:\n",
        "    print('Matrix is singular')\n",
        "\n",
        "d = np.array([r]*n_dim) - np.array([0]*n_dim)\n",
        "alphas = np.zeros((n_dim, n_dim))\n",
        "for i in range(n_dim):\n",
        "    alphas[i, i] = np.sum( np.dot(sigma[i], sigma[i]) )\n",
        "    for j in range(i):\n",
        "        alphas[i, j] = alphas[j, i] = np.sum( np.dot(sigma[i], sigma[j]) )\n",
        "DTYPE = 'float32'\n",
        "\n",
        "#Define domain boundaries\n",
        "lb = np.array( ([0.]*n_dim)+[0] )\n",
        "ub = np.array( ([3.*K]*n_dim)+[T] )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRyYIU8VXXrm"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Problem Definition\n",
        "def h_2(inp):\n",
        "    res = - np.ones( inp.shape[0] )\n",
        "    return res\n",
        "\n",
        "def g(inp):\n",
        "    res = list()\n",
        "    for inp_val in inp:\n",
        "        res.append( np.max([0,\n",
        "                            K*np.exp(-r*(T-inp_val[-1]))-\\\n",
        "                            np.min(inp_val[:-1])]) )\n",
        "    return np.array(res)\n",
        "\n",
        "def u_0(inp):\n",
        "    res = list()\n",
        "    for inp_val in inp:\n",
        "        res.append( np.max([0, K - np.min(inp_val[:-1])]) )\n",
        "    return np.array(res)\n",
        "\n",
        "def s_0(inp):\n",
        "    res = np.ones( (inp.shape[0], n_dim) ) * K\n",
        "    return np.array(res)\n",
        "\n",
        "#Point sampling\n",
        "N_to_sample = 1500\n",
        "\n",
        "#---------------- PDE Conditions\n",
        "print('PDE COnditions\\n')\n",
        "#Initial\n",
        "init_sampler = Sampler_IBC(np.array( ([0.]*n_dim)+[T] ),\n",
        "                           np.array( ([3.*K]*n_dim)+[T] ),\n",
        "                           u_0, N_to_sample, DTYPE=DTYPE )\n",
        "print( f'x: {init_sampler.x.shape}     y: {init_sampler.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "lb_dir = ([0.]*n_dim)+[0]; lb_dir[0] = 3.*K\n",
        "dir_sampler = Sampler_IBC(np.array( lb_dir ),\n",
        "                          np.array( ([3.*K]*n_dim)+[T] ),\n",
        "                          g, N_to_sample//n_dim, DTYPE=DTYPE )\n",
        "for dim in range(1, n_dim):\n",
        "    lb_dir = ([0.]*n_dim)+[0]; lb_dir[dim] = 3.*K\n",
        "    temp_sampler = Sampler_IBC(np.array( lb_dir ),\n",
        "                              np.array( ([3.*K]*n_dim)+[T] ),\n",
        "                              g, N_to_sample//n_dim, DTYPE=DTYPE )\n",
        "    dir_sampler.x = tf.concat([dir_sampler.x, temp_sampler.x], axis=0)\n",
        "    dir_sampler.y = tf.concat([dir_sampler.y, temp_sampler.y], axis=0)\n",
        "print( f'x: {dir_sampler.x.shape}     y: {dir_sampler.y.shape}' )\n",
        "\n",
        "#Neumann\n",
        "print('No')\n",
        "\n",
        "#---------------- Free Boundary Conditions\n",
        "print('\\nFree Boundary COnditions\\n')\n",
        "#Initial\n",
        "init_fb_sampler = Sampler_IBC(np.array([T]), np.array([T]),\n",
        "                              s_0, 1, DTYPE=DTYPE )\n",
        "print( f't: {init_fb_sampler.x.shape}     y: {init_fb_sampler.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "dir_fb_sampler = Sampler_IBC(np.array([0.]), np.array([T]),\n",
        "                             None, N_to_sample, DTYPE=DTYPE )\n",
        "print( f't: {dir_fb_sampler.x.shape}' )\n",
        "\n",
        "#Neumann\n",
        "neu_fb_sampler = Sampler_IBC(np.array([0.]), np.array([T]),\n",
        "                             None, N_to_sample, DTYPE=DTYPE )\n",
        "print( f't: {neu_fb_sampler.x.shape}' )\n",
        "\n",
        "#---------------- Conditions passed to PINN\n",
        "pinn_conditions = {'Initial':init_sampler,\n",
        "                   'Dirichlet':dir_sampler,\n",
        "                   'Neumann':None}\n",
        "fb_conditions = {'Initial':init_fb_sampler,\n",
        "                 'Dirichlet':dir_fb_sampler,\n",
        "                 'Neumann':neu_fb_sampler}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Yb34zbHXXrn"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_sampler = Sampler_IBC(lb, ub, cond=None, DTYPE=DTYPE,\n",
        "                           N_points=1000000, method='uniform', split=True)\n",
        "\n",
        "#Point sampling\n",
        "sample_to_test = 1000\n",
        "\n",
        "#---------------- PDE Conditions\n",
        "print('PDE COnditions\\n')\n",
        "#Initial\n",
        "init_sampler_test = Sampler_IBC(np.array( ([0.]*n_dim)+[T] ),\n",
        "                           np.array( ([3.*K]*n_dim)+[T] ),\n",
        "                           u_0, sample_to_test, DTYPE=DTYPE, method='uniform' )\n",
        "print( f'x: {init_sampler_test.x.shape}     y: {init_sampler_test.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "lb_dir = ([0.]*n_dim)+[0]; lb_dir[0] = 3.*K\n",
        "dir_sampler_test = Sampler_IBC(np.array( lb_dir ),\n",
        "                          np.array( ([3.*K]*n_dim)+[T] ),\n",
        "                          g, sample_to_test//n_dim, DTYPE=DTYPE, method='uniform' )\n",
        "for dim in range(1, n_dim):\n",
        "    lb_dir = ([0.]*n_dim)+[0]; lb_dir[dim] = 3.*K\n",
        "    temp_sampler = Sampler_IBC(np.array( lb_dir ),\n",
        "                              np.array( ([3.*K]*n_dim)+[T] ),\n",
        "                              g, sample_to_test//n_dim, DTYPE=DTYPE, method='uniform' )\n",
        "    dir_sampler_test.x = tf.concat([dir_sampler_test.x, temp_sampler.x], axis=0)\n",
        "    dir_sampler_test.y = tf.concat([dir_sampler_test.y, temp_sampler.y], axis=0)\n",
        "print( f'x: {dir_sampler_test.x.shape}     y: {dir_sampler_test.y.shape}' )\n",
        "\n",
        "#Neumann\n",
        "print('No')\n",
        "\n",
        "#---------------- Free Boundary Conditions\n",
        "print('\\nFree Boundary COnditions\\n')\n",
        "#Initial\n",
        "init_fb_sampler_test = Sampler_IBC(np.array([T]), np.array([T]),\n",
        "                              s_0, 1, DTYPE=DTYPE, method='uniform' )\n",
        "print( f't: {init_fb_sampler_test.x.shape}     y: {init_fb_sampler_test.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "dir_fb_sampler_test = Sampler_IBC(np.array([0.]), np.array([T]),\n",
        "                             None, sample_to_test, DTYPE=DTYPE, method='uniform' )\n",
        "print( f't: {dir_fb_sampler_test.x.shape}' )\n",
        "\n",
        "#Neumann\n",
        "neu_fb_sampler_test = Sampler_IBC(np.array([0.]), np.array([T]),\n",
        "                                  None, sample_to_test//n_dim, DTYPE=DTYPE, method='uniform' )\n",
        "print( f't: {neu_fb_sampler_test.x.shape}' )\n",
        "\n",
        "#---------------- Conditions passed to PINN\n",
        "pinn_cond_test = {'Initial':init_sampler_test,\n",
        "                   'Dirichlet':dir_sampler_test,\n",
        "                   'Neumann':None}\n",
        "fb_cond_test = {'Initial':init_fb_sampler_test,\n",
        "                 'Dirichlet':dir_fb_sampler_test,\n",
        "                 'Neumann':neu_fb_sampler_test}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ftpBs1PXXrn"
      },
      "source": [
        "### PINN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xU_VGk7XXro"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Define PDE\n",
        "def pde(tape, vars, fun_val, fo_grads):\n",
        "    second_order = 0\n",
        "    first_order = 0\n",
        "    zero_order = 0\n",
        "    zero_order += tape.gradient(fun_val, vars[-1]) - r * fun_val\n",
        "    for i, var in enumerate(vars[:-1]):\n",
        "        dx_i = fo_grads[i]\n",
        "        first_order += d[i] * var * dx_i\n",
        "        second_order +=\\\n",
        "        tape.gradient(dx_i, var) * (var**2) * alphas[i, i]\n",
        "        for j, var2 in enumerate(vars[:i]):\n",
        "            second_order +=\\\n",
        "            2*( tape.gradient(dx_i, var2) * var * var2 * alphas[i, j])\n",
        "    f = second_order/2 + first_order + zero_order\n",
        "    del(tape)\n",
        "    return f\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is0-fe_KXXro"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow import GradientTape as G_Tape\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.initializers import GlorotNormal, GlorotUniform\n",
        "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
        "from tensorflow.keras.metrics import mean_squared_error\n",
        "\n",
        "class FreeBoundary_PINN():\n",
        "\n",
        "    def __init__(self, params, pde, ibc_cond, ibc_fb_cond, lb, ub,\n",
        "                 N_f=10000, N_fb_ibc=150, DTYPE='float64', coll_points=None):\n",
        "        self.params = params\n",
        "        self.DTYPE = DTYPE\n",
        "        self.Default_Params()\n",
        "        #Set seed\n",
        "        if self.params['seed'] != None:\n",
        "            tf.keras.utils.set_random_seed( self.params['seed'] )\n",
        "            tf.config.experimental.enable_op_determinism()\n",
        "        #Define pde\n",
        "        self.pde = pde\n",
        "        #Define intial and boundary conditions\n",
        "        self.ibc_cond = ibc_cond\n",
        "        self.ibc_fb_cond = ibc_fb_cond\n",
        "        #All needed for points sampling\n",
        "        self.lb = tf.Variable(lb, trainable=False)\n",
        "        self.ub = tf.Variable(ub, trainable=False)\n",
        "        if coll_points == None:\n",
        "            self.N_f = N_f\n",
        "            self.Sample_Points()\n",
        "        else:\n",
        "            self.x_f = coll_points[0]\n",
        "            self.t_f = coll_points[1]\n",
        "        self.N_fb_ibc = N_fb_ibc\n",
        "        #Initialize the class: define the network\n",
        "        self.Define_Regularizer()\n",
        "        self.Define_Initializer()\n",
        "        self.Define_Optimizer()\n",
        "        self.Create_Network()\n",
        "        self.Create_FB_Network()\n",
        "\n",
        "    def Default_Params(self):\n",
        "        target = self.params.keys()\n",
        "        if 'seed' not in target:\n",
        "            self.params['seed'] = None\n",
        "        if 'optimizer' not in target:\n",
        "            self.params['optimizer'] = 'Adam'\n",
        "        if 'fb_optimizer' not in target:\n",
        "            self.params['fb_optimizer'] = 'Adam'\n",
        "        if 'reg_type' not in target:\n",
        "            self.params['reg_type'] = None\n",
        "        if 'initializer' not in target:\n",
        "            self.params['initializer'] = 'glorot_normal'\n",
        "        if 'activation' not in target:\n",
        "            self.params['activation'] = 'tanh'\n",
        "        if 'output_act' not in target:\n",
        "            self.params['output_act'] = 'linear'\n",
        "        if 'pde_weight' not in target:\n",
        "            self.params['pde_weight'] = 1.\n",
        "        if 'sup_weight' not in target:\n",
        "            self.params['sup_weight'] = [1., 1., 1.]\n",
        "        if 'fb_weight' not in target:\n",
        "            self.params['fb_weight'] = [1., 1., 1.]\n",
        "        if 'patience' not in target:\n",
        "            self.params['patience'] = np.inf\n",
        "        if 'sample_method' not in target:\n",
        "            self.params['sample_method'] = 'uniform'\n",
        "        if 'fb_output_act' not in target:\n",
        "            self.params['fb_output_act'] = 'linear'\n",
        "        if 'fb_activation' not in target:\n",
        "            self.params['fb_activation'] = 'tanh'\n",
        "        if 'verbose' not in target:\n",
        "            self.params['verbose'] = 1\n",
        "        if 'steps_fb_per_pde' not in target:\n",
        "            self.params['steps_fb_per_pde'] = 1\n",
        "        if 'fb_freezing' not in target:\n",
        "            self.params['fb_freezing'] = None\n",
        "\n",
        "    def Define_Regularizer(self):\n",
        "        if self.params['reg_type'] == 'l1':\n",
        "            self.regularizer = l1( self.params['reg'] )\n",
        "        elif self.params['reg_type'] == 'l2':\n",
        "            self.regularizer = l2( self.params['reg'] )\n",
        "        elif self.params['reg_type'] == 'l1_l2':\n",
        "            self.regularizer = l1_l2( self.params['reg'][0],\n",
        "                                      self.params['reg'][1] )\n",
        "        else:\n",
        "            self.regularizer = None\n",
        "\n",
        "    def Define_Initializer(self):\n",
        "        if self.params['initializer'] == 'glorot_normal':\n",
        "            self.initializer = GlorotNormal()\n",
        "        elif self.params['initializer'] == 'glorot_uniform':\n",
        "            self.initializer = GlorotUniform()\n",
        "        else:\n",
        "            self.initializer = None\n",
        "\n",
        "    def Define_Optimizer(self):\n",
        "        temp = self.params['optimizer']\n",
        "        if temp.lower() == 'adam':\n",
        "            self.opt = Adam( self.params['lr'] )\n",
        "        elif temp.lower() == 'rmsprop':\n",
        "            self.opt = RMSprop( self.params['lr'] )\n",
        "        else:\n",
        "            raise ValueError(f\"Optimizer {temp} not recognized\")\n",
        "\n",
        "        temp = self.params['fb_optimizer']\n",
        "        if temp.lower() == 'adam':\n",
        "            self.fb_opt = Adam( self.params['fb_lr'] )\n",
        "        elif temp.lower() == 'rmsprop':\n",
        "            self.fb_opt = RMSprop( self.params['fb_lr'] )\n",
        "        else:\n",
        "            raise ValueError(f\"fb_Optimizer {temp} not recognized\")\n",
        "\n",
        "    def Create_Network(self):\n",
        "        input_layer = Input(shape=self.params['layers'][0],\n",
        "                            name = 'Input')\n",
        "        x = Dense(units=self.params['layers'][1],\n",
        "                  activation=self.params['activation'],\n",
        "                  kernel_initializer=self.initializer,\n",
        "                  kernel_regularizer=self.regularizer,\n",
        "                  name='Dense_1')(input_layer)\n",
        "        for layer in range(2, len(self.params['layers'])-1):\n",
        "            x = Dense(units=self.params['layers'][layer],\n",
        "                      activation=self.params['activation'],\n",
        "                      kernel_initializer=self.initializer,\n",
        "                      kernel_regularizer=self.regularizer,\n",
        "                      name=f'Dense_{layer}')(x)\n",
        "        output = Dense(units=self.params['layers'][-1],\n",
        "                       activation=self.params['output_act'],\n",
        "                       kernel_initializer=self.initializer,\n",
        "                       kernel_regularizer=self.regularizer,\n",
        "                       name='Output')(x)\n",
        "        self.mdl = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "    def Create_FB_Network(self):\n",
        "        input_layer = Input(shape=self.params['fb_layers'][0],\n",
        "                            name = 'Input')\n",
        "        x = Dense(units=self.params['fb_layers'][1],\n",
        "                  activation=self.params['fb_activation'],\n",
        "                  kernel_initializer=self.initializer,\n",
        "                  kernel_regularizer=self.regularizer,\n",
        "                  name='Dense_1')(input_layer)\n",
        "        for layer in range(2, len(self.params['fb_layers'])-1):\n",
        "            x = Dense(units=self.params['fb_layers'][layer],\n",
        "                      activation=self.params['fb_activation'],\n",
        "                      kernel_initializer=self.initializer,\n",
        "                      kernel_regularizer=self.regularizer,\n",
        "                      name=f'Dense_{layer}')(x)\n",
        "        output = Dense(units=self.params['fb_layers'][-1],\n",
        "                       activation=self.params['fb_output_act'],\n",
        "                       kernel_initializer=self.initializer,\n",
        "                       kernel_regularizer=self.regularizer,\n",
        "                       name='Output')(x)\n",
        "        self.fb = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "    def Sample_Points(self):\n",
        "        #According to the selected method, sample collocation points\n",
        "        method = self.params['sample_method']\n",
        "        if method == 'latin':\n",
        "            from pyDOE import lhs\n",
        "            cps = self.lb + (self.ub - self.lb)*lhs(self.ub.shape[0], self.N_f)\n",
        "        elif method == 'uniform':\n",
        "            cps = np.random.uniform(0, 1, size=(self.N_f, self.ub.shape[0]))\n",
        "            cps = self.lb + (self.ub - self.lb)*cps\n",
        "        elif method == 'sobol':\n",
        "            import sobol\n",
        "            cps = sobol.sample(dimension=self.ub.shape[0], n_points=self.N_f)\n",
        "            cps = self.lb + (self.ub - self.lb)*cps\n",
        "        else:\n",
        "            raise ValueError(f'Sampling method {method} not recognized')\n",
        "        #Return collocation points as tf tensors\n",
        "        self.x_f_total = tf.cast(tf.Variable(cps[:, :-1], trainable=False),\n",
        "                           self.DTYPE)\n",
        "        self.t_f_total = tf.cast(tf.Variable(cps[:, -1:], trainable=False),\n",
        "                           self.DTYPE)\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self):\n",
        "        with G_Tape(watch_accessed_variables=False) as mdl_tape:\n",
        "            #Watch solution weights\n",
        "            mdl_tape.watch(self.mdl.trainable_variables)\n",
        "            #--------------- Compute Free Boundary losses\n",
        "            with G_Tape(watch_accessed_variables=False) as fb_tape:\n",
        "                #Watch free boundary weights\n",
        "                fb_tape.watch(self.fb.trainable_variables)\n",
        "                #Compute Initial Free Boundary Condition\n",
        "                fb_init = self.fb(self.ibc_fb_cond['Initial'].x,\n",
        "                                      training=True)\n",
        "                fb_init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_init-self.ibc_fb_cond['Initial'].y)\n",
        "                    )\n",
        "                #Compute Dirichlet Free Boundary Condition\n",
        "                if self.ibc_fb_cond['Dirichlet'] != None:\n",
        "                    #Compute Free Boundary values\n",
        "                    s_values = self.fb(self.ibc_fb_cond['Dirichlet'].x,\n",
        "                                       training=True)\n",
        "                    fb_dc = self.mdl(tf.concat(\n",
        "                        [s_values,\n",
        "                         self.ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                         training=True)\n",
        "                    fb_dir_target = tf.nn.relu(\n",
        "                        tf.ones_like(\n",
        "                            s_values[:,-1]\n",
        "                            ) * K - tf.math.reduce_min(s_values, axis=1)\n",
        "                    )\n",
        "                    fb_dir_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(fb_dc - fb_dir_target)\n",
        "                        )\n",
        "                else:\n",
        "                    fb_dir_loss = 0\n",
        "                #Compute Neumann Free Boundary Condition\n",
        "                if self.ibc_fb_cond['Neumann'] != None:\n",
        "                    s_values = self.fb(self.ibc_fb_cond['Neumann'].x,\n",
        "                                       training=True)\n",
        "                    with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                        neu_fb_tape.watch(s_values)\n",
        "                        pinn_nc_fb = self.mdl(\n",
        "                            tf.concat([s_values,\n",
        "                                       self.ibc_fb_cond['Neumann'].x],\n",
        "                                      axis=1),\n",
        "                                      training=True)\n",
        "                    pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                                      s_values)\n",
        "                    fb_neu_target = -tf.one_hot(\n",
        "                        tf.math.argmin( s_values, axis=1),\n",
        "                        depth=n_dim\n",
        "                        )\n",
        "                    fb_neu_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(pinn_nc_fb-fb_neu_target)\n",
        "                        )\n",
        "                else:\n",
        "                    fb_neu_loss = 0\n",
        "                #Compute final loss\n",
        "                fb_loss = self.params['fb_weight'][0] * fb_init_loss +\\\n",
        "                self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "                self.params['fb_weight'][2] * fb_neu_loss\n",
        "            #Compute gradient and apply optimizers for free boundary\n",
        "            gradient_fb = fb_tape.gradient(fb_loss,\n",
        "                                          self.fb.trainable_variables)\n",
        "            self.fb_opt.apply_gradients( zip(gradient_fb,\n",
        "                                          self.fb.trainable_variables) )\n",
        "\n",
        "            #--------------- Compute PINN losses\n",
        "            #Compute unsupervised loss\n",
        "            s_values = self.fb(tf.concat([self.t_f_total], axis=-1),\n",
        "                               training=False)\n",
        "            temp = tf.reduce_sum(tf.cast(self.x_f_total < s_values,\n",
        "                                         dtype=self.DTYPE),\n",
        "                                axis=-1) < tf.ones(self.t_f_total.shape[0])\n",
        "            x_f = self.x_f_total[ temp ]\n",
        "            t_f = tf.reshape(self.t_f_total[ temp ], (-1,1) )\n",
        "            variables = list()\n",
        "            derivatives = list()\n",
        "            for i in range(x_f.shape[1]):\n",
        "                variables.append( x_f[:, i:i+1] )\n",
        "            variables.append( t_f )\n",
        "            with G_Tape(persistent=True,\n",
        "                        watch_accessed_variables=False) as pinn_tape:\n",
        "                #Watch independet variables\n",
        "                for var in variables:\n",
        "                    pinn_tape.watch( var )\n",
        "                pinn_tape.watch(t_f)\n",
        "                u_val = self.mdl(tf.concat(variables, axis=1),\n",
        "                                    training=True)\n",
        "                u_x = pinn_tape.gradient(u_val, x_f)\n",
        "                for i, var in enumerate(variables[:-1]):\n",
        "                    derivatives.append( pinn_tape.gradient(u_val, var) )\n",
        "            unsup_loss = tf.reduce_mean(tf.square(\n",
        "                self.pde(pinn_tape, variables, u_val, derivatives) ))\n",
        "            #Compute Initial Condition\n",
        "            pinn_init = self.mdl(self.ibc_cond['Initial'].x,\n",
        "                                training=True)\n",
        "            if len(pinn_init.shape) > 1:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  tf.expand_dims(self.ibc_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  self.ibc_cond['Initial'].y )\n",
        "                    )\n",
        "            #Compute Dirichlet Boundary Condition\n",
        "            if self.ibc_cond['Dirichlet'] != None:\n",
        "                pinn_dc = self.mdl(self.ibc_cond['Dirichlet'].x,\n",
        "                                    training=True)\n",
        "                if len(pinn_dc.shape) > 1:\n",
        "                    dir_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(pinn_dc -\\\n",
        "                                      tf.expand_dims(self.ibc_cond['Dirichlet'].y,\n",
        "                                                     axis=-1 ) )\n",
        "                        )\n",
        "                else:\n",
        "                    dir_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(pinn_dc -\\\n",
        "                                      self.ibc_cond['Dirichlet'].y )\n",
        "                        )\n",
        "            else:\n",
        "                dir_loss = 0\n",
        "            #Compute Neumann Boundary Condition\n",
        "            if self.ibc_cond['Neumann'] != None:\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_tape:\n",
        "                    neu_tape.watch(self.ibc_cond['Neumann'].x)\n",
        "                    pinn_nc = self.mdl(tf.concat([self.ibc_cond['Neumann'].x,\n",
        "                                                  self.ibc_cond['Neumann'].t],\n",
        "                                                axis=1),\n",
        "                                        training=True)\n",
        "                pinn_nc = neu_tape.gradient(pinn_nc, self.ibc_cond['Neumann'].x)\n",
        "                neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc-self.ibc_cond['Neumann'].y)\n",
        "                    )\n",
        "            else:\n",
        "                neu_loss = 0\n",
        "\n",
        "            #--------------- Compute total loss\n",
        "            pinn_loss = (self.params['pde_weight'] * unsup_loss) +\\\n",
        "            self.params['sup_weight'][0] * init_loss +\\\n",
        "            self.params['sup_weight'][1] * dir_loss +\\\n",
        "            self.params['sup_weight'][2] * neu_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "        #Compute gradient and apply optimizers\n",
        "        gradient = mdl_tape.gradient(pinn_loss,self.mdl.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(gradient,self.mdl.trainable_variables))\n",
        "        #Return losses\n",
        "        final_fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "        final_pinn_loss = unsup_loss + init_loss +\\\n",
        "         dir_loss + neu_loss + fb_dir_loss + fb_neu_loss\n",
        "        return (unsup_loss, init_loss, dir_loss, neu_loss, fb_init_loss,\n",
        "                fb_dir_loss, fb_neu_loss, final_fb_loss, final_pinn_loss)\n",
        "\n",
        "    @tf.function\n",
        "    def train_mdl_solo(self):\n",
        "        with G_Tape(watch_accessed_variables=False) as mdl_tape:\n",
        "            #Watch solution weights\n",
        "            mdl_tape.watch(self.mdl.trainable_variables)\n",
        "            #--------------- Compute Free Boundary losses\n",
        "            #Compute Initial Free Boundary Condition\n",
        "            fb_init = self.fb(self.ibc_fb_cond['Initial'].x,\n",
        "                                  training=True)\n",
        "            fb_init_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(fb_init-self.ibc_fb_cond['Initial'].y)\n",
        "                )\n",
        "            #Compute Dirichlet Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Dirichlet'] != None:\n",
        "                #Compute Free Boundary values\n",
        "                s_values = self.fb(self.ibc_fb_cond['Dirichlet'].x,\n",
        "                                    training=True)\n",
        "                fb_dc = self.mdl(tf.concat(\n",
        "                    [s_values,\n",
        "                      self.ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                      training=True)\n",
        "                fb_dir_target = tf.nn.relu(\n",
        "                    tf.ones_like(\n",
        "                        s_values[:,-1]\n",
        "                        ) * K - tf.math.reduce_min(s_values, axis=1)\n",
        "                )\n",
        "                fb_dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_dc - fb_dir_target)\n",
        "                    )\n",
        "            else:\n",
        "                fb_dir_loss = 0\n",
        "            #Compute Neumann Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Neumann'] != None:\n",
        "                s_values = self.fb(self.ibc_fb_cond['Neumann'].x,\n",
        "                                    training=True)\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                    neu_fb_tape.watch(s_values)\n",
        "                    pinn_nc_fb = self.mdl(\n",
        "                        tf.concat([s_values,\n",
        "                                    self.ibc_fb_cond['Neumann'].x],\n",
        "                                  axis=1),\n",
        "                                  training=True)\n",
        "                pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                                  s_values)\n",
        "                fb_neu_target = -tf.one_hot(\n",
        "                    tf.math.argmin( s_values, axis=1),\n",
        "                    depth=n_dim\n",
        "                    )\n",
        "                fb_neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc_fb-fb_neu_target)\n",
        "                    )\n",
        "            else:\n",
        "                fb_neu_loss = 0\n",
        "            #Compute final loss\n",
        "            fb_loss = self.params['fb_weight'][0] * fb_init_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "\n",
        "            #--------------- Compute PINN losses\n",
        "            #Compute unsupervised loss\n",
        "            s_values = self.fb(tf.concat([self.t_f_total], axis=-1),\n",
        "                               training=False)\n",
        "            temp = tf.reduce_sum(tf.cast(self.x_f_total < s_values,\n",
        "                                         dtype=self.DTYPE),\n",
        "                                axis=-1) < tf.ones(self.t_f_total.shape[0])\n",
        "            x_f = self.x_f_total[ temp ]\n",
        "            t_f = tf.reshape(self.t_f_total[ temp ], (-1,1) )\n",
        "            variables = list()\n",
        "            derivatives = list()\n",
        "            for i in range(x_f.shape[1]):\n",
        "                variables.append( x_f[:, i:i+1] )\n",
        "            variables.append( t_f )\n",
        "            with G_Tape(persistent=True,\n",
        "                        watch_accessed_variables=False) as pinn_tape:\n",
        "                #Watch independet variables\n",
        "                for var in variables:\n",
        "                    pinn_tape.watch( var )\n",
        "                pinn_tape.watch(t_f)\n",
        "                u_val = self.mdl(tf.concat(variables, axis=1),\n",
        "                                    training=True)\n",
        "                u_x = pinn_tape.gradient(u_val, x_f)\n",
        "                for i, var in enumerate(variables[:-1]):\n",
        "                    derivatives.append( pinn_tape.gradient(u_val, var) )\n",
        "            unsup_loss = tf.reduce_mean(tf.square(\n",
        "                self.pde(pinn_tape, variables, u_val, derivatives) ))\n",
        "            #Compute Initial Condition\n",
        "            pinn_init = self.mdl(self.ibc_cond['Initial'].x,\n",
        "                                training=True)\n",
        "            if len(pinn_init.shape) > 1:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  tf.expand_dims(self.ibc_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  self.ibc_cond['Initial'].y )\n",
        "                    )\n",
        "            #Compute Dirichlet Boundary Condition\n",
        "            if self.ibc_cond['Dirichlet'] != None:\n",
        "                pinn_dc = self.mdl(self.ibc_cond['Dirichlet'].x,\n",
        "                                    training=True)\n",
        "                if len(pinn_dc.shape) > 1:\n",
        "                    dir_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(pinn_dc -\\\n",
        "                                      tf.expand_dims(self.ibc_cond['Dirichlet'].y,\n",
        "                                                     axis=-1 ) )\n",
        "                        )\n",
        "                else:\n",
        "                    dir_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(pinn_dc -\\\n",
        "                                      self.ibc_cond['Dirichlet'].y )\n",
        "                        )\n",
        "            else:\n",
        "                dir_loss = 0\n",
        "            #Compute Neumann Boundary Condition\n",
        "            if self.ibc_cond['Neumann'] != None:\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_tape:\n",
        "                    neu_tape.watch(self.ibc_cond['Neumann'].x)\n",
        "                    pinn_nc = self.mdl(tf.concat([self.ibc_cond['Neumann'].x,\n",
        "                                                  self.ibc_cond['Neumann'].t],\n",
        "                                                axis=1),\n",
        "                                        training=True)\n",
        "                pinn_nc = neu_tape.gradient(pinn_nc, self.ibc_cond['Neumann'].x)\n",
        "                neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc-self.ibc_cond['Neumann'].y)\n",
        "                    )\n",
        "            else:\n",
        "                neu_loss = 0\n",
        "\n",
        "            #--------------- Compute total loss\n",
        "            pinn_loss = (self.params['pde_weight'] * unsup_loss) +\\\n",
        "            self.params['sup_weight'][0] * init_loss +\\\n",
        "            self.params['sup_weight'][1] * dir_loss +\\\n",
        "            self.params['sup_weight'][2] * neu_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "        #Compute gradient and apply optimizers\n",
        "        gradient = mdl_tape.gradient(pinn_loss,self.mdl.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(gradient,self.mdl.trainable_variables))\n",
        "        #Return losses\n",
        "        final_fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "        final_pinn_loss = unsup_loss + init_loss +\\\n",
        "         dir_loss + neu_loss + fb_dir_loss + fb_neu_loss\n",
        "        return (unsup_loss, init_loss, dir_loss, neu_loss, fb_init_loss,\n",
        "                fb_dir_loss, fb_neu_loss, final_fb_loss, final_pinn_loss)\n",
        "\n",
        "    @tf.function\n",
        "    def train_fb_solo(self):\n",
        "        #--------------- Compute Free Boundary losses\n",
        "        with G_Tape(watch_accessed_variables=False) as fb_tape:\n",
        "            #Watch free boundary weights\n",
        "            fb_tape.watch(self.fb.trainable_variables)\n",
        "            #Compute Initial Free Boundary Condition\n",
        "            fb_init = self.fb(self.ibc_fb_cond['Initial'].x,\n",
        "                                  training=True)\n",
        "            fb_init_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(fb_init-self.ibc_fb_cond['Initial'].y)\n",
        "                )\n",
        "            #Compute Dirichlet Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Dirichlet'] != None:\n",
        "                #Compute Free Boundary values\n",
        "                s_values = self.fb(self.ibc_fb_cond['Dirichlet'].x,\n",
        "                                    training=True)\n",
        "                fb_dc = self.mdl(tf.concat(\n",
        "                    [s_values,\n",
        "                      self.ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                      training=True)\n",
        "                fb_dir_target = tf.nn.relu(\n",
        "                    tf.ones_like(\n",
        "                        s_values[:,-1]\n",
        "                        ) * K - tf.math.reduce_min(s_values, axis=1)\n",
        "                )\n",
        "                fb_dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_dc - fb_dir_target)\n",
        "                    )\n",
        "            else:\n",
        "                fb_dir_loss = 0\n",
        "            #Compute Neumann Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Neumann'] != None:\n",
        "                s_values = self.fb(self.ibc_fb_cond['Neumann'].x,\n",
        "                                    training=True)\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                    neu_fb_tape.watch(s_values)\n",
        "                    pinn_nc_fb = self.mdl(\n",
        "                        tf.concat([s_values,\n",
        "                                    self.ibc_fb_cond['Neumann'].x],\n",
        "                                  axis=1),\n",
        "                                  training=True)\n",
        "                pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                                  s_values)\n",
        "                fb_neu_target = -tf.one_hot(\n",
        "                    tf.math.argmin( s_values, axis=1),\n",
        "                    depth=n_dim\n",
        "                    )\n",
        "                fb_neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc_fb-fb_neu_target)\n",
        "                    )\n",
        "            else:\n",
        "                fb_neu_loss = 0\n",
        "            #Compute final loss\n",
        "            fb_loss = self.params['fb_weight'][0] * fb_init_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "        #Compute gradient and apply optimizers for free boundary\n",
        "        gradient_fb = fb_tape.gradient(fb_loss,\n",
        "                                      self.fb.trainable_variables)\n",
        "        self.fb_opt.apply_gradients( zip(gradient_fb,\n",
        "                                      self.fb.trainable_variables) )\n",
        "        #Return losses\n",
        "        final_fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "        return fb_init_loss, fb_dir_loss, fb_neu_loss, final_fb_loss\n",
        "\n",
        "    def fit(self, wandb_run=None):\n",
        "        #Early warning initialization\n",
        "        self.early_warning = {'Target':np.inf,\n",
        "                              'n_steps':0,\n",
        "                              'top_mdl':None,\n",
        "                              'weights':None}\n",
        "        old_top_mdl, old_weights = None, None\n",
        "        #Training\n",
        "        self.u_losses, self.i_losses = list(), list()\n",
        "        self.d_losses, self.n_losses = list(), list()\n",
        "        self.b_i_losses = list()\n",
        "        self.b_d_losses, self.b_n_losses = list(), list()\n",
        "        self.b_losses, self.p_losses = list(), list()\n",
        "        print_base = \"{:<10}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}\"\n",
        "        if self.params['fb_freezing'] == None:\n",
        "            for epoch in tqdm(range(self.params['epochs']),\n",
        "                              desc='PINNs - Training'):\n",
        "                if epoch == 0:\n",
        "                    print(print_base.format('Epoch', 'Unsupervised', 'Initial',\n",
        "                                            'Dirichlet', 'Neumann',\n",
        "                                            'FB_Init', 'FB_Dir', 'FB_Neu',\n",
        "                                            'Free Boundary', 'Total'))\n",
        "                    print('\\n')\n",
        "                #Case 1: more mdl steps for a single fb step\n",
        "                for _ in range(self.params['steps_fb_per_pde'] -1):\n",
        "                    self.train_mdl_solo();\n",
        "                #Case 2: more fb steps for a single mdl step\n",
        "                for _ in range(0, self.params['steps_fb_per_pde'] +1, -1):\n",
        "                    self.train_fb_solo();\n",
        "                u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l =\\\n",
        "                self.train_step()\n",
        "                u_l, i_l = np.array(u_l), np.array(i_l)\n",
        "                d_l, n_l = np.array(d_l), np.array(n_l)\n",
        "                b_i_l = np.array(b_i_l)\n",
        "                b_d_l, b_n_l = np.array(b_d_l), np.array(b_n_l)\n",
        "                b_l, p_l = np.array(b_l), np.array(p_l)\n",
        "                if epoch % self.params['verbose'] == 0:\n",
        "                    print(print_base.format(epoch+1, format(u_l, '.20f')[:10],\n",
        "                                            format(i_l, '.20f')[:10],\n",
        "                                            format(d_l, '.20f')[:10],\n",
        "                                            format(n_l, '.20f')[:10],\n",
        "                                            format(b_i_l, '.20f')[:10],\n",
        "                                            format(b_d_l, '.20f')[:10],\n",
        "                                            format(b_n_l, '.20f')[:10],\n",
        "                                            format(b_l, '.20f')[:10],\n",
        "                                            format(p_l, '.20f')[:10]))\n",
        "                self.u_losses.append(u_l)\n",
        "                self.i_losses.append(i_l)\n",
        "                self.d_losses.append(d_l)\n",
        "                self.n_losses.append(n_l)\n",
        "                self.b_i_losses.append(b_i_l)\n",
        "                self.b_d_losses.append(b_d_l)\n",
        "                self.b_n_losses.append(b_d_l)\n",
        "                self.b_losses.append(b_l)\n",
        "                self.p_losses.append(p_l)\n",
        "                #Eventually, update wandb\n",
        "                if wandb_run != None:\n",
        "                    wandb_run.log({'Unsupervised':u_l,\n",
        "                                  'Supervised':i_l + d_l + n_l,\n",
        "                                  'Total':p_l,\n",
        "                                   'Free Boundary':b_l,\n",
        "                                   'Initial':i_l,\n",
        "                                   'Dirichlet':d_l,\n",
        "                                   'Neumann':n_l,\n",
        "                                   'FB_Init':b_i_l,\n",
        "                                   'FB_Dir':b_d_l,\n",
        "                                   'FB_Neu':b_n_l})\n",
        "                #Check for the early warning\n",
        "                if p_l <= self.early_warning['Target']:\n",
        "                    self.early_warning['Target'] = p_l\n",
        "                    self.early_warning['n_steps'] = 0\n",
        "                    self.early_warning['top_mdl'] = old_top_mdl\n",
        "                    self.early_warning['weights'] = old_weights\n",
        "                else:\n",
        "                    self.early_warning['n_steps'] += 1\n",
        "                    if self.early_warning['n_steps'] >= self.params['patience']:\n",
        "                        break\n",
        "                #Save model and weights for next step early warning\n",
        "                old_top_mdl = tf.keras.models.clone_model( self.mdl )\n",
        "                old_weights = self.mdl.get_weights()\n",
        "        else:\n",
        "            #Before freezing, both mdl and fb are training\n",
        "            for epoch in tqdm(range(self.params['fb_freezing']),\n",
        "                              desc='PINNs - Training'):\n",
        "                if epoch == 0:\n",
        "                    print(print_base.format('Epoch', 'Unsupervised', 'Initial',\n",
        "                                            'Dirichlet', 'Neumann',\n",
        "                                            'FB_Init', 'FB_Dir', 'FB_Neu',\n",
        "                                            'Free Boundary', 'Total'))\n",
        "                    print('\\n')\n",
        "                #Case 1: more mdl steps for a single fb step\n",
        "                for _ in range(self.params['steps_fb_per_pde'] -1):\n",
        "                    self.train_mdl_solo();\n",
        "                #Case 2: more fb steps for a single mdl step\n",
        "                for _ in range(0, self.params['steps_fb_per_pde'] +1, -1):\n",
        "                    self.train_fb_solo();\n",
        "                u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l =\\\n",
        "                self.train_step()\n",
        "                u_l, i_l = np.array(u_l), np.array(i_l)\n",
        "                d_l, n_l = np.array(d_l), np.array(n_l)\n",
        "                b_i_l = np.array(b_i_l)\n",
        "                b_d_l, b_n_l = np.array(b_d_l), np.array(b_n_l)\n",
        "                b_l, p_l = np.array(b_l), np.array(p_l)\n",
        "                if epoch % self.params['verbose'] == 0:\n",
        "                    print(print_base.format(epoch+1, format(u_l, '.20f')[:10],\n",
        "                                            format(i_l, '.20f')[:10],\n",
        "                                            format(d_l, '.20f')[:10],\n",
        "                                            format(n_l, '.20f')[:10],\n",
        "                                            format(b_i_l, '.20f')[:10],\n",
        "                                            format(b_d_l, '.20f')[:10],\n",
        "                                            format(b_n_l, '.20f')[:10],\n",
        "                                            format(b_l, '.20f')[:10],\n",
        "                                            format(p_l, '.20f')[:10]))\n",
        "                self.u_losses.append(u_l)\n",
        "                self.i_losses.append(i_l)\n",
        "                self.d_losses.append(d_l)\n",
        "                self.n_losses.append(n_l)\n",
        "                self.b_i_losses.append(b_i_l)\n",
        "                self.b_d_losses.append(b_d_l)\n",
        "                self.b_n_losses.append(b_d_l)\n",
        "                self.b_losses.append(b_l)\n",
        "                self.p_losses.append(p_l)\n",
        "                #Eventually, update wandb\n",
        "                if wandb_run != None:\n",
        "                    wandb_run.log({'Unsupervised':u_l,\n",
        "                                  'Supervised':i_l + d_l + n_l,\n",
        "                                  'Total':p_l,\n",
        "                                   'Free Boundary':b_l,\n",
        "                                   'Initial':i_l,\n",
        "                                   'Dirichlet':d_l,\n",
        "                                   'Neumann':n_l,\n",
        "                                   'FB_Init':b_i_l,\n",
        "                                   'FB_Dir':b_d_l,\n",
        "                                   'FB_Neu':b_n_l})\n",
        "                #Check for the early warning\n",
        "                if p_l <= self.early_warning['Target']:\n",
        "                    self.early_warning['Target'] = p_l\n",
        "                    self.early_warning['n_steps'] = 0\n",
        "                    self.early_warning['top_mdl'] = old_top_mdl\n",
        "                    self.early_warning['weights'] = old_weights\n",
        "                else:\n",
        "                    self.early_warning['n_steps'] += 1\n",
        "                    if self.early_warning['n_steps'] >= self.params['patience']:\n",
        "                        break\n",
        "                #Save model and weights for next step early warning\n",
        "                old_top_mdl = tf.keras.models.clone_model( self.mdl )\n",
        "                old_weights = self.mdl.get_weights()\n",
        "            #Now, freeze fb and train only mdl\n",
        "            for epoch in tqdm(range(self.params['fb_freezing'],\n",
        "                                    self.params['epochs']),\n",
        "                              desc='PINNs - Training; Free Boundary Fixed'):\n",
        "                u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l =\\\n",
        "                self.train_mdl_solo()\n",
        "                u_l, i_l = np.array(u_l), np.array(i_l)\n",
        "                d_l, n_l = np.array(d_l), np.array(n_l)\n",
        "                b_i_l = np.array(b_i_l)\n",
        "                b_d_l, b_n_l = np.array(b_d_l), np.array(b_n_l)\n",
        "                b_l, p_l = np.array(b_l), np.array(p_l)\n",
        "                if epoch % self.params['verbose'] == 0:\n",
        "                    print(print_base.format(epoch+1, format(u_l, '.20f')[:10],\n",
        "                                            format(i_l, '.20f')[:10],\n",
        "                                            format(d_l, '.20f')[:10],\n",
        "                                            format(n_l, '.20f')[:10],\n",
        "                                            format(b_i_l, '.20f')[:10],\n",
        "                                            format(b_d_l, '.20f')[:10],\n",
        "                                            format(b_n_l, '.20f')[:10],\n",
        "                                            format(b_l, '.20f')[:10],\n",
        "                                            format(p_l, '.20f')[:10]))\n",
        "                self.u_losses.append(u_l)\n",
        "                self.i_losses.append(i_l)\n",
        "                self.d_losses.append(d_l)\n",
        "                self.n_losses.append(n_l)\n",
        "                self.b_i_losses.append(b_i_l)\n",
        "                self.b_d_losses.append(b_d_l)\n",
        "                self.b_n_losses.append(b_d_l)\n",
        "                self.b_losses.append(b_l)\n",
        "                self.p_losses.append(p_l)\n",
        "                #Eventually, update wandb\n",
        "                if wandb_run != None:\n",
        "                    wandb_run.log({'Unsupervised':u_l,\n",
        "                                  'Supervised':i_l + d_l + n_l,\n",
        "                                  'Total':p_l,\n",
        "                                   'Free Boundary':b_l,\n",
        "                                   'Initial':i_l,\n",
        "                                   'Dirichlet':d_l,\n",
        "                                   'Neumann':n_l,\n",
        "                                   'FB_Init':b_i_l,\n",
        "                                   'FB_Dir':b_d_l,\n",
        "                                   'FB_Neu':b_n_l})\n",
        "                #Check for the early warning\n",
        "                if p_l <= self.early_warning['Target']:\n",
        "                    self.early_warning['Target'] = p_l\n",
        "                    self.early_warning['n_steps'] = 0\n",
        "                    self.early_warning['top_mdl'] = old_top_mdl\n",
        "                    self.early_warning['weights'] = old_weights\n",
        "                else:\n",
        "                    self.early_warning['n_steps'] += 1\n",
        "                    if self.early_warning['n_steps'] >= self.params['patience']:\n",
        "                        break\n",
        "                #Save model and weights for next step early warning\n",
        "                old_top_mdl = tf.keras.models.clone_model( self.mdl )\n",
        "                old_weights = self.mdl.get_weights()\n",
        "        #Recover information about optimal epoch in early warning\n",
        "        self.mdl = tf.keras.models.clone_model( self.early_warning['top_mdl'] )\n",
        "        self.mdl.set_weights(self.early_warning['weights'])\n",
        "        top_epoch = epoch+1 - self.early_warning[\"n_steps\"]\n",
        "        print(f'Best loss achieved at step {top_epoch}')\n",
        "\n",
        "    def plot_losses(self):\n",
        "        plt.figure( figsize=(12,8) )\n",
        "        plt.semilogy(self.u_losses, label='Unsupervised')\n",
        "        plt.semilogy(np.array(self.i_losses) +\\\n",
        "                     np.array(self.d_losses) +\\\n",
        "                     np.array(self.n_losses),\n",
        "                     label='Supervised')\n",
        "        plt.semilogy(self.b_losses, label='Free Boundary')\n",
        "        plt.semilogy(self.p_losses, label='PINN')\n",
        "        plt.legend()\n",
        "        plt.title('Losses')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_unsupervised_test(self, test_sampler, test_ibc_cond,\n",
        "                               test_ibc_fb_cond, to_print=True, output=False):\n",
        "        #--------------- Compute Free Boundary losses\n",
        "        #Compute Initial Free Boundary Condition\n",
        "        if test_ibc_fb_cond['Initial'] != None:\n",
        "            fb_init = self.fb(test_ibc_fb_cond['Initial'].x,\n",
        "                                  training=False)\n",
        "            fb_init_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(fb_init-test_ibc_fb_cond['Initial'].y)\n",
        "                )\n",
        "        else:\n",
        "            fb_init_loss = 0\n",
        "        #Compute Dirichlet Free Boundary Condition\n",
        "        if test_ibc_fb_cond['Dirichlet'] != None:\n",
        "            #Compute Free Boundary values\n",
        "            s_values = self.fb(test_ibc_fb_cond['Dirichlet'].x,\n",
        "                                training=True)\n",
        "            fb_dc = self.mdl(tf.concat(\n",
        "                [s_values,\n",
        "                  test_ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                  training=True)\n",
        "            fb_dir_target = tf.nn.relu(\n",
        "                tf.ones_like(\n",
        "                    s_values[:,-1]\n",
        "                    ) * K - tf.math.reduce_min(s_values, axis=1)\n",
        "            )\n",
        "            fb_dir_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(fb_dc - fb_dir_target)\n",
        "                )\n",
        "        else:\n",
        "            fb_dir_loss = 0\n",
        "        #Compute Neumann Free Boundary Condition\n",
        "        if test_ibc_fb_cond['Neumann'] != None:\n",
        "            s_values = self.fb(test_ibc_fb_cond['Neumann'].x,\n",
        "                                training=True)\n",
        "            with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                neu_fb_tape.watch(s_values)\n",
        "                pinn_nc_fb = self.mdl(\n",
        "                    tf.concat([s_values,\n",
        "                                test_ibc_fb_cond['Neumann'].x],\n",
        "                              axis=1),\n",
        "                              training=True)\n",
        "            pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                              s_values)\n",
        "            fb_neu_target = -tf.one_hot(\n",
        "                tf.math.argmin( s_values, axis=1),\n",
        "                depth=n_dim\n",
        "                )\n",
        "            fb_neu_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(pinn_nc_fb-fb_neu_target)\n",
        "                )\n",
        "        else:\n",
        "            fb_neu_loss = 0\n",
        "        #Compute final loss\n",
        "        fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "\n",
        "        #--------------- Compute PINN losses\n",
        "        #Compute unsupervised loss\n",
        "        s_values = self.fb(tf.concat([test_sampler.t], axis=-1),\n",
        "                            training=False)\n",
        "        temp = tf.reduce_sum(tf.cast(test_sampler.x < s_values,\n",
        "                                      dtype=self.DTYPE),\n",
        "                            axis=-1) < tf.ones(test_sampler.t.shape[0])\n",
        "        x_f = test_sampler.x[ temp ]\n",
        "        t_f = tf.reshape(test_sampler.t[ temp ], (-1,1) )\n",
        "        variables = list()\n",
        "        derivatives = list()\n",
        "        for i in range(x_f.shape[1]):\n",
        "            variables.append( x_f[:, i:i+1] )\n",
        "        variables.append( t_f )\n",
        "        with G_Tape(persistent=True,\n",
        "                    watch_accessed_variables=False) as pinn_tape:\n",
        "            #Watch independet variables\n",
        "            for var in variables:\n",
        "                pinn_tape.watch( var )\n",
        "            pinn_tape.watch(t_f)\n",
        "            u_val = self.mdl(tf.concat(variables, axis=1),\n",
        "                                training=True)\n",
        "            u_x = pinn_tape.gradient(u_val, x_f)\n",
        "            for i, var in enumerate(variables[:-1]):\n",
        "                derivatives.append( pinn_tape.gradient(u_val, var) )\n",
        "        unsup_loss = tf.reduce_mean(tf.square(\n",
        "            self.pde(pinn_tape, variables, u_val, derivatives) ))\n",
        "        #Compute Initial Condition\n",
        "        if test_ibc_cond['Initial'] != None:\n",
        "            pinn_init = self.mdl(test_ibc_cond['Initial'].x,\n",
        "                                  training=False)\n",
        "            if len(pinn_init.shape) > 1:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  tf.expand_dims(test_ibc_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  test_ibc_cond['Initial'].y )\n",
        "                    )\n",
        "        else:\n",
        "            init_loss = 0\n",
        "        #Compute Dirichlet Boundary Condition\n",
        "        if test_ibc_cond['Dirichlet'] != None:\n",
        "            pinn_dc = self.mdl(test_ibc_cond['Dirichlet'].x,\n",
        "                                training=True)\n",
        "            if len(pinn_dc.shape) > 1:\n",
        "                dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_dc -\\\n",
        "                                  tf.expand_dims(test_ibc_cond['Dirichlet'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_dc -\\\n",
        "                                  test_ibc_cond['Dirichlet'].y )\n",
        "                    )\n",
        "        else:\n",
        "            dir_loss = 0\n",
        "        #Compute Neumann Boundary Condition\n",
        "        if test_ibc_cond['Neumann'] != None:\n",
        "            with G_Tape(watch_accessed_variables=False) as neu_tape:\n",
        "                neu_tape.watch(test_ibc_cond['Neumann'].x)\n",
        "                pinn_nc = self.mdl(tf.concat([test_ibc_cond['Neumann'].x,\n",
        "                                              test_ibc_cond['Neumann'].t],\n",
        "                                            axis=1),\n",
        "                                    training=False)\n",
        "            pinn_nc = neu_tape.gradient(pinn_nc, test_ibc_cond['Neumann'].x)\n",
        "            neu_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(pinn_nc-test_ibc_cond['Neumann'].y)\n",
        "                )\n",
        "        else:\n",
        "            neu_loss = 0\n",
        "\n",
        "        #--------------- Compute total loss\n",
        "        pinn_loss = unsup_loss + init_loss + dir_loss +\\\n",
        "        neu_loss + fb_dir_loss + fb_neu_loss\n",
        "\n",
        "        u_l, i_l = np.array(unsup_loss), np.array(init_loss)\n",
        "        d_l, n_l = np.array(dir_loss), np.array(neu_loss)\n",
        "        b_i_l = np.array(fb_init_loss)\n",
        "        b_d_l, b_n_l = np.array(fb_dir_loss), np.array(fb_neu_loss)\n",
        "        b_l, p_l = np.array(fb_loss), np.array(pinn_loss)\n",
        "\n",
        "        if to_print:\n",
        "            print_base = \"{:<10}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}\"\n",
        "            print(print_base.format('', 'Unsupervised', 'Initial',\n",
        "                                    'Dirichlet', 'Neumann', 'FB_Init', 'FB_Dir',\n",
        "                                    'FB_Neu', 'Free Boundary', 'Total'))\n",
        "            print(print_base.format('', format(u_l, '.20f')[:10],\n",
        "                                    format(i_l, '.20f')[:10],\n",
        "                                    format(d_l, '.20f')[:10],\n",
        "                                    format(n_l, '.20f')[:10],\n",
        "                                    format(b_i_l, '.20f')[:10],\n",
        "                                    format(b_d_l, '.20f')[:10],\n",
        "                                    format(b_n_l, '.20f')[:10],\n",
        "                                    format(b_l, '.20f')[:10],\n",
        "                                    format(p_l, '.20f')[:10]))\n",
        "        if output:\n",
        "            return u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHLHqOxxXXrz"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vFObDd5XXr0"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay as ex_d\n",
        "my_lr = ex_d(1e-2, 400, 0.9, staircase=False)\n",
        "fb_lr = ex_d(1e-2, 100, 0.9, staircase=False)\n",
        "\n",
        "params = {'sample_method':'sobol',\n",
        "          'layers':[n_dim+1, 20, 20, 20, 20, 20, 20, 20, 20, 1],\n",
        "          'activation':'tanh',\n",
        "          'output_act':'linear', 'initializer':'glorot_normal',\n",
        "          'fb_layers':[1, 100, 100, 100, n_dim], 'fb_activation':'tanh',\n",
        "          'fb_output_act':'linear', 'fb_initializer':'glorot_normal',\n",
        "          'lr':my_lr, 'optimizer':'rmsprop',\n",
        "          'fb_lr':fb_lr, 'fb_optimizer':'rmsprop', 'steps_fb_per_pde':4,\n",
        "          'pde_weight':1, 'epochs':10000, 'verbose':100}\n",
        "\n",
        "my_pinn = FreeBoundary_PINN(params, pde, pinn_conditions,\n",
        "                            fb_conditions, lb, ub, N_f=60000, DTYPE=DTYPE)\n",
        "\n",
        "START = time.time()\n",
        "my_pinn.fit()\n",
        "train_time = time.time() - START\n",
        "my_pinn.plot_losses()\n",
        "\n",
        "START = time.time()\n",
        "values = my_pinn.plot_unsupervised_test(test_sampler, pinn_cond_test,\n",
        "                                        fb_cond_test, output=True)\n",
        "test_time = time.time() - START\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-r6pXrZtBQj"
      },
      "source": [
        "# 3D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q2gNjq6CCBD"
      },
      "source": [
        "### Define problem, initial and boundary conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn1PiKXOCCBD"
      },
      "outputs": [],
      "source": [
        "\n",
        "import time\n",
        "import wandb\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from math import pi, exp\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "n_dim = 3\n",
        "\n",
        "#Fix seeds\n",
        "random_seed = 2\n",
        "np.random.seed(random_seed)\n",
        "tf.random.set_seed(random_seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YA2sYjiACCBE"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Sampler_IBC():\n",
        "    def __init__(self, lb, ub, cond=None, N_points=100,\n",
        "                 method='sobol', grid=None, split=False, DTYPE='float64'):\n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "        self.cond = cond\n",
        "        self.DTYPE = DTYPE\n",
        "        self.sample(N_points, method, grid, split)\n",
        "\n",
        "    def sample(self, N_points, method, grid, split):\n",
        "        if method == 'uniform':\n",
        "            x_ibc = np.random.uniform(0, 1, size=(N_points, self.ub.shape[0]))\n",
        "            x_ibc = self.lb + (self.ub - self.lb)*x_ibc\n",
        "        elif method == 'latin':\n",
        "            from pyDOE import lhs\n",
        "            x_ibc = self.lb + (self.ub - self.lb)*lhs(self.ub.shape[0],N_points)\n",
        "        elif method == 'sobol':\n",
        "            import sobol\n",
        "            x_ibc = sobol.sample(dimension=self.ub.shape[0], n_points=N_points)\n",
        "            x_ibc = self.lb + (self.ub - self.lb)*x_ibc\n",
        "        elif method == 'equi':\n",
        "            x_ibc = np.linspace(self.lb, self.ub, N_points)\n",
        "        elif method == 'grid':\n",
        "            x_ibc = np.linspace(self.lb, self.ub, N_points).T\n",
        "            temp_final = list()\n",
        "            for val in x_ibc[0]:\n",
        "                temp_final.append( [val] )\n",
        "            dim = 1\n",
        "            while dim < x_ibc.shape[0]:\n",
        "                temp = list()\n",
        "                for t1 in range(x_ibc.shape[1]):\n",
        "                    for t2 in range(len(temp_final)):\n",
        "                        temp_val = temp_final[t2].copy()\n",
        "                        temp_val.append( x_ibc[dim, t1] )\n",
        "                        temp.append( temp_val )\n",
        "                temp_final = temp\n",
        "                dim += 1\n",
        "            x_ibc = np.array(temp_final)\n",
        "        elif method == 'grid_old':\n",
        "            idx = np.random.choice(range(grid.shape[0]),N_points,replace=False)\n",
        "            x_ibc = grid[idx]\n",
        "        if self.cond != None:\n",
        "            y_ibc = self.cond(x_ibc)\n",
        "            self.y = tf.cast(tf.Variable(y_ibc, trainable=False ),\n",
        "                             self.DTYPE)\n",
        "        if split:\n",
        "            x_ibc, t_ibc = x_ibc[:, :-1], x_ibc[:, -1:]\n",
        "            self.t = tf.cast(tf.Variable(t_ibc, trainable=False ),\n",
        "                             self.DTYPE)\n",
        "        self.x = tf.cast(tf.Variable(x_ibc, trainable=False ),\n",
        "                         self.DTYPE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vS9Cq6NGCCBE"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Define global variables\n",
        "r = 0.01\n",
        "T = 3\n",
        "K = 10\n",
        "sigma = [[0.05, 0.01, 0.1], [0.01, 0.06, -0.03], [0.1, -0.03, 0.4]]\n",
        "sigma = np.array(sigma)\n",
        "if np.linalg.det(sigma) != 0:\n",
        "    if np.allclose(sigma, sigma.T):\n",
        "        if np.all(np.linalg.eigvals(sigma) > 0):\n",
        "            print('No problem with Covariance Matrix')\n",
        "        else:\n",
        "            print('Matrix is not positive semidefinite')\n",
        "    else:\n",
        "        print('Matrix is not symmetric')\n",
        "else:\n",
        "    print('Matrix is singular')\n",
        "\n",
        "d = np.array([r]*n_dim) - np.array([0]*n_dim)\n",
        "alphas = np.zeros((n_dim, n_dim))\n",
        "for i in range(n_dim):\n",
        "    alphas[i, i] = np.sum( np.dot(sigma[i], sigma[i]) )\n",
        "    for j in range(i):\n",
        "        alphas[i, j] = alphas[j, i] = np.sum( np.dot(sigma[i], sigma[j]) )\n",
        "DTYPE = 'float32'\n",
        "\n",
        "#Define domain boundaries\n",
        "lb = np.array( ([0.]*n_dim)+[0] )\n",
        "ub = np.array( ([3.*K]*n_dim)+[T] )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZREckeSCCBF"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Problem Definition\n",
        "def h_2(inp):\n",
        "    res = - np.ones( inp.shape[0] )\n",
        "    return res\n",
        "\n",
        "def g(inp):\n",
        "    res = list()\n",
        "    for inp_val in inp:\n",
        "        res.append( np.max([0,\n",
        "                            K*np.exp(-r*(T-inp_val[-1]))-\\\n",
        "                            np.min(inp_val[:-1])]) )\n",
        "    return np.array(res)\n",
        "\n",
        "def u_0(inp):\n",
        "    res = list()\n",
        "    for inp_val in inp:\n",
        "        res.append( np.max([0, K - np.min(inp_val[:-1])]) )\n",
        "    return np.array(res)\n",
        "\n",
        "def s_0(inp):\n",
        "    res = np.ones( (inp.shape[0], n_dim) ) * K\n",
        "    return np.array(res)\n",
        "\n",
        "#Point sampling\n",
        "N_to_sample = 9000\n",
        "\n",
        "#---------------- PDE Conditions\n",
        "print('PDE COnditions\\n')\n",
        "#Initial\n",
        "init_sampler = Sampler_IBC(np.array( ([0.]*n_dim)+[T] ),\n",
        "                           np.array( ([3.*K]*n_dim)+[T] ),\n",
        "                           u_0, 4000, DTYPE=DTYPE )\n",
        "print( f'x: {init_sampler.x.shape}     y: {init_sampler.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "lb_dir = ([0.]*n_dim)+[0]; lb_dir[0] = 3.*K\n",
        "dir_sampler = Sampler_IBC(np.array( lb_dir ),\n",
        "                          np.array( ([3.*K]*n_dim)+[T] ),\n",
        "                          g, N_to_sample//n_dim, DTYPE=DTYPE )\n",
        "for dim in range(1, n_dim):\n",
        "    lb_dir = ([0.]*n_dim)+[0]; lb_dir[dim] = 3.*K\n",
        "    temp_sampler = Sampler_IBC(np.array( lb_dir ),\n",
        "                              np.array( ([3.*K]*n_dim)+[T] ),\n",
        "                              g, N_to_sample//n_dim, DTYPE=DTYPE )\n",
        "    dir_sampler.x = tf.concat([dir_sampler.x, temp_sampler.x], axis=0)\n",
        "    dir_sampler.y = tf.concat([dir_sampler.y, temp_sampler.y], axis=0)\n",
        "print( f'x: {dir_sampler.x.shape}     y: {dir_sampler.y.shape}' )\n",
        "\n",
        "#Neumann\n",
        "print('No')\n",
        "\n",
        "#---------------- Free Boundary Conditions\n",
        "print('\\nFree Boundary COnditions\\n')\n",
        "#Initial\n",
        "init_fb_sampler = Sampler_IBC(np.array([T]), np.array([T]),\n",
        "                              s_0, 1, DTYPE=DTYPE )\n",
        "print( f't: {init_fb_sampler.x.shape}     y: {init_fb_sampler.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "dir_fb_sampler = Sampler_IBC(np.array([0.]), np.array([T]),\n",
        "                             None, N_to_sample, DTYPE=DTYPE )\n",
        "print( f't: {dir_fb_sampler.x.shape}' )\n",
        "\n",
        "#Neumann\n",
        "neu_fb_sampler = Sampler_IBC(np.array([0.]), np.array([T]),\n",
        "                             None, N_to_sample, DTYPE=DTYPE )\n",
        "print( f't: {neu_fb_sampler.x.shape}' )\n",
        "\n",
        "#---------------- Conditions passed to PINN\n",
        "pinn_conditions = {'Initial':init_sampler,\n",
        "                   'Dirichlet':dir_sampler,\n",
        "                   'Neumann':None}\n",
        "fb_conditions = {'Initial':init_fb_sampler,\n",
        "                 'Dirichlet':dir_fb_sampler,\n",
        "                 'Neumann':neu_fb_sampler}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZrRK6KcCCBG"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_sampler = Sampler_IBC(lb, ub, cond=None, DTYPE=DTYPE,\n",
        "                           N_points=1000000, method='uniform', split=True)\n",
        "\n",
        "#Point sampling\n",
        "sample_to_test = 1000\n",
        "\n",
        "#---------------- PDE Conditions\n",
        "print('PDE COnditions\\n')\n",
        "#Initial\n",
        "init_sampler_test = Sampler_IBC(np.array( ([0.]*n_dim)+[T] ),\n",
        "                           np.array( ([3.*K]*n_dim)+[T] ),\n",
        "                           u_0, sample_to_test, DTYPE=DTYPE, method='uniform' )\n",
        "print( f'x: {init_sampler_test.x.shape}     y: {init_sampler_test.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "lb_dir = ([0.]*n_dim)+[0]; lb_dir[0] = 3.*K\n",
        "dir_sampler_test = Sampler_IBC(np.array( lb_dir ),\n",
        "                          np.array( ([3.*K]*n_dim)+[T] ),\n",
        "                          g, sample_to_test//n_dim, DTYPE=DTYPE, method='uniform' )\n",
        "for dim in range(1, n_dim):\n",
        "    lb_dir = ([0.]*n_dim)+[0]; lb_dir[dim] = 3.*K\n",
        "    temp_sampler = Sampler_IBC(np.array( lb_dir ),\n",
        "                              np.array( ([3.*K]*n_dim)+[T] ),\n",
        "                              g, sample_to_test//n_dim, DTYPE=DTYPE, method='uniform' )\n",
        "    dir_sampler_test.x = tf.concat([dir_sampler_test.x, temp_sampler.x], axis=0)\n",
        "    dir_sampler_test.y = tf.concat([dir_sampler_test.y, temp_sampler.y], axis=0)\n",
        "print( f'x: {dir_sampler_test.x.shape}     y: {dir_sampler_test.y.shape}' )\n",
        "\n",
        "#Neumann\n",
        "print('No')\n",
        "\n",
        "#---------------- Free Boundary Conditions\n",
        "print('\\nFree Boundary COnditions\\n')\n",
        "#Initial\n",
        "init_fb_sampler_test = Sampler_IBC(np.array([T]), np.array([T]),\n",
        "                              s_0, 1, DTYPE=DTYPE, method='uniform' )\n",
        "print( f't: {init_fb_sampler_test.x.shape}     y: {init_fb_sampler_test.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "dir_fb_sampler_test = Sampler_IBC(np.array([0.]), np.array([T]),\n",
        "                             None, sample_to_test, DTYPE=DTYPE, method='uniform' )\n",
        "print( f't: {dir_fb_sampler_test.x.shape}' )\n",
        "\n",
        "#Neumann\n",
        "neu_fb_sampler_test = Sampler_IBC(np.array([0.]), np.array([T]),\n",
        "                                  None, sample_to_test//n_dim, DTYPE=DTYPE, method='uniform' )\n",
        "print( f't: {neu_fb_sampler_test.x.shape}' )\n",
        "\n",
        "#---------------- Conditions passed to PINN\n",
        "pinn_cond_test = {'Initial':init_sampler_test,\n",
        "                   'Dirichlet':dir_sampler_test,\n",
        "                   'Neumann':None}\n",
        "fb_cond_test = {'Initial':init_fb_sampler_test,\n",
        "                 'Dirichlet':dir_fb_sampler_test,\n",
        "                 'Neumann':neu_fb_sampler_test}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36b5drG7xuK7"
      },
      "source": [
        "### PINN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-T-oitJxuK8"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Define PDE\n",
        "def pde(tape, vars, fun_val, fo_grads):\n",
        "    second_order = 0\n",
        "    first_order = 0\n",
        "    zero_order = 0\n",
        "    zero_order += tape.gradient(fun_val, vars[-1]) - r * fun_val\n",
        "    for i, var in enumerate(vars[:-1]):\n",
        "        dx_i = fo_grads[i]\n",
        "        first_order += d[i] * var * dx_i\n",
        "        second_order +=\\\n",
        "        tape.gradient(dx_i, var) * (var**2) * alphas[i, i]\n",
        "        for j, var2 in enumerate(vars[:i]):\n",
        "            second_order +=\\\n",
        "            2*( tape.gradient(dx_i, var2) * var * var2 * alphas[i, j])\n",
        "    f = second_order/2 + first_order + zero_order\n",
        "    del(tape)\n",
        "    return f\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz5VbkH6xuK9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow import GradientTape as G_Tape\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.initializers import GlorotNormal, GlorotUniform\n",
        "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
        "from tensorflow.keras.metrics import mean_squared_error\n",
        "\n",
        "class FreeBoundary_PINN():\n",
        "\n",
        "    def __init__(self, params, pde, ibc_cond, ibc_fb_cond, lb, ub,\n",
        "                 N_f=10000, N_fb_ibc=150, DTYPE='float64', coll_points=None):\n",
        "        self.params = params\n",
        "        self.DTYPE = DTYPE\n",
        "        self.Default_Params()\n",
        "        #Set seed\n",
        "        if self.params['seed'] != None:\n",
        "            tf.keras.utils.set_random_seed( self.params['seed'] )\n",
        "            tf.config.experimental.enable_op_determinism()\n",
        "        #Define pde\n",
        "        self.pde = pde\n",
        "        #Define intial and boundary conditions\n",
        "        self.ibc_cond = ibc_cond\n",
        "        self.ibc_fb_cond = ibc_fb_cond\n",
        "        #All needed for points sampling\n",
        "        self.lb = tf.Variable(lb, trainable=False)\n",
        "        self.ub = tf.Variable(ub, trainable=False)\n",
        "        if coll_points == None:\n",
        "            self.N_f = N_f\n",
        "            self.Sample_Points()\n",
        "        else:\n",
        "            self.x_f = coll_points[0]\n",
        "            self.t_f = coll_points[1]\n",
        "        self.N_fb_ibc = N_fb_ibc\n",
        "        #Initialize the class: define the network\n",
        "        self.Define_Regularizer()\n",
        "        self.Define_Initializer()\n",
        "        self.Define_Optimizer()\n",
        "        self.Create_Network()\n",
        "        self.Create_FB_Network()\n",
        "\n",
        "    def Default_Params(self):\n",
        "        target = self.params.keys()\n",
        "        if 'seed' not in target:\n",
        "            self.params['seed'] = None\n",
        "        if 'optimizer' not in target:\n",
        "            self.params['optimizer'] = 'Adam'\n",
        "        if 'fb_optimizer' not in target:\n",
        "            self.params['fb_optimizer'] = 'Adam'\n",
        "        if 'reg_type' not in target:\n",
        "            self.params['reg_type'] = None\n",
        "        if 'initializer' not in target:\n",
        "            self.params['initializer'] = 'glorot_normal'\n",
        "        if 'activation' not in target:\n",
        "            self.params['activation'] = 'tanh'\n",
        "        if 'output_act' not in target:\n",
        "            self.params['output_act'] = 'linear'\n",
        "        if 'pde_weight' not in target:\n",
        "            self.params['pde_weight'] = 1.\n",
        "        if 'sup_weight' not in target:\n",
        "            self.params['sup_weight'] = [1., 1., 1.]\n",
        "        if 'fb_weight' not in target:\n",
        "            self.params['fb_weight'] = [1., 1., 1.]\n",
        "        if 'patience' not in target:\n",
        "            self.params['patience'] = np.inf\n",
        "        if 'sample_method' not in target:\n",
        "            self.params['sample_method'] = 'uniform'\n",
        "        if 'fb_output_act' not in target:\n",
        "            self.params['fb_output_act'] = 'linear'\n",
        "        if 'fb_activation' not in target:\n",
        "            self.params['fb_activation'] = 'tanh'\n",
        "        if 'verbose' not in target:\n",
        "            self.params['verbose'] = 1\n",
        "        if 'steps_fb_per_pde' not in target:\n",
        "            self.params['steps_fb_per_pde'] = 1\n",
        "        if 'fb_freezing' not in target:\n",
        "            self.params['fb_freezing'] = None\n",
        "\n",
        "    def Define_Regularizer(self):\n",
        "        if self.params['reg_type'] == 'l1':\n",
        "            self.regularizer = l1( self.params['reg'] )\n",
        "        elif self.params['reg_type'] == 'l2':\n",
        "            self.regularizer = l2( self.params['reg'] )\n",
        "        elif self.params['reg_type'] == 'l1_l2':\n",
        "            self.regularizer = l1_l2( self.params['reg'][0],\n",
        "                                      self.params['reg'][1] )\n",
        "        else:\n",
        "            self.regularizer = None\n",
        "\n",
        "    def Define_Initializer(self):\n",
        "        if self.params['initializer'] == 'glorot_normal':\n",
        "            self.initializer = GlorotNormal()\n",
        "        elif self.params['initializer'] == 'glorot_uniform':\n",
        "            self.initializer = GlorotUniform()\n",
        "        else:\n",
        "            self.initializer = None\n",
        "\n",
        "    def Define_Optimizer(self):\n",
        "        temp = self.params['optimizer']\n",
        "        if temp.lower() == 'adam':\n",
        "            self.opt = Adam( self.params['lr'] )\n",
        "        elif temp.lower() == 'rmsprop':\n",
        "            self.opt = RMSprop( self.params['lr'] )\n",
        "        else:\n",
        "            raise ValueError(f\"Optimizer {temp} not recognized\")\n",
        "\n",
        "        temp = self.params['fb_optimizer']\n",
        "        if temp.lower() == 'adam':\n",
        "            self.fb_opt = Adam( self.params['fb_lr'] )\n",
        "        elif temp.lower() == 'rmsprop':\n",
        "            self.fb_opt = RMSprop( self.params['fb_lr'] )\n",
        "        else:\n",
        "            raise ValueError(f\"fb_Optimizer {temp} not recognized\")\n",
        "\n",
        "    def Create_Network(self):\n",
        "        input_layer = Input(shape=self.params['layers'][0],\n",
        "                            name = 'Input')\n",
        "        x = Dense(units=self.params['layers'][1],\n",
        "                  activation=self.params['activation'],\n",
        "                  kernel_initializer=self.initializer,\n",
        "                  kernel_regularizer=self.regularizer,\n",
        "                  name='Dense_1')(input_layer)\n",
        "        for layer in range(2, len(self.params['layers'])-1):\n",
        "            x = Dense(units=self.params['layers'][layer],\n",
        "                      activation=self.params['activation'],\n",
        "                      kernel_initializer=self.initializer,\n",
        "                      kernel_regularizer=self.regularizer,\n",
        "                      name=f'Dense_{layer}')(x)\n",
        "        output = Dense(units=self.params['layers'][-1],\n",
        "                       activation=self.params['output_act'],\n",
        "                       kernel_initializer=self.initializer,\n",
        "                       kernel_regularizer=self.regularizer,\n",
        "                       name='Output')(x)\n",
        "        self.mdl = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "    def Create_FB_Network(self):\n",
        "        input_layer = Input(shape=self.params['fb_layers'][0],\n",
        "                            name = 'Input')\n",
        "        x = Dense(units=self.params['fb_layers'][1],\n",
        "                  activation=self.params['fb_activation'],\n",
        "                  kernel_initializer=self.initializer,\n",
        "                  kernel_regularizer=self.regularizer,\n",
        "                  name='Dense_1')(input_layer)\n",
        "        for layer in range(2, len(self.params['fb_layers'])-1):\n",
        "            x = Dense(units=self.params['fb_layers'][layer],\n",
        "                      activation=self.params['fb_activation'],\n",
        "                      kernel_initializer=self.initializer,\n",
        "                      kernel_regularizer=self.regularizer,\n",
        "                      name=f'Dense_{layer}')(x)\n",
        "        output = Dense(units=self.params['fb_layers'][-1],\n",
        "                       activation=self.params['fb_output_act'],\n",
        "                       kernel_initializer=self.initializer,\n",
        "                       kernel_regularizer=self.regularizer,\n",
        "                       name='Output')(x)\n",
        "        self.fb = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "    def Sample_Points(self):\n",
        "        #According to the selected method, sample collocation points\n",
        "        method = self.params['sample_method']\n",
        "        if method == 'latin':\n",
        "            from pyDOE import lhs\n",
        "            cps = self.lb + (self.ub - self.lb)*lhs(self.ub.shape[0], self.N_f)\n",
        "        elif method == 'uniform':\n",
        "            cps = np.random.uniform(0, 1, size=(self.N_f, self.ub.shape[0]))\n",
        "            cps = self.lb + (self.ub - self.lb)*cps\n",
        "        elif method == 'sobol':\n",
        "            import sobol\n",
        "            cps = sobol.sample(dimension=self.ub.shape[0], n_points=self.N_f)\n",
        "            cps = self.lb + (self.ub - self.lb)*cps\n",
        "        else:\n",
        "            raise ValueError(f'Sampling method {method} not recognized')\n",
        "        #Return collocation points as tf tensors\n",
        "        self.x_f_total = tf.cast(tf.Variable(cps[:, :-1], trainable=False),\n",
        "                           self.DTYPE)\n",
        "        self.t_f_total = tf.cast(tf.Variable(cps[:, -1:], trainable=False),\n",
        "                           self.DTYPE)\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self):\n",
        "        with G_Tape(watch_accessed_variables=False) as mdl_tape:\n",
        "            #Watch solution weights\n",
        "            mdl_tape.watch(self.mdl.trainable_variables)\n",
        "            #--------------- Compute Free Boundary losses\n",
        "            with G_Tape(watch_accessed_variables=False) as fb_tape:\n",
        "                #Watch free boundary weights\n",
        "                fb_tape.watch(self.fb.trainable_variables)\n",
        "                #Compute Initial Free Boundary Condition\n",
        "                fb_init = self.fb(self.ibc_fb_cond['Initial'].x,\n",
        "                                      training=True)\n",
        "                fb_init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_init-self.ibc_fb_cond['Initial'].y)\n",
        "                    )\n",
        "                #Compute Dirichlet Free Boundary Condition\n",
        "                if self.ibc_fb_cond['Dirichlet'] != None:\n",
        "                    #Compute Free Boundary values\n",
        "                    s_values = self.fb(self.ibc_fb_cond['Dirichlet'].x,\n",
        "                                       training=True)\n",
        "                    fb_dc = self.mdl(tf.concat(\n",
        "                        [s_values,\n",
        "                         self.ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                         training=True)\n",
        "                    fb_dir_target = tf.nn.relu(\n",
        "                        tf.ones_like(\n",
        "                            s_values[:,-1]\n",
        "                            ) * K - tf.math.reduce_min(s_values, axis=1)\n",
        "                    )\n",
        "                    fb_dir_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(fb_dc - fb_dir_target)\n",
        "                        )\n",
        "                else:\n",
        "                    fb_dir_loss = 0\n",
        "                #Compute Neumann Free Boundary Condition\n",
        "                if self.ibc_fb_cond['Neumann'] != None:\n",
        "                    s_values = self.fb(self.ibc_fb_cond['Neumann'].x,\n",
        "                                       training=True)\n",
        "                    with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                        neu_fb_tape.watch(s_values)\n",
        "                        pinn_nc_fb = self.mdl(\n",
        "                            tf.concat([s_values,\n",
        "                                       self.ibc_fb_cond['Neumann'].x],\n",
        "                                      axis=1),\n",
        "                                      training=True)\n",
        "                    pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                                      s_values)\n",
        "                    fb_neu_target = -tf.one_hot(\n",
        "                        tf.math.argmin( s_values, axis=1),\n",
        "                        depth=n_dim\n",
        "                        )\n",
        "                    fb_neu_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(pinn_nc_fb-fb_neu_target)\n",
        "                        )\n",
        "                else:\n",
        "                    fb_neu_loss = 0\n",
        "                #Compute final loss\n",
        "                fb_loss = self.params['fb_weight'][0] * fb_init_loss +\\\n",
        "                self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "                self.params['fb_weight'][2] * fb_neu_loss\n",
        "            #Compute gradient and apply optimizers for free boundary\n",
        "            gradient_fb = fb_tape.gradient(fb_loss,\n",
        "                                          self.fb.trainable_variables)\n",
        "            self.fb_opt.apply_gradients( zip(gradient_fb,\n",
        "                                          self.fb.trainable_variables) )\n",
        "\n",
        "            #--------------- Compute PINN losses\n",
        "            #Compute unsupervised loss\n",
        "            s_values = self.fb(tf.concat([self.t_f_total], axis=-1),\n",
        "                               training=False)\n",
        "            temp = tf.reduce_sum(tf.cast(self.x_f_total < s_values,\n",
        "                                         dtype=self.DTYPE),\n",
        "                                axis=-1) < tf.ones(self.t_f_total.shape[0])\n",
        "            x_f = self.x_f_total[ temp ]\n",
        "            t_f = tf.reshape(self.t_f_total[ temp ], (-1,1) )\n",
        "            variables = list()\n",
        "            derivatives = list()\n",
        "            for i in range(x_f.shape[1]):\n",
        "                variables.append( x_f[:, i:i+1] )\n",
        "            variables.append( t_f )\n",
        "            with G_Tape(persistent=True,\n",
        "                        watch_accessed_variables=False) as pinn_tape:\n",
        "                #Watch independet variables\n",
        "                for var in variables:\n",
        "                    pinn_tape.watch( var )\n",
        "                pinn_tape.watch(t_f)\n",
        "                u_val = self.mdl(tf.concat(variables, axis=1),\n",
        "                                    training=True)\n",
        "                u_x = pinn_tape.gradient(u_val, x_f)\n",
        "                for i, var in enumerate(variables[:-1]):\n",
        "                    derivatives.append( pinn_tape.gradient(u_val, var) )\n",
        "            unsup_loss = tf.reduce_mean(tf.square(\n",
        "                self.pde(pinn_tape, variables, u_val, derivatives) ))\n",
        "            #Compute Initial Condition\n",
        "            pinn_init = self.mdl(self.ibc_cond['Initial'].x,\n",
        "                                training=True)\n",
        "            if len(pinn_init.shape) > 1:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  tf.expand_dims(self.ibc_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  self.ibc_cond['Initial'].y )\n",
        "                    )\n",
        "            #Compute Dirichlet Boundary Condition\n",
        "            if self.ibc_cond['Dirichlet'] != None:\n",
        "                pinn_dc = self.mdl(self.ibc_cond['Dirichlet'].x,\n",
        "                                    training=True)\n",
        "                if len(pinn_dc.shape) > 1:\n",
        "                    dir_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(pinn_dc -\\\n",
        "                                      tf.expand_dims(self.ibc_cond['Dirichlet'].y,\n",
        "                                                     axis=-1 ) )\n",
        "                        )\n",
        "                else:\n",
        "                    dir_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(pinn_dc -\\\n",
        "                                      self.ibc_cond['Dirichlet'].y )\n",
        "                        )\n",
        "            else:\n",
        "                dir_loss = 0\n",
        "            #Compute Neumann Boundary Condition\n",
        "            if self.ibc_cond['Neumann'] != None:\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_tape:\n",
        "                    neu_tape.watch(self.ibc_cond['Neumann'].x)\n",
        "                    pinn_nc = self.mdl(tf.concat([self.ibc_cond['Neumann'].x,\n",
        "                                                  self.ibc_cond['Neumann'].t],\n",
        "                                                axis=1),\n",
        "                                        training=True)\n",
        "                pinn_nc = neu_tape.gradient(pinn_nc, self.ibc_cond['Neumann'].x)\n",
        "                neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc-self.ibc_cond['Neumann'].y)\n",
        "                    )\n",
        "            else:\n",
        "                neu_loss = 0\n",
        "\n",
        "            #--------------- Compute total loss\n",
        "            pinn_loss = (self.params['pde_weight'] * unsup_loss) +\\\n",
        "            self.params['sup_weight'][0] * init_loss +\\\n",
        "            self.params['sup_weight'][1] * dir_loss +\\\n",
        "            self.params['sup_weight'][2] * neu_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "        #Compute gradient and apply optimizers\n",
        "        gradient = mdl_tape.gradient(pinn_loss,self.mdl.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(gradient,self.mdl.trainable_variables))\n",
        "        #Return losses\n",
        "        final_fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "        final_pinn_loss = unsup_loss + init_loss +\\\n",
        "         dir_loss + neu_loss + fb_dir_loss + fb_neu_loss\n",
        "        return (unsup_loss, init_loss, dir_loss, neu_loss, fb_init_loss,\n",
        "                fb_dir_loss, fb_neu_loss, final_fb_loss, final_pinn_loss)\n",
        "\n",
        "    @tf.function\n",
        "    def train_mdl_solo(self):\n",
        "        with G_Tape(watch_accessed_variables=False) as mdl_tape:\n",
        "            #Watch solution weights\n",
        "            mdl_tape.watch(self.mdl.trainable_variables)\n",
        "            #--------------- Compute Free Boundary losses\n",
        "            #Compute Initial Free Boundary Condition\n",
        "            fb_init = self.fb(self.ibc_fb_cond['Initial'].x,\n",
        "                                  training=True)\n",
        "            fb_init_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(fb_init-self.ibc_fb_cond['Initial'].y)\n",
        "                )\n",
        "            #Compute Dirichlet Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Dirichlet'] != None:\n",
        "                #Compute Free Boundary values\n",
        "                s_values = self.fb(self.ibc_fb_cond['Dirichlet'].x,\n",
        "                                    training=True)\n",
        "                fb_dc = self.mdl(tf.concat(\n",
        "                    [s_values,\n",
        "                      self.ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                      training=True)\n",
        "                fb_dir_target = tf.nn.relu(\n",
        "                    tf.ones_like(\n",
        "                        s_values[:,-1]\n",
        "                        ) * K - tf.math.reduce_min(s_values, axis=1)\n",
        "                )\n",
        "                fb_dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_dc - fb_dir_target)\n",
        "                    )\n",
        "            else:\n",
        "                fb_dir_loss = 0\n",
        "            #Compute Neumann Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Neumann'] != None:\n",
        "                s_values = self.fb(self.ibc_fb_cond['Neumann'].x,\n",
        "                                    training=True)\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                    neu_fb_tape.watch(s_values)\n",
        "                    pinn_nc_fb = self.mdl(\n",
        "                        tf.concat([s_values,\n",
        "                                    self.ibc_fb_cond['Neumann'].x],\n",
        "                                  axis=1),\n",
        "                                  training=True)\n",
        "                pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                                  s_values)\n",
        "                fb_neu_target = -tf.one_hot(\n",
        "                    tf.math.argmin( s_values, axis=1),\n",
        "                    depth=n_dim\n",
        "                    )\n",
        "                fb_neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc_fb-fb_neu_target)\n",
        "                    )\n",
        "            else:\n",
        "                fb_neu_loss = 0\n",
        "            #Compute final loss\n",
        "            fb_loss = self.params['fb_weight'][0] * fb_init_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "\n",
        "            #--------------- Compute PINN losses\n",
        "            #Compute unsupervised loss\n",
        "            s_values = self.fb(tf.concat([self.t_f_total], axis=-1),\n",
        "                               training=False)\n",
        "            temp = tf.reduce_sum(tf.cast(self.x_f_total < s_values,\n",
        "                                         dtype=self.DTYPE),\n",
        "                                axis=-1) < tf.ones(self.t_f_total.shape[0])\n",
        "            x_f = self.x_f_total[ temp ]\n",
        "            t_f = tf.reshape(self.t_f_total[ temp ], (-1,1) )\n",
        "            variables = list()\n",
        "            derivatives = list()\n",
        "            for i in range(x_f.shape[1]):\n",
        "                variables.append( x_f[:, i:i+1] )\n",
        "            variables.append( t_f )\n",
        "            with G_Tape(persistent=True,\n",
        "                        watch_accessed_variables=False) as pinn_tape:\n",
        "                #Watch independet variables\n",
        "                for var in variables:\n",
        "                    pinn_tape.watch( var )\n",
        "                pinn_tape.watch(t_f)\n",
        "                u_val = self.mdl(tf.concat(variables, axis=1),\n",
        "                                    training=True)\n",
        "                u_x = pinn_tape.gradient(u_val, x_f)\n",
        "                for i, var in enumerate(variables[:-1]):\n",
        "                    derivatives.append( pinn_tape.gradient(u_val, var) )\n",
        "            unsup_loss = tf.reduce_mean(tf.square(\n",
        "                self.pde(pinn_tape, variables, u_val, derivatives) ))\n",
        "            #Compute Initial Condition\n",
        "            pinn_init = self.mdl(self.ibc_cond['Initial'].x,\n",
        "                                training=True)\n",
        "            if len(pinn_init.shape) > 1:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  tf.expand_dims(self.ibc_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  self.ibc_cond['Initial'].y )\n",
        "                    )\n",
        "            #Compute Dirichlet Boundary Condition\n",
        "            if self.ibc_cond['Dirichlet'] != None:\n",
        "                pinn_dc = self.mdl(self.ibc_cond['Dirichlet'].x,\n",
        "                                    training=True)\n",
        "                if len(pinn_dc.shape) > 1:\n",
        "                    dir_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(pinn_dc -\\\n",
        "                                      tf.expand_dims(self.ibc_cond['Dirichlet'].y,\n",
        "                                                     axis=-1 ) )\n",
        "                        )\n",
        "                else:\n",
        "                    dir_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(pinn_dc -\\\n",
        "                                      self.ibc_cond['Dirichlet'].y )\n",
        "                        )\n",
        "            else:\n",
        "                dir_loss = 0\n",
        "            #Compute Neumann Boundary Condition\n",
        "            if self.ibc_cond['Neumann'] != None:\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_tape:\n",
        "                    neu_tape.watch(self.ibc_cond['Neumann'].x)\n",
        "                    pinn_nc = self.mdl(tf.concat([self.ibc_cond['Neumann'].x,\n",
        "                                                  self.ibc_cond['Neumann'].t],\n",
        "                                                axis=1),\n",
        "                                        training=True)\n",
        "                pinn_nc = neu_tape.gradient(pinn_nc, self.ibc_cond['Neumann'].x)\n",
        "                neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc-self.ibc_cond['Neumann'].y)\n",
        "                    )\n",
        "            else:\n",
        "                neu_loss = 0\n",
        "\n",
        "            #--------------- Compute total loss\n",
        "            pinn_loss = (self.params['pde_weight'] * unsup_loss) +\\\n",
        "            self.params['sup_weight'][0] * init_loss +\\\n",
        "            self.params['sup_weight'][1] * dir_loss +\\\n",
        "            self.params['sup_weight'][2] * neu_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "        #Compute gradient and apply optimizers\n",
        "        gradient = mdl_tape.gradient(pinn_loss,self.mdl.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(gradient,self.mdl.trainable_variables))\n",
        "        #Return losses\n",
        "        final_fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "        final_pinn_loss = unsup_loss + init_loss +\\\n",
        "         dir_loss + neu_loss + fb_dir_loss + fb_neu_loss\n",
        "        return (unsup_loss, init_loss, dir_loss, neu_loss, fb_init_loss,\n",
        "                fb_dir_loss, fb_neu_loss, final_fb_loss, final_pinn_loss)\n",
        "\n",
        "    @tf.function\n",
        "    def train_fb_solo(self):\n",
        "        #--------------- Compute Free Boundary losses\n",
        "        with G_Tape(watch_accessed_variables=False) as fb_tape:\n",
        "            #Watch free boundary weights\n",
        "            fb_tape.watch(self.fb.trainable_variables)\n",
        "            #Compute Initial Free Boundary Condition\n",
        "            fb_init = self.fb(self.ibc_fb_cond['Initial'].x,\n",
        "                                  training=True)\n",
        "            fb_init_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(fb_init-self.ibc_fb_cond['Initial'].y)\n",
        "                )\n",
        "            #Compute Dirichlet Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Dirichlet'] != None:\n",
        "                #Compute Free Boundary values\n",
        "                s_values = self.fb(self.ibc_fb_cond['Dirichlet'].x,\n",
        "                                    training=True)\n",
        "                fb_dc = self.mdl(tf.concat(\n",
        "                    [s_values,\n",
        "                      self.ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                      training=True)\n",
        "                fb_dir_target = tf.nn.relu(\n",
        "                    tf.ones_like(\n",
        "                        s_values[:,-1]\n",
        "                        ) * K - tf.math.reduce_min(s_values, axis=1)\n",
        "                )\n",
        "                fb_dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_dc - fb_dir_target)\n",
        "                    )\n",
        "            else:\n",
        "                fb_dir_loss = 0\n",
        "            #Compute Neumann Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Neumann'] != None:\n",
        "                s_values = self.fb(self.ibc_fb_cond['Neumann'].x,\n",
        "                                    training=True)\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                    neu_fb_tape.watch(s_values)\n",
        "                    pinn_nc_fb = self.mdl(\n",
        "                        tf.concat([s_values,\n",
        "                                    self.ibc_fb_cond['Neumann'].x],\n",
        "                                  axis=1),\n",
        "                                  training=True)\n",
        "                pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                                  s_values)\n",
        "                fb_neu_target = -tf.one_hot(\n",
        "                    tf.math.argmin( s_values, axis=1),\n",
        "                    depth=n_dim\n",
        "                    )\n",
        "                fb_neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc_fb-fb_neu_target)\n",
        "                    )\n",
        "            else:\n",
        "                fb_neu_loss = 0\n",
        "            #Compute final loss\n",
        "            fb_loss = self.params['fb_weight'][0] * fb_init_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "        #Compute gradient and apply optimizers for free boundary\n",
        "        gradient_fb = fb_tape.gradient(fb_loss,\n",
        "                                      self.fb.trainable_variables)\n",
        "        self.fb_opt.apply_gradients( zip(gradient_fb,\n",
        "                                      self.fb.trainable_variables) )\n",
        "        #Return losses\n",
        "        final_fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "        return fb_init_loss, fb_dir_loss, fb_neu_loss, final_fb_loss\n",
        "\n",
        "    def fit(self, wandb_run=None):\n",
        "        #Early warning initialization\n",
        "        self.early_warning = {'Target':np.inf,\n",
        "                              'n_steps':0,\n",
        "                              'top_mdl':None,\n",
        "                              'weights':None}\n",
        "        old_top_mdl, old_weights = None, None\n",
        "        #Training\n",
        "        self.u_losses, self.i_losses = list(), list()\n",
        "        self.d_losses, self.n_losses = list(), list()\n",
        "        self.b_i_losses = list()\n",
        "        self.b_d_losses, self.b_n_losses = list(), list()\n",
        "        self.b_losses, self.p_losses = list(), list()\n",
        "        print_base = \"{:<10}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}\"\n",
        "        if self.params['fb_freezing'] == None:\n",
        "            for epoch in tqdm(range(self.params['epochs']),\n",
        "                              desc='PINNs - Training'):\n",
        "                if epoch == 0:\n",
        "                    print(print_base.format('Epoch', 'Unsupervised', 'Initial',\n",
        "                                            'Dirichlet', 'Neumann',\n",
        "                                            'FB_Init', 'FB_Dir', 'FB_Neu',\n",
        "                                            'Free Boundary', 'Total'))\n",
        "                    print('\\n')\n",
        "                #Case 1: more mdl steps for a single fb step\n",
        "                for _ in range(self.params['steps_fb_per_pde'] -1):\n",
        "                    self.train_mdl_solo();\n",
        "                #Case 2: more fb steps for a single mdl step\n",
        "                for _ in range(0, self.params['steps_fb_per_pde'] +1, -1):\n",
        "                    self.train_fb_solo();\n",
        "                u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l =\\\n",
        "                self.train_step()\n",
        "                u_l, i_l = np.array(u_l), np.array(i_l)\n",
        "                d_l, n_l = np.array(d_l), np.array(n_l)\n",
        "                b_i_l = np.array(b_i_l)\n",
        "                b_d_l, b_n_l = np.array(b_d_l), np.array(b_n_l)\n",
        "                b_l, p_l = np.array(b_l), np.array(p_l)\n",
        "                if epoch % self.params['verbose'] == 0:\n",
        "                    print(print_base.format(epoch+1, format(u_l, '.20f')[:10],\n",
        "                                            format(i_l, '.20f')[:10],\n",
        "                                            format(d_l, '.20f')[:10],\n",
        "                                            format(n_l, '.20f')[:10],\n",
        "                                            format(b_i_l, '.20f')[:10],\n",
        "                                            format(b_d_l, '.20f')[:10],\n",
        "                                            format(b_n_l, '.20f')[:10],\n",
        "                                            format(b_l, '.20f')[:10],\n",
        "                                            format(p_l, '.20f')[:10]))\n",
        "                self.u_losses.append(u_l)\n",
        "                self.i_losses.append(i_l)\n",
        "                self.d_losses.append(d_l)\n",
        "                self.n_losses.append(n_l)\n",
        "                self.b_i_losses.append(b_i_l)\n",
        "                self.b_d_losses.append(b_d_l)\n",
        "                self.b_n_losses.append(b_d_l)\n",
        "                self.b_losses.append(b_l)\n",
        "                self.p_losses.append(p_l)\n",
        "                #Eventually, update wandb\n",
        "                if wandb_run != None:\n",
        "                    wandb_run.log({'Unsupervised':u_l,\n",
        "                                  'Supervised':i_l + d_l + n_l,\n",
        "                                  'Total':p_l,\n",
        "                                   'Free Boundary':b_l,\n",
        "                                   'Initial':i_l,\n",
        "                                   'Dirichlet':d_l,\n",
        "                                   'Neumann':n_l,\n",
        "                                   'FB_Init':b_i_l,\n",
        "                                   'FB_Dir':b_d_l,\n",
        "                                   'FB_Neu':b_n_l})\n",
        "                #Check for the early warning\n",
        "                if p_l <= self.early_warning['Target']:\n",
        "                    self.early_warning['Target'] = p_l\n",
        "                    self.early_warning['n_steps'] = 0\n",
        "                    self.early_warning['top_mdl'] = old_top_mdl\n",
        "                    self.early_warning['weights'] = old_weights\n",
        "                else:\n",
        "                    self.early_warning['n_steps'] += 1\n",
        "                    if self.early_warning['n_steps'] >= self.params['patience']:\n",
        "                        break\n",
        "                #Save model and weights for next step early warning\n",
        "                old_top_mdl = tf.keras.models.clone_model( self.mdl )\n",
        "                old_weights = self.mdl.get_weights()\n",
        "        else:\n",
        "            #Before freezing, both mdl and fb are training\n",
        "            for epoch in tqdm(range(self.params['fb_freezing']),\n",
        "                              desc='PINNs - Training'):\n",
        "                if epoch == 0:\n",
        "                    print(print_base.format('Epoch', 'Unsupervised', 'Initial',\n",
        "                                            'Dirichlet', 'Neumann',\n",
        "                                            'FB_Init', 'FB_Dir', 'FB_Neu',\n",
        "                                            'Free Boundary', 'Total'))\n",
        "                    print('\\n')\n",
        "                #Case 1: more mdl steps for a single fb step\n",
        "                for _ in range(self.params['steps_fb_per_pde'] -1):\n",
        "                    self.train_mdl_solo();\n",
        "                #Case 2: more fb steps for a single mdl step\n",
        "                for _ in range(0, self.params['steps_fb_per_pde'] +1, -1):\n",
        "                    self.train_fb_solo();\n",
        "                u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l =\\\n",
        "                self.train_step()\n",
        "                u_l, i_l = np.array(u_l), np.array(i_l)\n",
        "                d_l, n_l = np.array(d_l), np.array(n_l)\n",
        "                b_i_l = np.array(b_i_l)\n",
        "                b_d_l, b_n_l = np.array(b_d_l), np.array(b_n_l)\n",
        "                b_l, p_l = np.array(b_l), np.array(p_l)\n",
        "                if epoch % self.params['verbose'] == 0:\n",
        "                    print(print_base.format(epoch+1, format(u_l, '.20f')[:10],\n",
        "                                            format(i_l, '.20f')[:10],\n",
        "                                            format(d_l, '.20f')[:10],\n",
        "                                            format(n_l, '.20f')[:10],\n",
        "                                            format(b_i_l, '.20f')[:10],\n",
        "                                            format(b_d_l, '.20f')[:10],\n",
        "                                            format(b_n_l, '.20f')[:10],\n",
        "                                            format(b_l, '.20f')[:10],\n",
        "                                            format(p_l, '.20f')[:10]))\n",
        "                self.u_losses.append(u_l)\n",
        "                self.i_losses.append(i_l)\n",
        "                self.d_losses.append(d_l)\n",
        "                self.n_losses.append(n_l)\n",
        "                self.b_i_losses.append(b_i_l)\n",
        "                self.b_d_losses.append(b_d_l)\n",
        "                self.b_n_losses.append(b_d_l)\n",
        "                self.b_losses.append(b_l)\n",
        "                self.p_losses.append(p_l)\n",
        "                #Eventually, update wandb\n",
        "                if wandb_run != None:\n",
        "                    wandb_run.log({'Unsupervised':u_l,\n",
        "                                  'Supervised':i_l + d_l + n_l,\n",
        "                                  'Total':p_l,\n",
        "                                   'Free Boundary':b_l,\n",
        "                                   'Initial':i_l,\n",
        "                                   'Dirichlet':d_l,\n",
        "                                   'Neumann':n_l,\n",
        "                                   'FB_Init':b_i_l,\n",
        "                                   'FB_Dir':b_d_l,\n",
        "                                   'FB_Neu':b_n_l})\n",
        "                #Check for the early warning\n",
        "                if p_l <= self.early_warning['Target']:\n",
        "                    self.early_warning['Target'] = p_l\n",
        "                    self.early_warning['n_steps'] = 0\n",
        "                    self.early_warning['top_mdl'] = old_top_mdl\n",
        "                    self.early_warning['weights'] = old_weights\n",
        "                else:\n",
        "                    self.early_warning['n_steps'] += 1\n",
        "                    if self.early_warning['n_steps'] >= self.params['patience']:\n",
        "                        break\n",
        "                #Save model and weights for next step early warning\n",
        "                old_top_mdl = tf.keras.models.clone_model( self.mdl )\n",
        "                old_weights = self.mdl.get_weights()\n",
        "            #Now, freeze fb and train only mdl\n",
        "            for epoch in tqdm(range(self.params['fb_freezing'],\n",
        "                                    self.params['epochs']),\n",
        "                              desc='PINNs - Training; Free Boundary Fixed'):\n",
        "                u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l =\\\n",
        "                self.train_mdl_solo()\n",
        "                u_l, i_l = np.array(u_l), np.array(i_l)\n",
        "                d_l, n_l = np.array(d_l), np.array(n_l)\n",
        "                b_i_l = np.array(b_i_l)\n",
        "                b_d_l, b_n_l = np.array(b_d_l), np.array(b_n_l)\n",
        "                b_l, p_l = np.array(b_l), np.array(p_l)\n",
        "                if epoch % self.params['verbose'] == 0:\n",
        "                    print(print_base.format(epoch+1, format(u_l, '.20f')[:10],\n",
        "                                            format(i_l, '.20f')[:10],\n",
        "                                            format(d_l, '.20f')[:10],\n",
        "                                            format(n_l, '.20f')[:10],\n",
        "                                            format(b_i_l, '.20f')[:10],\n",
        "                                            format(b_d_l, '.20f')[:10],\n",
        "                                            format(b_n_l, '.20f')[:10],\n",
        "                                            format(b_l, '.20f')[:10],\n",
        "                                            format(p_l, '.20f')[:10]))\n",
        "                self.u_losses.append(u_l)\n",
        "                self.i_losses.append(i_l)\n",
        "                self.d_losses.append(d_l)\n",
        "                self.n_losses.append(n_l)\n",
        "                self.b_i_losses.append(b_i_l)\n",
        "                self.b_d_losses.append(b_d_l)\n",
        "                self.b_n_losses.append(b_d_l)\n",
        "                self.b_losses.append(b_l)\n",
        "                self.p_losses.append(p_l)\n",
        "                #Eventually, update wandb\n",
        "                if wandb_run != None:\n",
        "                    wandb_run.log({'Unsupervised':u_l,\n",
        "                                  'Supervised':i_l + d_l + n_l,\n",
        "                                  'Total':p_l,\n",
        "                                   'Free Boundary':b_l,\n",
        "                                   'Initial':i_l,\n",
        "                                   'Dirichlet':d_l,\n",
        "                                   'Neumann':n_l,\n",
        "                                   'FB_Init':b_i_l,\n",
        "                                   'FB_Dir':b_d_l,\n",
        "                                   'FB_Neu':b_n_l})\n",
        "                #Check for the early warning\n",
        "                if p_l <= self.early_warning['Target']:\n",
        "                    self.early_warning['Target'] = p_l\n",
        "                    self.early_warning['n_steps'] = 0\n",
        "                    self.early_warning['top_mdl'] = old_top_mdl\n",
        "                    self.early_warning['weights'] = old_weights\n",
        "                else:\n",
        "                    self.early_warning['n_steps'] += 1\n",
        "                    if self.early_warning['n_steps'] >= self.params['patience']:\n",
        "                        break\n",
        "                #Save model and weights for next step early warning\n",
        "                old_top_mdl = tf.keras.models.clone_model( self.mdl )\n",
        "                old_weights = self.mdl.get_weights()\n",
        "        #Recover information about optimal epoch in early warning\n",
        "        self.mdl = tf.keras.models.clone_model( self.early_warning['top_mdl'] )\n",
        "        self.mdl.set_weights(self.early_warning['weights'])\n",
        "        top_epoch = epoch+1 - self.early_warning[\"n_steps\"]\n",
        "        print(f'Best loss achieved at step {top_epoch}')\n",
        "\n",
        "    def plot_losses(self):\n",
        "        plt.figure( figsize=(12,8) )\n",
        "        plt.semilogy(self.u_losses, label='Unsupervised')\n",
        "        plt.semilogy(np.array(self.i_losses) +\\\n",
        "                     np.array(self.d_losses) +\\\n",
        "                     np.array(self.n_losses),\n",
        "                     label='Supervised')\n",
        "        plt.semilogy(self.b_losses, label='Free Boundary')\n",
        "        plt.semilogy(self.p_losses, label='PINN')\n",
        "        plt.legend()\n",
        "        plt.title('Losses')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_unsupervised_test(self, test_sampler, test_ibc_cond,\n",
        "                               test_ibc_fb_cond, to_print=True, output=False):\n",
        "        #--------------- Compute Free Boundary losses\n",
        "        #Compute Initial Free Boundary Condition\n",
        "        if test_ibc_fb_cond['Initial'] != None:\n",
        "            fb_init = self.fb(test_ibc_fb_cond['Initial'].x,\n",
        "                                  training=False)\n",
        "            fb_init_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(fb_init-test_ibc_fb_cond['Initial'].y)\n",
        "                )\n",
        "        else:\n",
        "            fb_init_loss = 0\n",
        "        #Compute Dirichlet Free Boundary Condition\n",
        "        if test_ibc_fb_cond['Dirichlet'] != None:\n",
        "            #Compute Free Boundary values\n",
        "            s_values = self.fb(test_ibc_fb_cond['Dirichlet'].x,\n",
        "                                training=True)\n",
        "            fb_dc = self.mdl(tf.concat(\n",
        "                [s_values,\n",
        "                  test_ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                  training=True)\n",
        "            fb_dir_target = tf.nn.relu(\n",
        "                tf.ones_like(\n",
        "                    s_values[:,-1]\n",
        "                    ) * K - tf.math.reduce_min(s_values, axis=1)\n",
        "            )\n",
        "            fb_dir_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(fb_dc - fb_dir_target)\n",
        "                )\n",
        "        else:\n",
        "            fb_dir_loss = 0\n",
        "        #Compute Neumann Free Boundary Condition\n",
        "        if test_ibc_fb_cond['Neumann'] != None:\n",
        "            s_values = self.fb(test_ibc_fb_cond['Neumann'].x,\n",
        "                                training=True)\n",
        "            with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                neu_fb_tape.watch(s_values)\n",
        "                pinn_nc_fb = self.mdl(\n",
        "                    tf.concat([s_values,\n",
        "                                test_ibc_fb_cond['Neumann'].x],\n",
        "                              axis=1),\n",
        "                              training=True)\n",
        "            pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                              s_values)\n",
        "            fb_neu_target = -tf.one_hot(\n",
        "                tf.math.argmin( s_values, axis=1),\n",
        "                depth=n_dim\n",
        "                )\n",
        "            fb_neu_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(pinn_nc_fb-fb_neu_target)\n",
        "                )\n",
        "        else:\n",
        "            fb_neu_loss = 0\n",
        "        #Compute final loss\n",
        "        fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "\n",
        "        #--------------- Compute PINN losses\n",
        "        #Compute unsupervised loss\n",
        "        s_values = self.fb(tf.concat([test_sampler.t], axis=-1),\n",
        "                            training=False)\n",
        "        temp = tf.reduce_sum(tf.cast(test_sampler.x < s_values,\n",
        "                                      dtype=self.DTYPE),\n",
        "                            axis=-1) < tf.ones(test_sampler.t.shape[0])\n",
        "        x_f = test_sampler.x[ temp ]\n",
        "        t_f = tf.reshape(test_sampler.t[ temp ], (-1,1) )\n",
        "        variables = list()\n",
        "        derivatives = list()\n",
        "        for i in range(x_f.shape[1]):\n",
        "            variables.append( x_f[:, i:i+1] )\n",
        "        variables.append( t_f )\n",
        "        with G_Tape(persistent=True,\n",
        "                    watch_accessed_variables=False) as pinn_tape:\n",
        "            #Watch independet variables\n",
        "            for var in variables:\n",
        "                pinn_tape.watch( var )\n",
        "            pinn_tape.watch(t_f)\n",
        "            u_val = self.mdl(tf.concat(variables, axis=1),\n",
        "                                training=True)\n",
        "            u_x = pinn_tape.gradient(u_val, x_f)\n",
        "            for i, var in enumerate(variables[:-1]):\n",
        "                derivatives.append( pinn_tape.gradient(u_val, var) )\n",
        "        unsup_loss = tf.reduce_mean(tf.square(\n",
        "            self.pde(pinn_tape, variables, u_val, derivatives) ))\n",
        "        #Compute Initial Condition\n",
        "        if test_ibc_cond['Initial'] != None:\n",
        "            pinn_init = self.mdl(test_ibc_cond['Initial'].x,\n",
        "                                  training=False)\n",
        "            if len(pinn_init.shape) > 1:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  tf.expand_dims(test_ibc_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  test_ibc_cond['Initial'].y )\n",
        "                    )\n",
        "        else:\n",
        "            init_loss = 0\n",
        "        #Compute Dirichlet Boundary Condition\n",
        "        if test_ibc_cond['Dirichlet'] != None:\n",
        "            pinn_dc = self.mdl(test_ibc_cond['Dirichlet'].x,\n",
        "                                training=True)\n",
        "            if len(pinn_dc.shape) > 1:\n",
        "                dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_dc -\\\n",
        "                                  tf.expand_dims(test_ibc_cond['Dirichlet'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_dc -\\\n",
        "                                  test_ibc_cond['Dirichlet'].y )\n",
        "                    )\n",
        "        else:\n",
        "            dir_loss = 0\n",
        "        #Compute Neumann Boundary Condition\n",
        "        if test_ibc_cond['Neumann'] != None:\n",
        "            with G_Tape(watch_accessed_variables=False) as neu_tape:\n",
        "                neu_tape.watch(test_ibc_cond['Neumann'].x)\n",
        "                pinn_nc = self.mdl(tf.concat([test_ibc_cond['Neumann'].x,\n",
        "                                              test_ibc_cond['Neumann'].t],\n",
        "                                            axis=1),\n",
        "                                    training=False)\n",
        "            pinn_nc = neu_tape.gradient(pinn_nc, test_ibc_cond['Neumann'].x)\n",
        "            neu_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(pinn_nc-test_ibc_cond['Neumann'].y)\n",
        "                )\n",
        "        else:\n",
        "            neu_loss = 0\n",
        "\n",
        "        #--------------- Compute total loss\n",
        "        pinn_loss = unsup_loss + init_loss + dir_loss +\\\n",
        "        neu_loss + fb_dir_loss + fb_neu_loss\n",
        "\n",
        "        u_l, i_l = np.array(unsup_loss), np.array(init_loss)\n",
        "        d_l, n_l = np.array(dir_loss), np.array(neu_loss)\n",
        "        b_i_l = np.array(fb_init_loss)\n",
        "        b_d_l, b_n_l = np.array(fb_dir_loss), np.array(fb_neu_loss)\n",
        "        b_l, p_l = np.array(fb_loss), np.array(pinn_loss)\n",
        "\n",
        "        if to_print:\n",
        "            print_base = \"{:<10}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}\"\n",
        "            print(print_base.format('', 'Unsupervised', 'Initial',\n",
        "                                    'Dirichlet', 'Neumann', 'FB_Init', 'FB_Dir',\n",
        "                                    'FB_Neu', 'Free Boundary', 'Total'))\n",
        "            print(print_base.format('', format(u_l, '.20f')[:10],\n",
        "                                    format(i_l, '.20f')[:10],\n",
        "                                    format(d_l, '.20f')[:10],\n",
        "                                    format(n_l, '.20f')[:10],\n",
        "                                    format(b_i_l, '.20f')[:10],\n",
        "                                    format(b_d_l, '.20f')[:10],\n",
        "                                    format(b_n_l, '.20f')[:10],\n",
        "                                    format(b_l, '.20f')[:10],\n",
        "                                    format(p_l, '.20f')[:10]))\n",
        "        if output:\n",
        "            return u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TaeFH_vCCBS"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "6TNAy9JACCBS",
        "outputId": "107ef647-b64a-4279-8536-23043c8f4b0a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'n_dim' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-85bd3e48218f>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m params = {'sample_method':'sobol',\n\u001b[0;32m----> 6\u001b[0;31m           \u001b[0;34m'layers'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_dim\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m           \u001b[0;34m'activation'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0;34m'output_act'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'initializer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'glorot_normal'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'n_dim' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay as ex_d\n",
        "my_lr = ex_d(1e-2, 320, 0.975, staircase=False)\n",
        "fb_lr = ex_d(1e-2, 80, 0.975, staircase=False)\n",
        "\n",
        "params = {'sample_method':'sobol',\n",
        "          'layers':[n_dim+1, 20, 20, 20, 20, 20, 20, 20, 20, 1],\n",
        "          'activation':'tanh',\n",
        "          'output_act':'linear', 'initializer':'glorot_normal',\n",
        "          'fb_layers':[1, 100, 100, 100, n_dim], 'fb_activation':'tanh',\n",
        "          'fb_output_act':'linear', 'fb_initializer':'glorot_normal',\n",
        "          'lr':my_lr, 'optimizer':'rmsprop',\n",
        "          'fb_lr':fb_lr, 'fb_optimizer':'rmsprop', 'steps_fb_per_pde':4,\n",
        "          'pde_weight':1, 'epochs':15000, 'verbose':100}\n",
        "\n",
        "my_pinn = FreeBoundary_PINN(params, pde, pinn_conditions,\n",
        "                            fb_conditions, lb, ub, N_f=120000, DTYPE=DTYPE)\n",
        "\n",
        "START = time.time()\n",
        "my_pinn.fit()\n",
        "train_time = time.time() - START\n",
        "my_pinn.plot_losses()\n",
        "\n",
        "START = time.time()\n",
        "values = my_pinn.plot_unsupervised_test(test_sampler, pinn_cond_test,\n",
        "                                        fb_cond_test, output=True)\n",
        "test_time = time.time() - START\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtB_MCaEwNQ_"
      },
      "source": [
        "# 4D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2v6j6gX83ZW"
      },
      "source": [
        "### Define problem, initial and boundary conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuA3cwrY83ZX"
      },
      "outputs": [],
      "source": [
        "\n",
        "import time\n",
        "import wandb\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from math import pi, exp\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "n_dim = 4\n",
        "\n",
        "#Fix seeds\n",
        "random_seed = 2\n",
        "np.random.seed(random_seed)\n",
        "tf.random.set_seed(random_seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDTCC_Rh83ZY"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Sampler_IBC():\n",
        "    def __init__(self, lb, ub, cond=None, N_points=100,\n",
        "                 method='sobol', grid=None, split=False, DTYPE='float64'):\n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "        self.cond = cond\n",
        "        self.DTYPE = DTYPE\n",
        "        self.sample(N_points, method, grid, split)\n",
        "\n",
        "    def sample(self, N_points, method, grid, split):\n",
        "        if method == 'uniform':\n",
        "            x_ibc = np.random.uniform(0, 1, size=(N_points, self.ub.shape[0]))\n",
        "            x_ibc = self.lb + (self.ub - self.lb)*x_ibc\n",
        "        elif method == 'latin':\n",
        "            from pyDOE import lhs\n",
        "            x_ibc = self.lb + (self.ub - self.lb)*lhs(self.ub.shape[0],N_points)\n",
        "        elif method == 'sobol':\n",
        "            import sobol\n",
        "            x_ibc = sobol.sample(dimension=self.ub.shape[0], n_points=N_points)\n",
        "            x_ibc = self.lb + (self.ub - self.lb)*x_ibc\n",
        "        elif method == 'equi':\n",
        "            x_ibc = np.linspace(self.lb, self.ub, N_points)\n",
        "        elif method == 'grid':\n",
        "            x_ibc = np.linspace(self.lb, self.ub, N_points).T\n",
        "            temp_final = list()\n",
        "            for val in x_ibc[0]:\n",
        "                temp_final.append( [val] )\n",
        "            dim = 1\n",
        "            while dim < x_ibc.shape[0]:\n",
        "                temp = list()\n",
        "                for t1 in range(x_ibc.shape[1]):\n",
        "                    for t2 in range(len(temp_final)):\n",
        "                        temp_val = temp_final[t2].copy()\n",
        "                        temp_val.append( x_ibc[dim, t1] )\n",
        "                        temp.append( temp_val )\n",
        "                temp_final = temp\n",
        "                dim += 1\n",
        "            x_ibc = np.array(temp_final)\n",
        "        elif method == 'grid_old':\n",
        "            idx = np.random.choice(range(grid.shape[0]),N_points,replace=False)\n",
        "            x_ibc = grid[idx]\n",
        "        if self.cond != None:\n",
        "            y_ibc = self.cond(x_ibc)\n",
        "            self.y = tf.cast(tf.Variable(y_ibc, trainable=False ),\n",
        "                             self.DTYPE)\n",
        "        if split:\n",
        "            x_ibc, t_ibc = x_ibc[:, :-1], x_ibc[:, -1:]\n",
        "            self.t = tf.cast(tf.Variable(t_ibc, trainable=False ),\n",
        "                             self.DTYPE)\n",
        "        self.x = tf.cast(tf.Variable(x_ibc, trainable=False ),\n",
        "                         self.DTYPE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3_mQ6eT83ZY"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Define global variables\n",
        "r = 0.01\n",
        "T = 3\n",
        "K = 10\n",
        "sigma = [[0.05, 0.01, 0.1, 0], [0.01, 0.06, -0.03, 0],\n",
        "         [0.1, -0.03, 0.4, 0.2], [0, 0, 0.2, 0.3]]\n",
        "sigma = np.array(sigma)\n",
        "if np.linalg.det(sigma) != 0:\n",
        "    if np.allclose(sigma, sigma.T):\n",
        "        if np.all(np.linalg.eigvals(sigma) > 0):\n",
        "            print('No problem with Covariance Matrix')\n",
        "        else:\n",
        "            print('Matrix is not positive semidefinite')\n",
        "    else:\n",
        "        print('Matrix is not symmetric')\n",
        "else:\n",
        "    print('Matrix is singular')\n",
        "\n",
        "d = np.array([r]*n_dim) - np.array([0]*n_dim)\n",
        "alphas = np.zeros((n_dim, n_dim))\n",
        "for i in range(n_dim):\n",
        "    alphas[i, i] = np.sum( np.dot(sigma[i], sigma[i]) )\n",
        "    for j in range(i):\n",
        "        alphas[i, j] = alphas[j, i] = np.sum( np.dot(sigma[i], sigma[j]) )\n",
        "DTYPE = 'float32'\n",
        "\n",
        "#Define domain boundaries\n",
        "lb = np.array( ([0.]*n_dim)+[0] )\n",
        "ub = np.array( ([3.*K]*n_dim)+[T] )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOxlD5dj83ZZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Problem Definition\n",
        "def h_2(inp):\n",
        "    res = - np.ones( inp.shape[0] )\n",
        "    return res\n",
        "\n",
        "def g(inp):\n",
        "    res = list()\n",
        "    for inp_val in inp:\n",
        "        res.append( np.max([0,\n",
        "                            K*np.exp(-r*(T-inp_val[-1]))-\\\n",
        "                            np.min(inp_val[:-1])]) )\n",
        "    return np.array(res)\n",
        "\n",
        "def u_0(inp):\n",
        "    res = list()\n",
        "    for inp_val in inp:\n",
        "        res.append( np.max([0, K - np.min(inp_val[:-1])]) )\n",
        "    return np.array(res)\n",
        "\n",
        "def s_0(inp):\n",
        "    res = np.ones( (inp.shape[0], n_dim) ) * K\n",
        "    return np.array(res)\n",
        "\n",
        "#Point sampling\n",
        "N_to_sample = 12000\n",
        "\n",
        "#---------------- PDE Conditions\n",
        "print('PDE COnditions\\n')\n",
        "#Initial\n",
        "init_sampler = Sampler_IBC(np.array( ([0.]*n_dim)+[T] ),\n",
        "                           np.array( ([3.*K]*n_dim)+[T] ),\n",
        "                           u_0, 12000, DTYPE=DTYPE )\n",
        "print( f'x: {init_sampler.x.shape}     y: {init_sampler.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "lb_dir = ([0.]*n_dim)+[0]; lb_dir[0] = 3.*K\n",
        "dir_sampler = Sampler_IBC(np.array( lb_dir ),\n",
        "                          np.array( ([3.*K]*n_dim)+[T] ),\n",
        "                          g, N_to_sample//n_dim, DTYPE=DTYPE )\n",
        "for dim in range(1, n_dim):\n",
        "    lb_dir = ([0.]*n_dim)+[0]; lb_dir[dim] = 3.*K\n",
        "    temp_sampler = Sampler_IBC(np.array( lb_dir ),\n",
        "                              np.array( ([3.*K]*n_dim)+[T] ),\n",
        "                              g, N_to_sample//n_dim, DTYPE=DTYPE )\n",
        "    dir_sampler.x = tf.concat([dir_sampler.x, temp_sampler.x], axis=0)\n",
        "    dir_sampler.y = tf.concat([dir_sampler.y, temp_sampler.y], axis=0)\n",
        "print( f'x: {dir_sampler.x.shape}     y: {dir_sampler.y.shape}' )\n",
        "\n",
        "#Neumann\n",
        "print('No')\n",
        "\n",
        "#---------------- Free Boundary Conditions\n",
        "print('\\nFree Boundary COnditions\\n')\n",
        "#Initial\n",
        "init_fb_sampler = Sampler_IBC(np.array([T]), np.array([T]),\n",
        "                              s_0, 1, DTYPE=DTYPE )\n",
        "print( f't: {init_fb_sampler.x.shape}     y: {init_fb_sampler.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "dir_fb_sampler = Sampler_IBC(np.array([0.]), np.array([T]),\n",
        "                             None, N_to_sample, DTYPE=DTYPE )\n",
        "print( f't: {dir_fb_sampler.x.shape}' )\n",
        "\n",
        "#Neumann\n",
        "neu_fb_sampler = Sampler_IBC(np.array([0.]), np.array([T]),\n",
        "                             None, N_to_sample, DTYPE=DTYPE )\n",
        "print( f't: {neu_fb_sampler.x.shape}' )\n",
        "\n",
        "#---------------- Conditions passed to PINN\n",
        "pinn_conditions = {'Initial':init_sampler,\n",
        "                   'Dirichlet':dir_sampler,\n",
        "                   'Neumann':None}\n",
        "fb_conditions = {'Initial':init_fb_sampler,\n",
        "                 'Dirichlet':dir_fb_sampler,\n",
        "                 'Neumann':neu_fb_sampler}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFe015kA83Zb"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_sampler = Sampler_IBC(lb, ub, cond=None, DTYPE=DTYPE,\n",
        "                           N_points=1000000, method='uniform', split=True)\n",
        "\n",
        "#Point sampling\n",
        "sample_to_test = 1000\n",
        "\n",
        "#---------------- PDE Conditions\n",
        "print('PDE COnditions\\n')\n",
        "#Initial\n",
        "init_sampler_test = Sampler_IBC(np.array( ([0.]*n_dim)+[T] ),\n",
        "                           np.array( ([3.*K]*n_dim)+[T] ),\n",
        "                           u_0, sample_to_test, DTYPE=DTYPE, method='uniform' )\n",
        "print( f'x: {init_sampler_test.x.shape}     y: {init_sampler_test.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "lb_dir = ([0.]*n_dim)+[0]; lb_dir[0] = 3.*K\n",
        "dir_sampler_test = Sampler_IBC(np.array( lb_dir ),\n",
        "                          np.array( ([3.*K]*n_dim)+[T] ),\n",
        "                          g, sample_to_test//n_dim, DTYPE=DTYPE, method='uniform' )\n",
        "for dim in range(1, n_dim):\n",
        "    lb_dir = ([0.]*n_dim)+[0]; lb_dir[dim] = 3.*K\n",
        "    temp_sampler = Sampler_IBC(np.array( lb_dir ),\n",
        "                              np.array( ([3.*K]*n_dim)+[T] ),\n",
        "                              g, sample_to_test//n_dim, DTYPE=DTYPE, method='uniform' )\n",
        "    dir_sampler_test.x = tf.concat([dir_sampler_test.x, temp_sampler.x], axis=0)\n",
        "    dir_sampler_test.y = tf.concat([dir_sampler_test.y, temp_sampler.y], axis=0)\n",
        "print( f'x: {dir_sampler_test.x.shape}     y: {dir_sampler_test.y.shape}' )\n",
        "\n",
        "#Neumann\n",
        "print('No')\n",
        "\n",
        "#---------------- Free Boundary Conditions\n",
        "print('\\nFree Boundary COnditions\\n')\n",
        "#Initial\n",
        "init_fb_sampler_test = Sampler_IBC(np.array([T]), np.array([T]),\n",
        "                              s_0, 1, DTYPE=DTYPE, method='uniform' )\n",
        "print( f't: {init_fb_sampler_test.x.shape}     y: {init_fb_sampler_test.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "dir_fb_sampler_test = Sampler_IBC(np.array([0.]), np.array([T]),\n",
        "                             None, sample_to_test, DTYPE=DTYPE, method='uniform' )\n",
        "print( f't: {dir_fb_sampler_test.x.shape}' )\n",
        "\n",
        "#Neumann\n",
        "neu_fb_sampler_test = Sampler_IBC(np.array([0.]), np.array([T]),\n",
        "                                  None, sample_to_test//n_dim, DTYPE=DTYPE, method='uniform' )\n",
        "print( f't: {neu_fb_sampler_test.x.shape}' )\n",
        "\n",
        "#---------------- Conditions passed to PINN\n",
        "pinn_cond_test = {'Initial':init_sampler_test,\n",
        "                   'Dirichlet':dir_sampler_test,\n",
        "                   'Neumann':None}\n",
        "fb_cond_test = {'Initial':init_fb_sampler_test,\n",
        "                 'Dirichlet':dir_fb_sampler_test,\n",
        "                 'Neumann':neu_fb_sampler_test}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IuLEGrOxu6u"
      },
      "source": [
        "### PINN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY8xF9Yxxu6v"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Define PDE\n",
        "def pde(tape, vars, fun_val, fo_grads):\n",
        "    second_order = 0\n",
        "    first_order = 0\n",
        "    zero_order = 0\n",
        "    zero_order += tape.gradient(fun_val, vars[-1]) - r * fun_val\n",
        "    for i, var in enumerate(vars[:-1]):\n",
        "        dx_i = fo_grads[i]\n",
        "        first_order += d[i] * var * dx_i\n",
        "        second_order +=\\\n",
        "        tape.gradient(dx_i, var) * (var**2) * alphas[i, i]\n",
        "        for j, var2 in enumerate(vars[:i]):\n",
        "            second_order +=\\\n",
        "            2*( tape.gradient(dx_i, var2) * var * var2 * alphas[i, j])\n",
        "    f = second_order/2 + first_order + zero_order\n",
        "    del(tape)\n",
        "    return f\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7jsxz_pxu6w"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow import GradientTape as G_Tape\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.initializers import GlorotNormal, GlorotUniform\n",
        "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
        "from tensorflow.keras.metrics import mean_squared_error\n",
        "\n",
        "class FreeBoundary_PINN():\n",
        "\n",
        "    def __init__(self, params, pde, ibc_cond, ibc_fb_cond, lb, ub,\n",
        "                 N_f=10000, N_fb_ibc=150, DTYPE='float64', coll_points=None):\n",
        "        self.params = params\n",
        "        self.DTYPE = DTYPE\n",
        "        self.Default_Params()\n",
        "        #Set seed\n",
        "        if self.params['seed'] != None:\n",
        "            tf.keras.utils.set_random_seed( self.params['seed'] )\n",
        "            tf.config.experimental.enable_op_determinism()\n",
        "        #Define pde\n",
        "        self.pde = pde\n",
        "        #Define intial and boundary conditions\n",
        "        self.ibc_cond = ibc_cond\n",
        "        self.ibc_fb_cond = ibc_fb_cond\n",
        "        #All needed for points sampling\n",
        "        self.lb = tf.Variable(lb, trainable=False)\n",
        "        self.ub = tf.Variable(ub, trainable=False)\n",
        "        if coll_points == None:\n",
        "            self.N_f = N_f\n",
        "            self.Sample_Points()\n",
        "        else:\n",
        "            self.x_f = coll_points[0]\n",
        "            self.t_f = coll_points[1]\n",
        "        self.N_fb_ibc = N_fb_ibc\n",
        "        #Initialize the class: define the network\n",
        "        self.Define_Regularizer()\n",
        "        self.Define_Initializer()\n",
        "        self.Define_Optimizer()\n",
        "        self.Create_Network()\n",
        "        self.Create_FB_Network()\n",
        "\n",
        "    def Default_Params(self):\n",
        "        target = self.params.keys()\n",
        "        if 'seed' not in target:\n",
        "            self.params['seed'] = None\n",
        "        if 'optimizer' not in target:\n",
        "            self.params['optimizer'] = 'Adam'\n",
        "        if 'fb_optimizer' not in target:\n",
        "            self.params['fb_optimizer'] = 'Adam'\n",
        "        if 'reg_type' not in target:\n",
        "            self.params['reg_type'] = None\n",
        "        if 'initializer' not in target:\n",
        "            self.params['initializer'] = 'glorot_normal'\n",
        "        if 'activation' not in target:\n",
        "            self.params['activation'] = 'tanh'\n",
        "        if 'output_act' not in target:\n",
        "            self.params['output_act'] = 'linear'\n",
        "        if 'pde_weight' not in target:\n",
        "            self.params['pde_weight'] = 1.\n",
        "        if 'sup_weight' not in target:\n",
        "            self.params['sup_weight'] = [1., 1., 1.]\n",
        "        if 'fb_weight' not in target:\n",
        "            self.params['fb_weight'] = [1., 1., 1.]\n",
        "        if 'patience' not in target:\n",
        "            self.params['patience'] = np.inf\n",
        "        if 'sample_method' not in target:\n",
        "            self.params['sample_method'] = 'uniform'\n",
        "        if 'fb_output_act' not in target:\n",
        "            self.params['fb_output_act'] = 'linear'\n",
        "        if 'fb_activation' not in target:\n",
        "            self.params['fb_activation'] = 'tanh'\n",
        "        if 'verbose' not in target:\n",
        "            self.params['verbose'] = 1\n",
        "        if 'steps_fb_per_pde' not in target:\n",
        "            self.params['steps_fb_per_pde'] = 1\n",
        "        if 'fb_freezing' not in target:\n",
        "            self.params['fb_freezing'] = None\n",
        "\n",
        "    def Define_Regularizer(self):\n",
        "        if self.params['reg_type'] == 'l1':\n",
        "            self.regularizer = l1( self.params['reg'] )\n",
        "        elif self.params['reg_type'] == 'l2':\n",
        "            self.regularizer = l2( self.params['reg'] )\n",
        "        elif self.params['reg_type'] == 'l1_l2':\n",
        "            self.regularizer = l1_l2( self.params['reg'][0],\n",
        "                                      self.params['reg'][1] )\n",
        "        else:\n",
        "            self.regularizer = None\n",
        "\n",
        "    def Define_Initializer(self):\n",
        "        if self.params['initializer'] == 'glorot_normal':\n",
        "            self.initializer = GlorotNormal()\n",
        "        elif self.params['initializer'] == 'glorot_uniform':\n",
        "            self.initializer = GlorotUniform()\n",
        "        else:\n",
        "            self.initializer = None\n",
        "\n",
        "    def Define_Optimizer(self):\n",
        "        temp = self.params['optimizer']\n",
        "        if temp.lower() == 'adam':\n",
        "            self.opt = Adam( self.params['lr'] )\n",
        "        elif temp.lower() == 'rmsprop':\n",
        "            self.opt = RMSprop( self.params['lr'] )\n",
        "        else:\n",
        "            raise ValueError(f\"Optimizer {temp} not recognized\")\n",
        "\n",
        "        temp = self.params['fb_optimizer']\n",
        "        if temp.lower() == 'adam':\n",
        "            self.fb_opt = Adam( self.params['fb_lr'] )\n",
        "        elif temp.lower() == 'rmsprop':\n",
        "            self.fb_opt = RMSprop( self.params['fb_lr'] )\n",
        "        else:\n",
        "            raise ValueError(f\"fb_Optimizer {temp} not recognized\")\n",
        "\n",
        "    def Create_Network(self):\n",
        "        input_layer = Input(shape=self.params['layers'][0],\n",
        "                            name = 'Input')\n",
        "        x = Dense(units=self.params['layers'][1],\n",
        "                  activation=self.params['activation'],\n",
        "                  kernel_initializer=self.initializer,\n",
        "                  kernel_regularizer=self.regularizer,\n",
        "                  name='Dense_1')(input_layer)\n",
        "        for layer in range(2, len(self.params['layers'])-1):\n",
        "            x = Dense(units=self.params['layers'][layer],\n",
        "                      activation=self.params['activation'],\n",
        "                      kernel_initializer=self.initializer,\n",
        "                      kernel_regularizer=self.regularizer,\n",
        "                      name=f'Dense_{layer}')(x)\n",
        "        output = Dense(units=self.params['layers'][-1],\n",
        "                       activation=self.params['output_act'],\n",
        "                       kernel_initializer=self.initializer,\n",
        "                       kernel_regularizer=self.regularizer,\n",
        "                       name='Output')(x)\n",
        "        self.mdl = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "    def Create_FB_Network(self):\n",
        "        input_layer = Input(shape=self.params['fb_layers'][0],\n",
        "                            name = 'Input')\n",
        "        x = Dense(units=self.params['fb_layers'][1],\n",
        "                  activation=self.params['fb_activation'],\n",
        "                  kernel_initializer=self.initializer,\n",
        "                  kernel_regularizer=self.regularizer,\n",
        "                  name='Dense_1')(input_layer)\n",
        "        for layer in range(2, len(self.params['fb_layers'])-1):\n",
        "            x = Dense(units=self.params['fb_layers'][layer],\n",
        "                      activation=self.params['fb_activation'],\n",
        "                      kernel_initializer=self.initializer,\n",
        "                      kernel_regularizer=self.regularizer,\n",
        "                      name=f'Dense_{layer}')(x)\n",
        "        output = Dense(units=self.params['fb_layers'][-1],\n",
        "                       activation=self.params['fb_output_act'],\n",
        "                       kernel_initializer=self.initializer,\n",
        "                       kernel_regularizer=self.regularizer,\n",
        "                       name='Output')(x)\n",
        "        self.fb = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "    def Sample_Points(self):\n",
        "        #According to the selected method, sample collocation points\n",
        "        method = self.params['sample_method']\n",
        "        if method == 'latin':\n",
        "            from pyDOE import lhs\n",
        "            cps = self.lb + (self.ub - self.lb)*lhs(self.ub.shape[0], self.N_f)\n",
        "        elif method == 'uniform':\n",
        "            cps = np.random.uniform(0, 1, size=(self.N_f, self.ub.shape[0]))\n",
        "            cps = self.lb + (self.ub - self.lb)*cps\n",
        "        elif method == 'sobol':\n",
        "            import sobol\n",
        "            cps = sobol.sample(dimension=self.ub.shape[0], n_points=self.N_f)\n",
        "            cps = self.lb + (self.ub - self.lb)*cps\n",
        "        else:\n",
        "            raise ValueError(f'Sampling method {method} not recognized')\n",
        "        #Return collocation points as tf tensors\n",
        "        self.x_f_total = tf.cast(tf.Variable(cps[:, :-1], trainable=False),\n",
        "                           self.DTYPE)\n",
        "        self.t_f_total = tf.cast(tf.Variable(cps[:, -1:], trainable=False),\n",
        "                           self.DTYPE)\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self):\n",
        "        with G_Tape(watch_accessed_variables=False) as mdl_tape:\n",
        "            #Watch solution weights\n",
        "            mdl_tape.watch(self.mdl.trainable_variables)\n",
        "            #--------------- Compute Free Boundary losses\n",
        "            with G_Tape(watch_accessed_variables=False) as fb_tape:\n",
        "                #Watch free boundary weights\n",
        "                fb_tape.watch(self.fb.trainable_variables)\n",
        "                #Compute Initial Free Boundary Condition\n",
        "                fb_init = self.fb(self.ibc_fb_cond['Initial'].x,\n",
        "                                      training=True)\n",
        "                fb_init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_init-self.ibc_fb_cond['Initial'].y)\n",
        "                    )\n",
        "                #Compute Dirichlet Free Boundary Condition\n",
        "                if self.ibc_fb_cond['Dirichlet'] != None:\n",
        "                    #Compute Free Boundary values\n",
        "                    s_values = self.fb(self.ibc_fb_cond['Dirichlet'].x,\n",
        "                                       training=True)\n",
        "                    fb_dc = self.mdl(tf.concat(\n",
        "                        [s_values,\n",
        "                         self.ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                         training=True)\n",
        "                    fb_dir_target = tf.nn.relu(\n",
        "                        tf.ones_like(\n",
        "                            s_values[:,-1]\n",
        "                            ) * K - tf.math.reduce_min(s_values, axis=1)\n",
        "                    )\n",
        "                    fb_dir_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(fb_dc - fb_dir_target)\n",
        "                        )\n",
        "                else:\n",
        "                    fb_dir_loss = 0\n",
        "                #Compute Neumann Free Boundary Condition\n",
        "                if self.ibc_fb_cond['Neumann'] != None:\n",
        "                    s_values = self.fb(self.ibc_fb_cond['Neumann'].x,\n",
        "                                       training=True)\n",
        "                    with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                        neu_fb_tape.watch(s_values)\n",
        "                        pinn_nc_fb = self.mdl(\n",
        "                            tf.concat([s_values,\n",
        "                                       self.ibc_fb_cond['Neumann'].x],\n",
        "                                      axis=1),\n",
        "                                      training=True)\n",
        "                    pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                                      s_values)\n",
        "                    fb_neu_target = -tf.one_hot(\n",
        "                        tf.math.argmin( s_values, axis=1),\n",
        "                        depth=n_dim\n",
        "                        )\n",
        "                    fb_neu_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(pinn_nc_fb-fb_neu_target)\n",
        "                        )\n",
        "                else:\n",
        "                    fb_neu_loss = 0\n",
        "                #Compute final loss\n",
        "                fb_loss = self.params['fb_weight'][0] * fb_init_loss +\\\n",
        "                self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "                self.params['fb_weight'][2] * fb_neu_loss\n",
        "            #Compute gradient and apply optimizers for free boundary\n",
        "            gradient_fb = fb_tape.gradient(fb_loss,\n",
        "                                          self.fb.trainable_variables)\n",
        "            self.fb_opt.apply_gradients( zip(gradient_fb,\n",
        "                                          self.fb.trainable_variables) )\n",
        "\n",
        "            #--------------- Compute PINN losses\n",
        "            #Compute unsupervised loss\n",
        "            s_values = self.fb(tf.concat([self.t_f_total], axis=-1),\n",
        "                               training=False)\n",
        "            temp = tf.reduce_sum(tf.cast(self.x_f_total < s_values,\n",
        "                                         dtype=self.DTYPE),\n",
        "                                axis=-1) < tf.ones(self.t_f_total.shape[0])\n",
        "            x_f = self.x_f_total[ temp ]\n",
        "            t_f = tf.reshape(self.t_f_total[ temp ], (-1,1) )\n",
        "            variables = list()\n",
        "            derivatives = list()\n",
        "            for i in range(x_f.shape[1]):\n",
        "                variables.append( x_f[:, i:i+1] )\n",
        "            variables.append( t_f )\n",
        "            with G_Tape(persistent=True,\n",
        "                        watch_accessed_variables=False) as pinn_tape:\n",
        "                #Watch independet variables\n",
        "                for var in variables:\n",
        "                    pinn_tape.watch( var )\n",
        "                pinn_tape.watch(t_f)\n",
        "                u_val = self.mdl(tf.concat(variables, axis=1),\n",
        "                                    training=True)\n",
        "                u_x = pinn_tape.gradient(u_val, x_f)\n",
        "                for i, var in enumerate(variables[:-1]):\n",
        "                    derivatives.append( pinn_tape.gradient(u_val, var) )\n",
        "            unsup_loss = tf.reduce_mean(tf.square(\n",
        "                self.pde(pinn_tape, variables, u_val, derivatives) ))\n",
        "            #Compute Initial Condition\n",
        "            pinn_init = self.mdl(self.ibc_cond['Initial'].x,\n",
        "                                training=True)\n",
        "            if len(pinn_init.shape) > 1:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  tf.expand_dims(self.ibc_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  self.ibc_cond['Initial'].y )\n",
        "                    )\n",
        "            #Compute Dirichlet Boundary Condition\n",
        "            if self.ibc_cond['Dirichlet'] != None:\n",
        "                pinn_dc = self.mdl(self.ibc_cond['Dirichlet'].x,\n",
        "                                    training=True)\n",
        "                if len(pinn_dc.shape) > 1:\n",
        "                    dir_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(pinn_dc -\\\n",
        "                                      tf.expand_dims(self.ibc_cond['Dirichlet'].y,\n",
        "                                                     axis=-1 ) )\n",
        "                        )\n",
        "                else:\n",
        "                    dir_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(pinn_dc -\\\n",
        "                                      self.ibc_cond['Dirichlet'].y )\n",
        "                        )\n",
        "            else:\n",
        "                dir_loss = 0\n",
        "            #Compute Neumann Boundary Condition\n",
        "            if self.ibc_cond['Neumann'] != None:\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_tape:\n",
        "                    neu_tape.watch(self.ibc_cond['Neumann'].x)\n",
        "                    pinn_nc = self.mdl(tf.concat([self.ibc_cond['Neumann'].x,\n",
        "                                                  self.ibc_cond['Neumann'].t],\n",
        "                                                axis=1),\n",
        "                                        training=True)\n",
        "                pinn_nc = neu_tape.gradient(pinn_nc, self.ibc_cond['Neumann'].x)\n",
        "                neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc-self.ibc_cond['Neumann'].y)\n",
        "                    )\n",
        "            else:\n",
        "                neu_loss = 0\n",
        "\n",
        "            #--------------- Compute total loss\n",
        "            pinn_loss = (self.params['pde_weight'] * unsup_loss) +\\\n",
        "            self.params['sup_weight'][0] * init_loss +\\\n",
        "            self.params['sup_weight'][1] * dir_loss +\\\n",
        "            self.params['sup_weight'][2] * neu_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "        #Compute gradient and apply optimizers\n",
        "        gradient = mdl_tape.gradient(pinn_loss,self.mdl.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(gradient,self.mdl.trainable_variables))\n",
        "        #Return losses\n",
        "        final_fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "        final_pinn_loss = unsup_loss + init_loss +\\\n",
        "         dir_loss + neu_loss + fb_dir_loss + fb_neu_loss\n",
        "        return (unsup_loss, init_loss, dir_loss, neu_loss, fb_init_loss,\n",
        "                fb_dir_loss, fb_neu_loss, final_fb_loss, final_pinn_loss)\n",
        "\n",
        "    @tf.function\n",
        "    def train_mdl_solo(self):\n",
        "        with G_Tape(watch_accessed_variables=False) as mdl_tape:\n",
        "            #Watch solution weights\n",
        "            mdl_tape.watch(self.mdl.trainable_variables)\n",
        "            #--------------- Compute Free Boundary losses\n",
        "            #Compute Initial Free Boundary Condition\n",
        "            fb_init = self.fb(self.ibc_fb_cond['Initial'].x,\n",
        "                                  training=True)\n",
        "            fb_init_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(fb_init-self.ibc_fb_cond['Initial'].y)\n",
        "                )\n",
        "            #Compute Dirichlet Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Dirichlet'] != None:\n",
        "                #Compute Free Boundary values\n",
        "                s_values = self.fb(self.ibc_fb_cond['Dirichlet'].x,\n",
        "                                    training=True)\n",
        "                fb_dc = self.mdl(tf.concat(\n",
        "                    [s_values,\n",
        "                      self.ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                      training=True)\n",
        "                fb_dir_target = tf.nn.relu(\n",
        "                    tf.ones_like(\n",
        "                        s_values[:,-1]\n",
        "                        ) * K - tf.math.reduce_min(s_values, axis=1)\n",
        "                )\n",
        "                fb_dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_dc - fb_dir_target)\n",
        "                    )\n",
        "            else:\n",
        "                fb_dir_loss = 0\n",
        "            #Compute Neumann Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Neumann'] != None:\n",
        "                s_values = self.fb(self.ibc_fb_cond['Neumann'].x,\n",
        "                                    training=True)\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                    neu_fb_tape.watch(s_values)\n",
        "                    pinn_nc_fb = self.mdl(\n",
        "                        tf.concat([s_values,\n",
        "                                    self.ibc_fb_cond['Neumann'].x],\n",
        "                                  axis=1),\n",
        "                                  training=True)\n",
        "                pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                                  s_values)\n",
        "                fb_neu_target = -tf.one_hot(\n",
        "                    tf.math.argmin( s_values, axis=1),\n",
        "                    depth=n_dim\n",
        "                    )\n",
        "                fb_neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc_fb-fb_neu_target)\n",
        "                    )\n",
        "            else:\n",
        "                fb_neu_loss = 0\n",
        "            #Compute final loss\n",
        "            fb_loss = self.params['fb_weight'][0] * fb_init_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "\n",
        "            #--------------- Compute PINN losses\n",
        "            #Compute unsupervised loss\n",
        "            s_values = self.fb(tf.concat([self.t_f_total], axis=-1),\n",
        "                               training=False)\n",
        "            temp = tf.reduce_sum(tf.cast(self.x_f_total < s_values,\n",
        "                                         dtype=self.DTYPE),\n",
        "                                axis=-1) < tf.ones(self.t_f_total.shape[0])\n",
        "            x_f = self.x_f_total[ temp ]\n",
        "            t_f = tf.reshape(self.t_f_total[ temp ], (-1,1) )\n",
        "            variables = list()\n",
        "            derivatives = list()\n",
        "            for i in range(x_f.shape[1]):\n",
        "                variables.append( x_f[:, i:i+1] )\n",
        "            variables.append( t_f )\n",
        "            with G_Tape(persistent=True,\n",
        "                        watch_accessed_variables=False) as pinn_tape:\n",
        "                #Watch independet variables\n",
        "                for var in variables:\n",
        "                    pinn_tape.watch( var )\n",
        "                pinn_tape.watch(t_f)\n",
        "                u_val = self.mdl(tf.concat(variables, axis=1),\n",
        "                                    training=True)\n",
        "                u_x = pinn_tape.gradient(u_val, x_f)\n",
        "                for i, var in enumerate(variables[:-1]):\n",
        "                    derivatives.append( pinn_tape.gradient(u_val, var) )\n",
        "            unsup_loss = tf.reduce_mean(tf.square(\n",
        "                self.pde(pinn_tape, variables, u_val, derivatives) ))\n",
        "            #Compute Initial Condition\n",
        "            pinn_init = self.mdl(self.ibc_cond['Initial'].x,\n",
        "                                training=True)\n",
        "            if len(pinn_init.shape) > 1:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  tf.expand_dims(self.ibc_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  self.ibc_cond['Initial'].y )\n",
        "                    )\n",
        "            #Compute Dirichlet Boundary Condition\n",
        "            if self.ibc_cond['Dirichlet'] != None:\n",
        "                pinn_dc = self.mdl(self.ibc_cond['Dirichlet'].x,\n",
        "                                    training=True)\n",
        "                if len(pinn_dc.shape) > 1:\n",
        "                    dir_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(pinn_dc -\\\n",
        "                                      tf.expand_dims(self.ibc_cond['Dirichlet'].y,\n",
        "                                                     axis=-1 ) )\n",
        "                        )\n",
        "                else:\n",
        "                    dir_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(pinn_dc -\\\n",
        "                                      self.ibc_cond['Dirichlet'].y )\n",
        "                        )\n",
        "            else:\n",
        "                dir_loss = 0\n",
        "            #Compute Neumann Boundary Condition\n",
        "            if self.ibc_cond['Neumann'] != None:\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_tape:\n",
        "                    neu_tape.watch(self.ibc_cond['Neumann'].x)\n",
        "                    pinn_nc = self.mdl(tf.concat([self.ibc_cond['Neumann'].x,\n",
        "                                                  self.ibc_cond['Neumann'].t],\n",
        "                                                axis=1),\n",
        "                                        training=True)\n",
        "                pinn_nc = neu_tape.gradient(pinn_nc, self.ibc_cond['Neumann'].x)\n",
        "                neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc-self.ibc_cond['Neumann'].y)\n",
        "                    )\n",
        "            else:\n",
        "                neu_loss = 0\n",
        "\n",
        "            #--------------- Compute total loss\n",
        "            pinn_loss = (self.params['pde_weight'] * unsup_loss) +\\\n",
        "            self.params['sup_weight'][0] * init_loss +\\\n",
        "            self.params['sup_weight'][1] * dir_loss +\\\n",
        "            self.params['sup_weight'][2] * neu_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "        #Compute gradient and apply optimizers\n",
        "        gradient = mdl_tape.gradient(pinn_loss,self.mdl.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(gradient,self.mdl.trainable_variables))\n",
        "        #Return losses\n",
        "        final_fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "        final_pinn_loss = unsup_loss + init_loss +\\\n",
        "         dir_loss + neu_loss + fb_dir_loss + fb_neu_loss\n",
        "        return (unsup_loss, init_loss, dir_loss, neu_loss, fb_init_loss,\n",
        "                fb_dir_loss, fb_neu_loss, final_fb_loss, final_pinn_loss)\n",
        "\n",
        "    @tf.function\n",
        "    def train_fb_solo(self):\n",
        "        #--------------- Compute Free Boundary losses\n",
        "        with G_Tape(watch_accessed_variables=False) as fb_tape:\n",
        "            #Watch free boundary weights\n",
        "            fb_tape.watch(self.fb.trainable_variables)\n",
        "            #Compute Initial Free Boundary Condition\n",
        "            fb_init = self.fb(self.ibc_fb_cond['Initial'].x,\n",
        "                                  training=True)\n",
        "            fb_init_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(fb_init-self.ibc_fb_cond['Initial'].y)\n",
        "                )\n",
        "            #Compute Dirichlet Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Dirichlet'] != None:\n",
        "                #Compute Free Boundary values\n",
        "                s_values = self.fb(self.ibc_fb_cond['Dirichlet'].x,\n",
        "                                    training=True)\n",
        "                fb_dc = self.mdl(tf.concat(\n",
        "                    [s_values,\n",
        "                      self.ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                      training=True)\n",
        "                fb_dir_target = tf.nn.relu(\n",
        "                    tf.ones_like(\n",
        "                        s_values[:,-1]\n",
        "                        ) * K - tf.math.reduce_min(s_values, axis=1)\n",
        "                )\n",
        "                fb_dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_dc - fb_dir_target)\n",
        "                    )\n",
        "            else:\n",
        "                fb_dir_loss = 0\n",
        "            #Compute Neumann Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Neumann'] != None:\n",
        "                s_values = self.fb(self.ibc_fb_cond['Neumann'].x,\n",
        "                                    training=True)\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                    neu_fb_tape.watch(s_values)\n",
        "                    pinn_nc_fb = self.mdl(\n",
        "                        tf.concat([s_values,\n",
        "                                    self.ibc_fb_cond['Neumann'].x],\n",
        "                                  axis=1),\n",
        "                                  training=True)\n",
        "                pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                                  s_values)\n",
        "                fb_neu_target = -tf.one_hot(\n",
        "                    tf.math.argmin( s_values, axis=1),\n",
        "                    depth=n_dim\n",
        "                    )\n",
        "                fb_neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc_fb-fb_neu_target)\n",
        "                    )\n",
        "            else:\n",
        "                fb_neu_loss = 0\n",
        "            #Compute final loss\n",
        "            fb_loss = self.params['fb_weight'][0] * fb_init_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "        #Compute gradient and apply optimizers for free boundary\n",
        "        gradient_fb = fb_tape.gradient(fb_loss,\n",
        "                                      self.fb.trainable_variables)\n",
        "        self.fb_opt.apply_gradients( zip(gradient_fb,\n",
        "                                      self.fb.trainable_variables) )\n",
        "        #Return losses\n",
        "        final_fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "        return fb_init_loss, fb_dir_loss, fb_neu_loss, final_fb_loss\n",
        "\n",
        "    def fit(self, wandb_run=None):\n",
        "        #Early warning initialization\n",
        "        self.early_warning = {'Target':np.inf,\n",
        "                              'n_steps':0,\n",
        "                              'top_mdl':None,\n",
        "                              'weights':None}\n",
        "        old_top_mdl, old_weights = None, None\n",
        "        #Training\n",
        "        self.u_losses, self.i_losses = list(), list()\n",
        "        self.d_losses, self.n_losses = list(), list()\n",
        "        self.b_i_losses = list()\n",
        "        self.b_d_losses, self.b_n_losses = list(), list()\n",
        "        self.b_losses, self.p_losses = list(), list()\n",
        "        print_base = \"{:<10}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}\"\n",
        "        if self.params['fb_freezing'] == None:\n",
        "            for epoch in tqdm(range(self.params['epochs']),\n",
        "                              desc='PINNs - Training'):\n",
        "                if epoch == 0:\n",
        "                    print(print_base.format('Epoch', 'Unsupervised', 'Initial',\n",
        "                                            'Dirichlet', 'Neumann',\n",
        "                                            'FB_Init', 'FB_Dir', 'FB_Neu',\n",
        "                                            'Free Boundary', 'Total'))\n",
        "                    print('\\n')\n",
        "                #Case 1: more mdl steps for a single fb step\n",
        "                for _ in range(self.params['steps_fb_per_pde'] -1):\n",
        "                    self.train_mdl_solo();\n",
        "                #Case 2: more fb steps for a single mdl step\n",
        "                for _ in range(0, self.params['steps_fb_per_pde'] +1, -1):\n",
        "                    self.train_fb_solo();\n",
        "                u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l =\\\n",
        "                self.train_step()\n",
        "                u_l, i_l = np.array(u_l), np.array(i_l)\n",
        "                d_l, n_l = np.array(d_l), np.array(n_l)\n",
        "                b_i_l = np.array(b_i_l)\n",
        "                b_d_l, b_n_l = np.array(b_d_l), np.array(b_n_l)\n",
        "                b_l, p_l = np.array(b_l), np.array(p_l)\n",
        "                if epoch % self.params['verbose'] == 0:\n",
        "                    print(print_base.format(epoch+1, format(u_l, '.20f')[:10],\n",
        "                                            format(i_l, '.20f')[:10],\n",
        "                                            format(d_l, '.20f')[:10],\n",
        "                                            format(n_l, '.20f')[:10],\n",
        "                                            format(b_i_l, '.20f')[:10],\n",
        "                                            format(b_d_l, '.20f')[:10],\n",
        "                                            format(b_n_l, '.20f')[:10],\n",
        "                                            format(b_l, '.20f')[:10],\n",
        "                                            format(p_l, '.20f')[:10]))\n",
        "                self.u_losses.append(u_l)\n",
        "                self.i_losses.append(i_l)\n",
        "                self.d_losses.append(d_l)\n",
        "                self.n_losses.append(n_l)\n",
        "                self.b_i_losses.append(b_i_l)\n",
        "                self.b_d_losses.append(b_d_l)\n",
        "                self.b_n_losses.append(b_d_l)\n",
        "                self.b_losses.append(b_l)\n",
        "                self.p_losses.append(p_l)\n",
        "                #Eventually, update wandb\n",
        "                if wandb_run != None:\n",
        "                    wandb_run.log({'Unsupervised':u_l,\n",
        "                                  'Supervised':i_l + d_l + n_l,\n",
        "                                  'Total':p_l,\n",
        "                                   'Free Boundary':b_l,\n",
        "                                   'Initial':i_l,\n",
        "                                   'Dirichlet':d_l,\n",
        "                                   'Neumann':n_l,\n",
        "                                   'FB_Init':b_i_l,\n",
        "                                   'FB_Dir':b_d_l,\n",
        "                                   'FB_Neu':b_n_l})\n",
        "                #Check for the early warning\n",
        "                if p_l <= self.early_warning['Target']:\n",
        "                    self.early_warning['Target'] = p_l\n",
        "                    self.early_warning['n_steps'] = 0\n",
        "                    self.early_warning['top_mdl'] = old_top_mdl\n",
        "                    self.early_warning['weights'] = old_weights\n",
        "                else:\n",
        "                    self.early_warning['n_steps'] += 1\n",
        "                    if self.early_warning['n_steps'] >= self.params['patience']:\n",
        "                        break\n",
        "                #Save model and weights for next step early warning\n",
        "                old_top_mdl = tf.keras.models.clone_model( self.mdl )\n",
        "                old_weights = self.mdl.get_weights()\n",
        "        else:\n",
        "            #Before freezing, both mdl and fb are training\n",
        "            for epoch in tqdm(range(self.params['fb_freezing']),\n",
        "                              desc='PINNs - Training'):\n",
        "                if epoch == 0:\n",
        "                    print(print_base.format('Epoch', 'Unsupervised', 'Initial',\n",
        "                                            'Dirichlet', 'Neumann',\n",
        "                                            'FB_Init', 'FB_Dir', 'FB_Neu',\n",
        "                                            'Free Boundary', 'Total'))\n",
        "                    print('\\n')\n",
        "                #Case 1: more mdl steps for a single fb step\n",
        "                for _ in range(self.params['steps_fb_per_pde'] -1):\n",
        "                    self.train_mdl_solo();\n",
        "                #Case 2: more fb steps for a single mdl step\n",
        "                for _ in range(0, self.params['steps_fb_per_pde'] +1, -1):\n",
        "                    self.train_fb_solo();\n",
        "                u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l =\\\n",
        "                self.train_step()\n",
        "                u_l, i_l = np.array(u_l), np.array(i_l)\n",
        "                d_l, n_l = np.array(d_l), np.array(n_l)\n",
        "                b_i_l = np.array(b_i_l)\n",
        "                b_d_l, b_n_l = np.array(b_d_l), np.array(b_n_l)\n",
        "                b_l, p_l = np.array(b_l), np.array(p_l)\n",
        "                if epoch % self.params['verbose'] == 0:\n",
        "                    print(print_base.format(epoch+1, format(u_l, '.20f')[:10],\n",
        "                                            format(i_l, '.20f')[:10],\n",
        "                                            format(d_l, '.20f')[:10],\n",
        "                                            format(n_l, '.20f')[:10],\n",
        "                                            format(b_i_l, '.20f')[:10],\n",
        "                                            format(b_d_l, '.20f')[:10],\n",
        "                                            format(b_n_l, '.20f')[:10],\n",
        "                                            format(b_l, '.20f')[:10],\n",
        "                                            format(p_l, '.20f')[:10]))\n",
        "                self.u_losses.append(u_l)\n",
        "                self.i_losses.append(i_l)\n",
        "                self.d_losses.append(d_l)\n",
        "                self.n_losses.append(n_l)\n",
        "                self.b_i_losses.append(b_i_l)\n",
        "                self.b_d_losses.append(b_d_l)\n",
        "                self.b_n_losses.append(b_d_l)\n",
        "                self.b_losses.append(b_l)\n",
        "                self.p_losses.append(p_l)\n",
        "                #Eventually, update wandb\n",
        "                if wandb_run != None:\n",
        "                    wandb_run.log({'Unsupervised':u_l,\n",
        "                                  'Supervised':i_l + d_l + n_l,\n",
        "                                  'Total':p_l,\n",
        "                                   'Free Boundary':b_l,\n",
        "                                   'Initial':i_l,\n",
        "                                   'Dirichlet':d_l,\n",
        "                                   'Neumann':n_l,\n",
        "                                   'FB_Init':b_i_l,\n",
        "                                   'FB_Dir':b_d_l,\n",
        "                                   'FB_Neu':b_n_l})\n",
        "                #Check for the early warning\n",
        "                if p_l <= self.early_warning['Target']:\n",
        "                    self.early_warning['Target'] = p_l\n",
        "                    self.early_warning['n_steps'] = 0\n",
        "                    self.early_warning['top_mdl'] = old_top_mdl\n",
        "                    self.early_warning['weights'] = old_weights\n",
        "                else:\n",
        "                    self.early_warning['n_steps'] += 1\n",
        "                    if self.early_warning['n_steps'] >= self.params['patience']:\n",
        "                        break\n",
        "                #Save model and weights for next step early warning\n",
        "                old_top_mdl = tf.keras.models.clone_model( self.mdl )\n",
        "                old_weights = self.mdl.get_weights()\n",
        "            #Now, freeze fb and train only mdl\n",
        "            for epoch in tqdm(range(self.params['fb_freezing'],\n",
        "                                    self.params['epochs']),\n",
        "                              desc='PINNs - Training; Free Boundary Fixed'):\n",
        "                u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l =\\\n",
        "                self.train_mdl_solo()\n",
        "                u_l, i_l = np.array(u_l), np.array(i_l)\n",
        "                d_l, n_l = np.array(d_l), np.array(n_l)\n",
        "                b_i_l = np.array(b_i_l)\n",
        "                b_d_l, b_n_l = np.array(b_d_l), np.array(b_n_l)\n",
        "                b_l, p_l = np.array(b_l), np.array(p_l)\n",
        "                if epoch % self.params['verbose'] == 0:\n",
        "                    print(print_base.format(epoch+1, format(u_l, '.20f')[:10],\n",
        "                                            format(i_l, '.20f')[:10],\n",
        "                                            format(d_l, '.20f')[:10],\n",
        "                                            format(n_l, '.20f')[:10],\n",
        "                                            format(b_i_l, '.20f')[:10],\n",
        "                                            format(b_d_l, '.20f')[:10],\n",
        "                                            format(b_n_l, '.20f')[:10],\n",
        "                                            format(b_l, '.20f')[:10],\n",
        "                                            format(p_l, '.20f')[:10]))\n",
        "                self.u_losses.append(u_l)\n",
        "                self.i_losses.append(i_l)\n",
        "                self.d_losses.append(d_l)\n",
        "                self.n_losses.append(n_l)\n",
        "                self.b_i_losses.append(b_i_l)\n",
        "                self.b_d_losses.append(b_d_l)\n",
        "                self.b_n_losses.append(b_d_l)\n",
        "                self.b_losses.append(b_l)\n",
        "                self.p_losses.append(p_l)\n",
        "                #Eventually, update wandb\n",
        "                if wandb_run != None:\n",
        "                    wandb_run.log({'Unsupervised':u_l,\n",
        "                                  'Supervised':i_l + d_l + n_l,\n",
        "                                  'Total':p_l,\n",
        "                                   'Free Boundary':b_l,\n",
        "                                   'Initial':i_l,\n",
        "                                   'Dirichlet':d_l,\n",
        "                                   'Neumann':n_l,\n",
        "                                   'FB_Init':b_i_l,\n",
        "                                   'FB_Dir':b_d_l,\n",
        "                                   'FB_Neu':b_n_l})\n",
        "                #Check for the early warning\n",
        "                if p_l <= self.early_warning['Target']:\n",
        "                    self.early_warning['Target'] = p_l\n",
        "                    self.early_warning['n_steps'] = 0\n",
        "                    self.early_warning['top_mdl'] = old_top_mdl\n",
        "                    self.early_warning['weights'] = old_weights\n",
        "                else:\n",
        "                    self.early_warning['n_steps'] += 1\n",
        "                    if self.early_warning['n_steps'] >= self.params['patience']:\n",
        "                        break\n",
        "                #Save model and weights for next step early warning\n",
        "                old_top_mdl = tf.keras.models.clone_model( self.mdl )\n",
        "                old_weights = self.mdl.get_weights()\n",
        "        #Recover information about optimal epoch in early warning\n",
        "        self.mdl = tf.keras.models.clone_model( self.early_warning['top_mdl'] )\n",
        "        self.mdl.set_weights(self.early_warning['weights'])\n",
        "        top_epoch = epoch+1 - self.early_warning[\"n_steps\"]\n",
        "        print(f'Best loss achieved at step {top_epoch}')\n",
        "\n",
        "    def plot_losses(self):\n",
        "        plt.figure( figsize=(12,8) )\n",
        "        plt.semilogy(self.u_losses, label='Unsupervised')\n",
        "        plt.semilogy(np.array(self.i_losses) +\\\n",
        "                     np.array(self.d_losses) +\\\n",
        "                     np.array(self.n_losses),\n",
        "                     label='Supervised')\n",
        "        plt.semilogy(self.b_losses, label='Free Boundary')\n",
        "        plt.semilogy(self.p_losses, label='PINN')\n",
        "        plt.legend()\n",
        "        plt.title('Losses')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_unsupervised_test(self, test_sampler, test_ibc_cond,\n",
        "                               test_ibc_fb_cond, to_print=True, output=False):\n",
        "        #--------------- Compute Free Boundary losses\n",
        "        #Compute Initial Free Boundary Condition\n",
        "        if test_ibc_fb_cond['Initial'] != None:\n",
        "            fb_init = self.fb(test_ibc_fb_cond['Initial'].x,\n",
        "                                  training=False)\n",
        "            fb_init_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(fb_init-test_ibc_fb_cond['Initial'].y)\n",
        "                )\n",
        "        else:\n",
        "            fb_init_loss = 0\n",
        "        #Compute Dirichlet Free Boundary Condition\n",
        "        if test_ibc_fb_cond['Dirichlet'] != None:\n",
        "            #Compute Free Boundary values\n",
        "            s_values = self.fb(test_ibc_fb_cond['Dirichlet'].x,\n",
        "                                training=True)\n",
        "            fb_dc = self.mdl(tf.concat(\n",
        "                [s_values,\n",
        "                  test_ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                  training=True)\n",
        "            fb_dir_target = tf.nn.relu(\n",
        "                tf.ones_like(\n",
        "                    s_values[:,-1]\n",
        "                    ) * K - tf.math.reduce_min(s_values, axis=1)\n",
        "            )\n",
        "            fb_dir_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(fb_dc - fb_dir_target)\n",
        "                )\n",
        "        else:\n",
        "            fb_dir_loss = 0\n",
        "        #Compute Neumann Free Boundary Condition\n",
        "        if test_ibc_fb_cond['Neumann'] != None:\n",
        "            s_values = self.fb(test_ibc_fb_cond['Neumann'].x,\n",
        "                                training=True)\n",
        "            with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                neu_fb_tape.watch(s_values)\n",
        "                pinn_nc_fb = self.mdl(\n",
        "                    tf.concat([s_values,\n",
        "                                test_ibc_fb_cond['Neumann'].x],\n",
        "                              axis=1),\n",
        "                              training=True)\n",
        "            pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                              s_values)\n",
        "            fb_neu_target = -tf.one_hot(\n",
        "                tf.math.argmin( s_values, axis=1),\n",
        "                depth=n_dim\n",
        "                )\n",
        "            fb_neu_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(pinn_nc_fb-fb_neu_target)\n",
        "                )\n",
        "        else:\n",
        "            fb_neu_loss = 0\n",
        "        #Compute final loss\n",
        "        fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "\n",
        "        #--------------- Compute PINN losses\n",
        "        #Compute unsupervised loss\n",
        "        s_values = self.fb(tf.concat([test_sampler.t], axis=-1),\n",
        "                            training=False)\n",
        "        temp = tf.reduce_sum(tf.cast(test_sampler.x < s_values,\n",
        "                                      dtype=self.DTYPE),\n",
        "                            axis=-1) < tf.ones(test_sampler.t.shape[0])\n",
        "        x_f = test_sampler.x[ temp ]\n",
        "        t_f = tf.reshape(test_sampler.t[ temp ], (-1,1) )\n",
        "        variables = list()\n",
        "        derivatives = list()\n",
        "        for i in range(x_f.shape[1]):\n",
        "            variables.append( x_f[:, i:i+1] )\n",
        "        variables.append( t_f )\n",
        "        with G_Tape(persistent=True,\n",
        "                    watch_accessed_variables=False) as pinn_tape:\n",
        "            #Watch independet variables\n",
        "            for var in variables:\n",
        "                pinn_tape.watch( var )\n",
        "            pinn_tape.watch(t_f)\n",
        "            u_val = self.mdl(tf.concat(variables, axis=1),\n",
        "                                training=True)\n",
        "            u_x = pinn_tape.gradient(u_val, x_f)\n",
        "            for i, var in enumerate(variables[:-1]):\n",
        "                derivatives.append( pinn_tape.gradient(u_val, var) )\n",
        "        unsup_loss = tf.reduce_mean(tf.square(\n",
        "            self.pde(pinn_tape, variables, u_val, derivatives) ))\n",
        "        #Compute Initial Condition\n",
        "        if test_ibc_cond['Initial'] != None:\n",
        "            pinn_init = self.mdl(test_ibc_cond['Initial'].x,\n",
        "                                  training=False)\n",
        "            if len(pinn_init.shape) > 1:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  tf.expand_dims(test_ibc_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  test_ibc_cond['Initial'].y )\n",
        "                    )\n",
        "        else:\n",
        "            init_loss = 0\n",
        "        #Compute Dirichlet Boundary Condition\n",
        "        if test_ibc_cond['Dirichlet'] != None:\n",
        "            pinn_dc = self.mdl(test_ibc_cond['Dirichlet'].x,\n",
        "                                training=True)\n",
        "            if len(pinn_dc.shape) > 1:\n",
        "                dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_dc -\\\n",
        "                                  tf.expand_dims(test_ibc_cond['Dirichlet'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_dc -\\\n",
        "                                  test_ibc_cond['Dirichlet'].y )\n",
        "                    )\n",
        "        else:\n",
        "            dir_loss = 0\n",
        "        #Compute Neumann Boundary Condition\n",
        "        if test_ibc_cond['Neumann'] != None:\n",
        "            with G_Tape(watch_accessed_variables=False) as neu_tape:\n",
        "                neu_tape.watch(test_ibc_cond['Neumann'].x)\n",
        "                pinn_nc = self.mdl(tf.concat([test_ibc_cond['Neumann'].x,\n",
        "                                              test_ibc_cond['Neumann'].t],\n",
        "                                            axis=1),\n",
        "                                    training=False)\n",
        "            pinn_nc = neu_tape.gradient(pinn_nc, test_ibc_cond['Neumann'].x)\n",
        "            neu_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(pinn_nc-test_ibc_cond['Neumann'].y)\n",
        "                )\n",
        "        else:\n",
        "            neu_loss = 0\n",
        "\n",
        "        #--------------- Compute total loss\n",
        "        pinn_loss = unsup_loss + init_loss + dir_loss +\\\n",
        "        neu_loss + fb_dir_loss + fb_neu_loss\n",
        "\n",
        "        u_l, i_l = np.array(unsup_loss), np.array(init_loss)\n",
        "        d_l, n_l = np.array(dir_loss), np.array(neu_loss)\n",
        "        b_i_l = np.array(fb_init_loss)\n",
        "        b_d_l, b_n_l = np.array(fb_dir_loss), np.array(fb_neu_loss)\n",
        "        b_l, p_l = np.array(fb_loss), np.array(pinn_loss)\n",
        "\n",
        "        if to_print:\n",
        "            print_base = \"{:<10}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}\"\n",
        "            print(print_base.format('', 'Unsupervised', 'Initial',\n",
        "                                    'Dirichlet', 'Neumann', 'FB_Init', 'FB_Dir',\n",
        "                                    'FB_Neu', 'Free Boundary', 'Total'))\n",
        "            print(print_base.format('', format(u_l, '.20f')[:10],\n",
        "                                    format(i_l, '.20f')[:10],\n",
        "                                    format(d_l, '.20f')[:10],\n",
        "                                    format(n_l, '.20f')[:10],\n",
        "                                    format(b_i_l, '.20f')[:10],\n",
        "                                    format(b_d_l, '.20f')[:10],\n",
        "                                    format(b_n_l, '.20f')[:10],\n",
        "                                    format(b_l, '.20f')[:10],\n",
        "                                    format(p_l, '.20f')[:10]))\n",
        "        if output:\n",
        "            return u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyxPC8nb5gy4"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M89TygzQ5Ogn"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay as ex_d\n",
        "my_lr = ex_d(1e-2, 640, 0.975, staircase=False)\n",
        "fb_lr = ex_d(1e-2, 160, 0.975, staircase=False)\n",
        "\n",
        "params = {'sample_method':'sobol',\n",
        "          'layers':[n_dim+1, 20, 20, 20, 20, 20, 20, 20, 20, 1],\n",
        "          'activation':'tanh',\n",
        "          'output_act':'linear', 'initializer':'glorot_normal',\n",
        "          'fb_layers':[1, 100, 100, 100, n_dim], 'fb_activation':'tanh',\n",
        "          'fb_output_act':'linear', 'fb_initializer':'glorot_normal',\n",
        "          'lr':my_lr, 'optimizer':'rmsprop',\n",
        "          'fb_lr':fb_lr, 'fb_optimizer':'rmsprop', 'steps_fb_per_pde':4,\n",
        "          'pde_weight':1, 'epochs':25000, 'verbose':100}\n",
        "\n",
        "my_pinn = FreeBoundary_PINN(params, pde, pinn_conditions,\n",
        "                            fb_conditions, lb, ub, N_f=120000, DTYPE=DTYPE)\n",
        "\n",
        "START = time.time()\n",
        "my_pinn.fit()\n",
        "train_time = time.time() - START\n",
        "my_pinn.plot_losses()\n",
        "\n",
        "START = time.time()\n",
        "values = my_pinn.plot_unsupervised_test(test_sampler, pinn_cond_test,\n",
        "                                        fb_cond_test, output=True)\n",
        "test_time = time.time() - START\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9OrJcIrEOeX"
      },
      "source": [
        "# 1D Semi-parametric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWPYzfOvERuk"
      },
      "source": [
        "### Define problem, initial and boundary conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PNqkxzIERul"
      },
      "outputs": [],
      "source": [
        "\n",
        "import time\n",
        "import wandb\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from math import pi, exp\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "#Define global variables\n",
        "n_dim = 1\n",
        "DTYPE = 'float32'\n",
        "\n",
        "#Fix seeds\n",
        "random_seed = 2\n",
        "np.random.seed(random_seed)\n",
        "tf.random.set_seed(random_seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RgIJuW6ERul"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Sampler_IBC():\n",
        "    def __init__(self, lb, ub, cond=None, N_points=100,\n",
        "                 method='sobol', grid=None, this_initial_cond=False,\n",
        "                 split=False, params_split=False, DTYPE='float64'):\n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "        self.cond = cond\n",
        "        self.DTYPE = DTYPE\n",
        "        self.sample(N_points, method, grid, split,\n",
        "                    params_split, this_initial_cond)\n",
        "\n",
        "    def sample(self, N_points, method, grid,\n",
        "               split, params_split, this_initial_cond):\n",
        "        if method == 'uniform':\n",
        "            x_ibc = np.random.uniform(0, 1, size=(N_points, self.ub.shape[0]))\n",
        "            x_ibc = self.lb + (self.ub - self.lb)*x_ibc\n",
        "        elif method == 'latin':\n",
        "            from pyDOE import lhs\n",
        "            x_ibc = self.lb + (self.ub - self.lb)*lhs(self.ub.shape[0],N_points)\n",
        "        elif method == 'sobol':\n",
        "            import sobol\n",
        "            x_ibc = sobol.sample(dimension=self.ub.shape[0], n_points=N_points)\n",
        "            x_ibc = self.lb + (self.ub - self.lb)*x_ibc\n",
        "        elif method == 'equi':\n",
        "            x_ibc = np.linspace(self.lb, self.ub, N_points)\n",
        "        elif method == 'grid':\n",
        "            x_ibc = np.linspace(self.lb, self.ub, N_points).T\n",
        "            temp_final = list()\n",
        "            for val in x_ibc[0]:\n",
        "                temp_final.append( [val] )\n",
        "            dim = 1\n",
        "            while dim < x_ibc.shape[0]:\n",
        "                temp = list()\n",
        "                for t1 in range(x_ibc.shape[1]):\n",
        "                    for t2 in range(len(temp_final)):\n",
        "                        temp_val = temp_final[t2].copy()\n",
        "                        temp_val.append( x_ibc[dim, t1] )\n",
        "                        temp.append( temp_val )\n",
        "                temp_final = temp\n",
        "                dim += 1\n",
        "            x_ibc = np.array(temp_final)\n",
        "        elif method == 'grid_old':\n",
        "            idx = np.random.choice(range(grid.shape[0]),N_points,replace=False)\n",
        "            x_ibc = grid[idx]\n",
        "        #Check and, eventually, inversion of t and T\n",
        "        x_ibc[:,-5] = x_ibc[:,-5] * x_ibc[:,-2]\n",
        "        if this_initial_cond:\n",
        "            x_ibc[:,-5] = x_ibc[:,-2]\n",
        "        #Define y\n",
        "        if self.cond != None:\n",
        "            y_ibc = self.cond(x_ibc)\n",
        "            self.y = tf.cast(tf.Variable(y_ibc, trainable=False ),\n",
        "                             self.DTYPE)\n",
        "        if params_split:\n",
        "            x_ibc, m_p = x_ibc[:, :-4], x_ibc[:, -4:]\n",
        "            self.m_p = tf.cast(tf.Variable(m_p, trainable=False ),\n",
        "                             self.DTYPE)\n",
        "        if split:\n",
        "            x_ibc, t_ibc = x_ibc[:, :-1], x_ibc[:, -1:]\n",
        "            self.t = tf.cast(tf.Variable(t_ibc, trainable=False ),\n",
        "                             self.DTYPE)\n",
        "        self.x = tf.cast(tf.Variable(x_ibc, trainable=False ),\n",
        "                         self.DTYPE)\n",
        "\n",
        "#Define domain boundaries\n",
        "lb = np.array([0., 0., 0.01, 0.001, 3, 8])\n",
        "ub = np.array([30, 1., 0.01, 0.005, 3, 12])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KF56RzUERum"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Problem Definition\n",
        "def h_1(inp):\n",
        "    res = list()\n",
        "    for inp_val in inp:\n",
        "        res.append( K - inp_val[0] )\n",
        "    return np.array(res)\n",
        "\n",
        "def h_2(inp):\n",
        "    res = - np.ones( inp.shape[0] )\n",
        "    return res\n",
        "\n",
        "def g(inp):\n",
        "    res = np.zeros( inp.shape[0] )\n",
        "    return np.array(res)\n",
        "\n",
        "def u_0(inp):\n",
        "    res = list()\n",
        "    for inp_val in inp:\n",
        "        res.append( np.max([0, inp_val[-1]-inp_val[0]]) )\n",
        "    return np.array(res)\n",
        "\n",
        "def s_0(inp):\n",
        "    res = list()\n",
        "    for inp_val in inp:\n",
        "        res.append( inp_val[-1] )\n",
        "    return np.array(res)\n",
        "\n",
        "#Point sampling\n",
        "N_to_sample = 9000\n",
        "N_to_sample_reduced = 4500\n",
        "\n",
        "#---------------- PDE Conditions\n",
        "print('PDE COnditions\\n')\n",
        "#Initial\n",
        "init_sampler = Sampler_IBC(np.array([0., -1, 0.01, 0.001, 3, 8]),\n",
        "                           np.array([30, -1, 0.01, 0.005, 3, 12]),\n",
        "                           u_0, N_to_sample,\n",
        "                           DTYPE=DTYPE, this_initial_cond=True )\n",
        "print( f'x: {init_sampler.x.shape}     y: {init_sampler.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "dir_sampler = Sampler_IBC(np.array([30, 0., 0.01, 0.001, 3, 8]),\n",
        "                          np.array([30, 1., 0.01, 0.005, 3, 12]),\n",
        "                          g, N_to_sample, DTYPE=DTYPE )\n",
        "print( f'x: {dir_sampler.x.shape}     y: {dir_sampler.y.shape}' )\n",
        "\n",
        "#Neumann\n",
        "print('No')\n",
        "\n",
        "#---------------- Free Boundary Conditions\n",
        "print('\\nFree Boundary COnditions\\n')\n",
        "#Initial\n",
        "init_fb_sampler = Sampler_IBC(np.array([-1, 0.01, 0.001, 3, 8]),\n",
        "                              np.array([-1, 0.01, 0.005, 3, 12]),\n",
        "                              s_0, N_to_sample_reduced,\n",
        "                              DTYPE=DTYPE, this_initial_cond=True )\n",
        "print( f't: {init_fb_sampler.x.shape}     y: {init_fb_sampler.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "dir_fb_sampler = Sampler_IBC(np.array([0., 0.01, 0.001, 3, 8]),\n",
        "                             np.array([1., 0.01, 0.005, 3, 12]),\n",
        "                             None, N_to_sample, DTYPE=DTYPE )\n",
        "print( f't: {dir_fb_sampler.x.shape}' )\n",
        "\n",
        "#Neumann\n",
        "neu_fb_sampler = Sampler_IBC(np.array([0., 0.01, 0.001, 3, 8]),\n",
        "                             np.array([1., 0.01, 0.005, 3, 12]),\n",
        "                             h_2, N_to_sample, DTYPE=DTYPE )\n",
        "print( f't: {neu_fb_sampler.x.shape}     y: {neu_fb_sampler.y.shape}' )\n",
        "\n",
        "#---------------- Conditions passed to PINN\n",
        "pinn_conditions = {'Initial':init_sampler,\n",
        "                   'Dirichlet':dir_sampler,\n",
        "                   'Neumann':None}\n",
        "fb_conditions = {'Initial':init_fb_sampler,\n",
        "                 'Dirichlet':dir_fb_sampler,\n",
        "                 'Neumann':neu_fb_sampler}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xc87CowtERup"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_sampler = Sampler_IBC(lb, ub, cond=None, DTYPE=DTYPE,\n",
        "                           N_points=10000, method='uniform', split=True, params_split=True)\n",
        "\n",
        "\n",
        "#Point sampling\n",
        "sample_to_test = 1000\n",
        "\n",
        "#---------------- PDE Conditions\n",
        "print('PDE COnditions\\n')\n",
        "#Initial\n",
        "test_init_sampler = Sampler_IBC(np.array([0., 0, 0.01, 0.001, 3, 8]),\n",
        "                           np.array([30, 0, 0.01, 0.005, 3, 12]),\n",
        "                           u_0, sample_to_test, method='uniform',\n",
        "                           DTYPE=DTYPE, this_initial_cond=True )\n",
        "print( f'x: {test_init_sampler.x.shape}     y: {test_init_sampler.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "test_dir_sampler = Sampler_IBC(np.array([30, 0., 0.01, 0.001, 3, 8]),\n",
        "                          np.array([30, 1., 0.01, 0.005, 3, 12]),\n",
        "                          g, sample_to_test, DTYPE=DTYPE, method='uniform' )\n",
        "print( f'x: {test_dir_sampler.x.shape}     y: {test_dir_sampler.y.shape}' )\n",
        "\n",
        "#Neumann\n",
        "print('No')\n",
        "\n",
        "#---------------- Free Boundary Conditions\n",
        "print('\\nFree Boundary COnditions\\n')\n",
        "#Initial\n",
        "test_init_fb_sampler = Sampler_IBC(np.array([0, 0.01, 0.001, 3, 8]),\n",
        "                              np.array([0, 0.01, 0.005, 3, 12]),\n",
        "                              s_0, sample_to_test, method='uniform',\n",
        "                              DTYPE=DTYPE, this_initial_cond=True )\n",
        "print(f't: {test_init_fb_sampler.x.shape}     y: {test_init_fb_sampler.y.shape}')\n",
        "\n",
        "#Dirichlet\n",
        "test_dir_fb_sampler = Sampler_IBC(np.array([0., 0.01, 0.001, 3, 8]),\n",
        "                             np.array([1., 0.01, 0.005, 3, 12]),\n",
        "                             None, sample_to_test, method='uniform', DTYPE=DTYPE )\n",
        "print( f't: {test_dir_fb_sampler.x.shape}' )\n",
        "\n",
        "#Neumann\n",
        "test_neu_fb_sampler = Sampler_IBC(np.array([0., 0.01, 0.001, 3, 8]),\n",
        "                             np.array([1., 0.01, 0.005, 3, 12]),\n",
        "                             h_2, sample_to_test, DTYPE=DTYPE, method='uniform' )\n",
        "print( f't: {test_neu_fb_sampler.x.shape}     y: {test_neu_fb_sampler.y.shape}' )\n",
        "\n",
        "#---------------- Conditions passed to PINN\n",
        "pinn_cond_test = {'Initial':test_init_sampler,\n",
        "                  'Dirichlet':test_dir_sampler,\n",
        "                  'Neumann':None}\n",
        "fb_cond_test = {'Initial':test_init_fb_sampler,\n",
        "                'Dirichlet':test_dir_fb_sampler,\n",
        "                'Neumann':test_neu_fb_sampler}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbrOGK3GERuq"
      },
      "source": [
        "### PINN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPR-d_fmERuq"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow import GradientTape as G_Tape\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.initializers import GlorotNormal, GlorotUniform\n",
        "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
        "from tensorflow.keras.metrics import mean_squared_error\n",
        "\n",
        "class FreeBoundary_PINN():\n",
        "    def __init__(self, params, ibc_cond, ibc_fb_cond, lb, ub,\n",
        "                 N_f=10000, N_fb_ibc=150, DTYPE='float64', coll_points=None):\n",
        "        self.params = params\n",
        "        self.DTYPE = DTYPE\n",
        "        self.Default_Params()\n",
        "        #Set seed\n",
        "        if self.params['seed'] != None:\n",
        "            tf.keras.utils.set_random_seed( self.params['seed'] )\n",
        "            tf.config.experimental.enable_op_determinism()\n",
        "        #Define intial and boundary conditions\n",
        "        self.ibc_cond = ibc_cond\n",
        "        self.ibc_fb_cond = ibc_fb_cond\n",
        "        #All needed for points sampling\n",
        "        self.lb = tf.Variable(lb, trainable=False)\n",
        "        self.ub = tf.Variable(ub, trainable=False)\n",
        "        if coll_points == None:\n",
        "            self.N_f = N_f\n",
        "            self.Sample_Points()\n",
        "        else:\n",
        "            self.x_f = coll_points[0]\n",
        "            self.t_f = coll_points[1]\n",
        "            self.m_p = coll_points[2:]\n",
        "        self.N_fb_ibc = N_fb_ibc\n",
        "        #Initialize the class: define the network\n",
        "        self.Define_Regularizer()\n",
        "        self.Define_Initializer()\n",
        "        self.Define_Optimizer()\n",
        "        self.Create_Network()\n",
        "        self.Create_FB_Network()\n",
        "\n",
        "    def Default_Params(self):\n",
        "        target = self.params.keys()\n",
        "        if 'seed' not in target:\n",
        "            self.params['seed'] = None\n",
        "        if 'optimizer' not in target:\n",
        "            self.params['optimizer'] = 'Adam'\n",
        "        if 'fb_optimizer' not in target:\n",
        "            self.params['fb_optimizer'] = 'Adam'\n",
        "        if 'reg_type' not in target:\n",
        "            self.params['reg_type'] = None\n",
        "        if 'initializer' not in target:\n",
        "            self.params['initializer'] = 'glorot_normal'\n",
        "        if 'activation' not in target:\n",
        "            self.params['activation'] = 'tanh'\n",
        "        if 'output_act' not in target:\n",
        "            self.params['output_act'] = 'linear'\n",
        "        if 'pde_weight' not in target:\n",
        "            self.params['pde_weight'] = 1.\n",
        "        if 'sup_weight' not in target:\n",
        "            self.params['sup_weight'] = [1., 1., 1.]\n",
        "        if 'fb_weight' not in target:\n",
        "            self.params['fb_weight'] = [1., 1., 1.]\n",
        "        if 'patience' not in target:\n",
        "            self.params['patience'] = np.inf\n",
        "        if 'sample_method' not in target:\n",
        "            self.params['sample_method'] = 'uniform'\n",
        "        if 'fb_output_act' not in target:\n",
        "            self.params['fb_output_act'] = 'linear'\n",
        "        if 'fb_activation' not in target:\n",
        "            self.params['fb_activation'] = 'tanh'\n",
        "        if 'verbose' not in target:\n",
        "            self.params['verbose'] = 1\n",
        "        if 'steps_fb_per_pde' not in target:\n",
        "            self.params['steps_fb_per_pde'] = 1\n",
        "        if 'fb_freezing' not in target:\n",
        "            self.params['fb_freezing'] = None\n",
        "\n",
        "    def Define_Regularizer(self):\n",
        "        if self.params['reg_type'] == 'l1':\n",
        "            self.regularizer = l1( self.params['reg'] )\n",
        "        elif self.params['reg_type'] == 'l2':\n",
        "            self.regularizer = l2( self.params['reg'] )\n",
        "        elif self.params['reg_type'] == 'l1_l2':\n",
        "            self.regularizer = l1_l2( self.params['reg'][0],\n",
        "                                      self.params['reg'][1] )\n",
        "        else:\n",
        "            self.regularizer = None\n",
        "\n",
        "    def Define_Initializer(self):\n",
        "        if self.params['initializer'] == 'glorot_normal':\n",
        "            self.initializer = GlorotNormal()\n",
        "        elif self.params['initializer'] == 'glorot_uniform':\n",
        "            self.initializer = GlorotUniform()\n",
        "        else:\n",
        "            self.initializer = None\n",
        "\n",
        "    def Define_Optimizer(self):\n",
        "        temp = self.params['optimizer']\n",
        "        if temp.lower() == 'adam':\n",
        "            self.opt = Adam( self.params['lr'] )\n",
        "        elif temp.lower() == 'rmsprop':\n",
        "            self.opt = RMSprop( self.params['lr'] )\n",
        "        else:\n",
        "            raise ValueError(f\"Optimizer {temp} not recognized\")\n",
        "\n",
        "        temp = self.params['fb_optimizer']\n",
        "        if temp.lower() == 'adam':\n",
        "            self.fb_opt = Adam( self.params['fb_lr'] )\n",
        "        elif temp.lower() == 'rmsprop':\n",
        "            self.fb_opt = RMSprop( self.params['fb_lr'] )\n",
        "        else:\n",
        "            raise ValueError(f\"fb_Optimizer {temp} not recognized\")\n",
        "\n",
        "    def Create_Network(self):\n",
        "        input_layer = Input(shape=self.params['layers'][0],\n",
        "                            name = 'Input')\n",
        "        x = Dense(units=self.params['layers'][1],\n",
        "                  activation=self.params['activation'],\n",
        "                  kernel_initializer=self.initializer,\n",
        "                  kernel_regularizer=self.regularizer,\n",
        "                  name='Dense_1')(input_layer)\n",
        "        for layer in range(2, len(self.params['layers'])-1):\n",
        "            x = Dense(units=self.params['layers'][layer],\n",
        "                      activation=self.params['activation'],\n",
        "                      kernel_initializer=self.initializer,\n",
        "                      kernel_regularizer=self.regularizer,\n",
        "                      name=f'Dense_{layer}')(x)\n",
        "        output = Dense(units=self.params['layers'][-1],\n",
        "                       activation=self.params['output_act'],\n",
        "                       kernel_initializer=self.initializer,\n",
        "                       kernel_regularizer=self.regularizer,\n",
        "                       name='Output')(x)\n",
        "        self.mdl = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "    def Create_FB_Network(self):\n",
        "        input_layer = Input(shape=self.params['fb_layers'][0],\n",
        "                            name = 'Input')\n",
        "        x = Dense(units=self.params['fb_layers'][1],\n",
        "                  activation=self.params['fb_activation'],\n",
        "                  kernel_initializer=self.initializer,\n",
        "                  kernel_regularizer=self.regularizer,\n",
        "                  name='Dense_1')(input_layer)\n",
        "        for layer in range(2, len(self.params['fb_layers'])-1):\n",
        "            x = Dense(units=self.params['fb_layers'][layer],\n",
        "                      activation=self.params['fb_activation'],\n",
        "                      kernel_initializer=self.initializer,\n",
        "                      kernel_regularizer=self.regularizer,\n",
        "                      name=f'Dense_{layer}')(x)\n",
        "        output = Dense(units=self.params['fb_layers'][-1],\n",
        "                       activation=self.params['fb_output_act'],\n",
        "                       kernel_initializer=self.initializer,\n",
        "                       kernel_regularizer=self.regularizer,\n",
        "                       name='Output')(x)\n",
        "        self.fb = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "    def Sample_Points(self):\n",
        "        #According to the selected method, sample collocation points\n",
        "        method = self.params['sample_method']\n",
        "        if method == 'latin':\n",
        "            from pyDOE import lhs\n",
        "            cps = self.lb + (self.ub - self.lb)*lhs(self.ub.shape[0], self.N_f)\n",
        "        elif method == 'uniform':\n",
        "            cps = np.random.uniform(0, 1, size=(self.N_f, self.ub.shape[0]))\n",
        "            cps = self.lb + (self.ub - self.lb)*cps\n",
        "        elif method == 'sobol':\n",
        "            import sobol\n",
        "            cps = sobol.sample(dimension=self.ub.shape[0], n_points=self.N_f)\n",
        "            cps = self.lb + (self.ub - self.lb)*cps\n",
        "        else:\n",
        "            raise ValueError(f'Sampling method {method} not recognized')\n",
        "        #Check and, eventually, invert t and T\n",
        "        cps = np.array(cps)\n",
        "        cps[:,-5] = cps[:,-5] * cps[:,-2]\n",
        "        #Return collocation points as tf tensors\n",
        "        self.x_f_total = tf.cast(tf.Variable(cps[:, :-5], trainable=False),\n",
        "                           self.DTYPE)\n",
        "        self.t_f_total = tf.cast(tf.Variable(cps[:, -5:-4], trainable=False),\n",
        "                           self.DTYPE)\n",
        "        self.m_p_total = tf.cast(tf.Variable(cps[:, -4:], trainable=False),\n",
        "                           self.DTYPE)\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self):\n",
        "        with G_Tape(watch_accessed_variables=False) as mdl_tape:\n",
        "            #Watch solution weights\n",
        "            mdl_tape.watch(self.mdl.trainable_variables)\n",
        "            #--------------- Compute Free Boundary losses\n",
        "            with G_Tape(watch_accessed_variables=False) as fb_tape:\n",
        "                #Watch free boundary weights\n",
        "                fb_tape.watch(self.fb.trainable_variables)\n",
        "                #Compute Initial Free Boundary Condition\n",
        "                fb_init = self.fb(self.ibc_fb_cond['Initial'].x,\n",
        "                                      training=True)\n",
        "                if len(fb_init.shape) > 1:\n",
        "                    fb_init_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(fb_init -\\\n",
        "                                      tf.expand_dims(self.ibc_fb_cond['Initial'].y,\n",
        "                                                     axis=-1 ) )\n",
        "                        )\n",
        "                else:\n",
        "                    fb_init_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(fb_init -\\\n",
        "                                      self.ibc_fb_cond['Initial'].y )\n",
        "                        )\n",
        "                #Compute Dirichlet Free Boundary Condition\n",
        "                if self.ibc_fb_cond['Dirichlet'] != None:\n",
        "                    #Compute Free Boundary values\n",
        "                    s_values = self.fb(self.ibc_fb_cond['Dirichlet'].x,\n",
        "                                       training=True)\n",
        "                    fb_dc = self.mdl(tf.concat(\n",
        "                        [s_values,\n",
        "                         self.ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                         training=True)\n",
        "                    fb_dir_target = tf.nn.relu(\n",
        "                        tf.ones_like(\n",
        "                            s_values\n",
        "                            ) * self.ibc_fb_cond['Dirichlet'].x[:,-1] - s_values\n",
        "                    )\n",
        "                    fb_dir_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(fb_dc - fb_dir_target)\n",
        "                        )\n",
        "                else:\n",
        "                    fb_dir_loss = 0\n",
        "                #Compute Neumann Free Boundary Condition\n",
        "                if self.ibc_fb_cond['Neumann'] != None:\n",
        "                    s_values = self.fb(self.ibc_fb_cond['Neumann'].x,\n",
        "                                       training=True)\n",
        "                    with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                        neu_fb_tape.watch(s_values)\n",
        "                        pinn_nc_fb = self.mdl(\n",
        "                            tf.concat([s_values,\n",
        "                                       self.ibc_fb_cond['Neumann'].x],\n",
        "                                      axis=1),\n",
        "                                      training=True)\n",
        "                    pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                                      s_values)\n",
        "                    fb_neu_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(pinn_nc_fb-self.ibc_fb_cond['Neumann'].y)\n",
        "                        )\n",
        "                else:\n",
        "                    fb_neu_loss = 0\n",
        "                #Compute final loss\n",
        "                fb_loss = self.params['fb_weight'][0] * fb_init_loss +\\\n",
        "                self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "                self.params['fb_weight'][2] * fb_neu_loss\n",
        "            #Compute gradient and apply optimizers for free boundary\n",
        "            gradient_fb = fb_tape.gradient(fb_loss,\n",
        "                                          self.fb.trainable_variables)\n",
        "            self.fb_opt.apply_gradients( zip(gradient_fb,\n",
        "                                          self.fb.trainable_variables) )\n",
        "\n",
        "            #--------------- Compute PINN losses\n",
        "            #Compute unsupervised loss\n",
        "            s_values = self.fb(tf.concat([self.t_f_total,\n",
        "                                          self.m_p_total], axis=-1),\n",
        "                               training=False)\n",
        "            x_f = tf.reshape(self.x_f_total[ self.x_f_total > s_values ],\n",
        "                             (-1,1) )\n",
        "            t_f = tf.reshape(self.t_f_total[ self.x_f_total > s_values ],\n",
        "                             (-1,1) )\n",
        "            m_p = self.m_p_total[ self.x_f_total[:, 0] > s_values[:, 0] ]\n",
        "            with G_Tape(persistent=True,\n",
        "                        watch_accessed_variables=False) as pinn_tape:\n",
        "                #Watch independet variables\n",
        "                pinn_tape.watch(x_f)\n",
        "                pinn_tape.watch(t_f)\n",
        "                #Apply u function for unsupervised\n",
        "                u_val = self.mdl(tf.concat([x_f, t_f, m_p],\n",
        "                                          axis=1),\n",
        "                                training=True)\n",
        "                u_x = pinn_tape.gradient(u_val, x_f)\n",
        "            u_xx = pinn_tape.gradient(u_x, x_f)\n",
        "            u_t = pinn_tape.gradient(u_val, t_f)\n",
        "            del(pinn_tape)\n",
        "            u_val = tf.cast(u_val, self.DTYPE)\n",
        "            f = (m_p[:,0:1] * x_f * u_x) + u_t +\\\n",
        "            (m_p[:,1:2] * x_f**2 * u_xx)/2 -\\\n",
        "            (m_p[:,0:1] * u_val)\n",
        "            unsup_loss = tf.reduce_mean(tf.square( f ))\n",
        "            #Compute Initial Condition\n",
        "            pinn_init = self.mdl(self.ibc_cond['Initial'].x, training=True)\n",
        "            if len(pinn_init.shape) > 1:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  tf.expand_dims(self.ibc_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  self.ibc_cond['Initial'].y )\n",
        "                    )\n",
        "            #Compute Dirichlet Boundary Condition\n",
        "            if self.ibc_cond['Dirichlet'] != None:\n",
        "                pinn_dc = self.mdl(self.ibc_cond['Dirichlet'].x, training=True)\n",
        "                dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_dc-self.ibc_cond['Dirichlet'].y)\n",
        "                    )\n",
        "            else:\n",
        "                dir_loss = 0\n",
        "            #Compute Neumann Boundary Condition\n",
        "            if self.ibc_cond['Neumann'] != None:\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_tape:\n",
        "                    neu_tape.watch(self.ibc_cond['Neumann'].x)\n",
        "                    pinn_nc = self.mdl(tf.concat([self.ibc_cond['Neumann'].x,\n",
        "                                                  self.ibc_cond['Neumann'].t,\n",
        "                                                  self.ibc_cond['Neumann'].m_p],\n",
        "                                                axis=1),\n",
        "                                        training=True)\n",
        "                pinn_nc = neu_tape.gradient(pinn_nc, self.ibc_cond['Neumann'].x)\n",
        "                neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc-self.ibc_cond['Neumann'].y)\n",
        "                    )\n",
        "            else:\n",
        "                neu_loss = 0\n",
        "\n",
        "            #--------------- Compute total loss\n",
        "            pinn_loss = (self.params['pde_weight'] * unsup_loss) +\\\n",
        "            self.params['sup_weight'][0] * init_loss +\\\n",
        "            self.params['sup_weight'][1] * dir_loss +\\\n",
        "            self.params['sup_weight'][2] * neu_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "        #Compute gradient and apply optimizers\n",
        "        gradient = mdl_tape.gradient(pinn_loss,self.mdl.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(gradient,self.mdl.trainable_variables))\n",
        "        #Return losses\n",
        "        final_fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "        final_pinn_loss = unsup_loss + init_loss +\\\n",
        "         dir_loss + neu_loss + fb_dir_loss + fb_neu_loss\n",
        "        return (unsup_loss, init_loss, dir_loss, neu_loss, fb_init_loss,\n",
        "                fb_dir_loss, fb_neu_loss, final_fb_loss, final_pinn_loss)\n",
        "\n",
        "    @tf.function\n",
        "    def train_mdl_solo(self):\n",
        "        with G_Tape(watch_accessed_variables=False) as mdl_tape:\n",
        "            #Watch solution weights\n",
        "            mdl_tape.watch(self.mdl.trainable_variables)\n",
        "            #--------------- Compute Free Boundary losses\n",
        "            #Compute Initial Free Boundary Condition\n",
        "            fb_init = self.fb(self.ibc_fb_cond['Initial'].x,\n",
        "                                  training=True)\n",
        "            if len(fb_init.shape) > 1:\n",
        "                fb_init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_init -\\\n",
        "                                  tf.expand_dims(self.ibc_fb_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                fb_init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_init -\\\n",
        "                                  self.ibc_fb_cond['Initial'].y )\n",
        "                    )\n",
        "            #Compute Dirichlet Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Dirichlet'] != None:\n",
        "                #Compute Free Boundary values\n",
        "                s_values = self.fb(self.ibc_fb_cond['Dirichlet'].x,\n",
        "                                    training=True)\n",
        "                fb_dc = self.mdl(tf.concat(\n",
        "                    [s_values,\n",
        "                      self.ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                      training=True)\n",
        "                fb_dir_target = tf.nn.relu(\n",
        "                    tf.ones_like(\n",
        "                        s_values\n",
        "                        ) * self.ibc_fb_cond['Dirichlet'].x[:,-1] - s_values\n",
        "                )\n",
        "                fb_dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_dc - fb_dir_target)\n",
        "                    )\n",
        "            else:\n",
        "                fb_dir_loss = 0\n",
        "            #Compute Neumann Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Neumann'] != None:\n",
        "                s_values = self.fb(self.ibc_fb_cond['Neumann'].x,\n",
        "                                    training=True)\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                    neu_fb_tape.watch(s_values)\n",
        "                    pinn_nc_fb = self.mdl(\n",
        "                        tf.concat([s_values,\n",
        "                                    self.ibc_fb_cond['Neumann'].x],\n",
        "                                  axis=1),\n",
        "                                  training=True)\n",
        "                pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                                  s_values)\n",
        "                fb_neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc_fb-self.ibc_fb_cond['Neumann'].y)\n",
        "                    )\n",
        "            else:\n",
        "                fb_neu_loss = 0\n",
        "            #Compute final loss\n",
        "            fb_loss = self.params['fb_weight'][0] * fb_init_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "\n",
        "            #--------------- Compute PINN losses\n",
        "            #Compute unsupervised loss\n",
        "            s_values = self.fb(tf.concat([self.t_f_total,\n",
        "                                          self.m_p_total], axis=-1),\n",
        "                               training=False)\n",
        "            x_f = tf.reshape(self.x_f_total[ self.x_f_total > s_values ],\n",
        "                             (-1,1) )\n",
        "            t_f = tf.reshape(self.t_f_total[ self.x_f_total > s_values ],\n",
        "                             (-1,1) )\n",
        "            m_p = self.m_p_total[ self.x_f_total[:, 0] > s_values[:, 0] ]\n",
        "            with G_Tape(persistent=True,\n",
        "                        watch_accessed_variables=False) as pinn_tape:\n",
        "                #Watch independet variables\n",
        "                pinn_tape.watch(x_f)\n",
        "                pinn_tape.watch(t_f)\n",
        "                #Apply u function for unsupervised\n",
        "                u_val = self.mdl(tf.concat([x_f, t_f, m_p],\n",
        "                                          axis=1),\n",
        "                                training=True)\n",
        "                u_x = pinn_tape.gradient(u_val, x_f)\n",
        "            u_xx = pinn_tape.gradient(u_x, x_f)\n",
        "            u_t = pinn_tape.gradient(u_val, t_f)\n",
        "            del(pinn_tape)\n",
        "            u_val = tf.cast(u_val, self.DTYPE)\n",
        "            f = (m_p[:,0:1] * x_f * u_x) + u_t +\\\n",
        "            (m_p[:,1:2] * x_f**2 * u_xx)/2 -\\\n",
        "            (m_p[:,0:1] * u_val)\n",
        "            unsup_loss = tf.reduce_mean(tf.square( f ))\n",
        "            #Compute Initial Condition\n",
        "            pinn_init = self.mdl(self.ibc_cond['Initial'].x,\n",
        "                                training=True)\n",
        "            if len(pinn_init.shape) > 1:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  tf.expand_dims(self.ibc_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  self.ibc_cond['Initial'].y )\n",
        "                    )\n",
        "            #Compute Dirichlet Boundary Condition\n",
        "            if self.ibc_cond['Dirichlet'] != None:\n",
        "                pinn_dc = self.mdl(self.ibc_cond['Dirichlet'].x,\n",
        "                                    training=True)\n",
        "                dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_dc-self.ibc_cond['Dirichlet'].y)\n",
        "                    )\n",
        "            else:\n",
        "                dir_loss = 0\n",
        "            #Compute Neumann Boundary Condition\n",
        "            if self.ibc_cond['Neumann'] != None:\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_tape:\n",
        "                    neu_tape.watch(self.ibc_cond['Neumann'].x)\n",
        "                    pinn_nc = self.mdl(tf.concat([self.ibc_cond['Neumann'].x,\n",
        "                                                  self.ibc_cond['Neumann'].t,\n",
        "                                                  self.ibc_cond['Neumann'].m_p],\n",
        "                                                axis=1),\n",
        "                                        training=True)\n",
        "                pinn_nc = neu_tape.gradient(pinn_nc, self.ibc_cond['Neumann'].x)\n",
        "                neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc-self.ibc_cond['Neumann'].y)\n",
        "                    )\n",
        "            else:\n",
        "                neu_loss = 0\n",
        "\n",
        "            #--------------- Compute total loss\n",
        "            pinn_loss = (self.params['pde_weight'] * unsup_loss) +\\\n",
        "            self.params['sup_weight'][0] * init_loss +\\\n",
        "            self.params['sup_weight'][1] * dir_loss +\\\n",
        "            self.params['sup_weight'][2] * neu_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "        #Compute gradient and apply optimizers\n",
        "        gradient = mdl_tape.gradient(pinn_loss,self.mdl.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(gradient,self.mdl.trainable_variables))\n",
        "        #Return losses\n",
        "        final_fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "        final_pinn_loss = unsup_loss + init_loss +\\\n",
        "         dir_loss + neu_loss + fb_dir_loss + fb_neu_loss\n",
        "        return (unsup_loss, init_loss, dir_loss, neu_loss, fb_init_loss,\n",
        "                fb_dir_loss, fb_neu_loss, final_fb_loss, final_pinn_loss)\n",
        "\n",
        "    @tf.function\n",
        "    def train_fb_solo(self):\n",
        "        #--------------- Compute Free Boundary losses\n",
        "        with G_Tape(watch_accessed_variables=False) as fb_tape:\n",
        "            #Watch free boundary weights\n",
        "            fb_tape.watch(self.fb.trainable_variables)\n",
        "            #Compute Initial Free Boundary Condition\n",
        "            fb_init = self.fb(self.ibc_fb_cond['Initial'].x,\n",
        "                                  training=True)\n",
        "            if len(fb_init.shape) > 1:\n",
        "                fb_init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_init -\\\n",
        "                                  tf.expand_dims(self.ibc_fb_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                fb_init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_init -\\\n",
        "                                  self.ibc_fb_cond['Initial'].y )\n",
        "                    )\n",
        "            #Compute Dirichlet Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Dirichlet'] != None:\n",
        "                #Compute Free Boundary values\n",
        "                s_values = self.fb(self.ibc_fb_cond['Dirichlet'].x,\n",
        "                                    training=True)\n",
        "                fb_dc = self.mdl(tf.concat(\n",
        "                    [s_values,\n",
        "                      self.ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                      training=True)\n",
        "                fb_dir_target = tf.nn.relu(\n",
        "                    tf.ones_like(\n",
        "                        s_values\n",
        "                        ) * self.ibc_fb_cond['Dirichlet'].x[:,-1] - s_values\n",
        "                )\n",
        "                fb_dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_dc - fb_dir_target)\n",
        "                    )\n",
        "            else:\n",
        "                fb_dir_loss = 0\n",
        "            #Compute Neumann Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Neumann'] != None:\n",
        "                s_values = self.fb(self.ibc_fb_cond['Neumann'].x,\n",
        "                                    training=True)\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                    neu_fb_tape.watch(s_values)\n",
        "                    pinn_nc_fb = self.mdl(\n",
        "                        tf.concat([s_values,\n",
        "                                    self.ibc_fb_cond['Neumann'].x],\n",
        "                                  axis=1),\n",
        "                                  training=True)\n",
        "                pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                                  s_values)\n",
        "                fb_neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc_fb-self.ibc_fb_cond['Neumann'].y)\n",
        "                    )\n",
        "            else:\n",
        "                fb_neu_loss = 0\n",
        "            #Compute final loss\n",
        "            fb_loss = self.params['fb_weight'][0] * fb_init_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "        #Compute gradient and apply optimizers for free boundary\n",
        "        gradient_fb = fb_tape.gradient(fb_loss,\n",
        "                                      self.fb.trainable_variables)\n",
        "        self.fb_opt.apply_gradients( zip(gradient_fb,\n",
        "                                      self.fb.trainable_variables) )\n",
        "        #Return losses\n",
        "        final_fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "        return fb_init_loss, fb_dir_loss, fb_neu_loss, final_fb_loss\n",
        "\n",
        "    def fit(self, wandb_run=None):\n",
        "        #Early warning initialization\n",
        "        self.early_warning = {'Target':np.inf,\n",
        "                              'n_steps':0,\n",
        "                              'top_mdl':None,\n",
        "                              'weights':None}\n",
        "        old_top_mdl, old_weights = None, None\n",
        "        #Training\n",
        "        self.u_losses, self.i_losses = list(), list()\n",
        "        self.d_losses, self.n_losses = list(), list()\n",
        "        self.b_i_losses = list()\n",
        "        self.b_d_losses, self.b_n_losses = list(), list()\n",
        "        self.b_losses, self.p_losses = list(), list()\n",
        "        print_base = \"{:<10}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}\"\n",
        "        if self.params['fb_freezing'] == None:\n",
        "            for epoch in tqdm(range(self.params['epochs']),\n",
        "                              desc='PINNs - Training'):\n",
        "                if epoch == 0:\n",
        "                    print(print_base.format('Epoch', 'Unsupervised', 'Initial',\n",
        "                                            'Dirichlet', 'Neumann',\n",
        "                                            'FB_Init', 'FB_Dir', 'FB_Neu',\n",
        "                                            'Free Boundary', 'Total'))\n",
        "                    print('\\n')\n",
        "                #Case 1: more mdl steps for a single fb step\n",
        "                for _ in range(self.params['steps_fb_per_pde'] -1):\n",
        "                    self.train_mdl_solo();\n",
        "                #Case 2: more fb steps for a single mdl step\n",
        "                for _ in range(0, self.params['steps_fb_per_pde'] +1, -1):\n",
        "                    self.train_fb_solo();\n",
        "                u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l =\\\n",
        "                self.train_step()\n",
        "                u_l, i_l = np.array(u_l), np.array(i_l)\n",
        "                d_l, n_l = np.array(d_l), np.array(n_l)\n",
        "                b_i_l = np.array(b_i_l)\n",
        "                b_d_l, b_n_l = np.array(b_d_l), np.array(b_n_l)\n",
        "                b_l, p_l = np.array(b_l), np.array(p_l)\n",
        "                if epoch % self.params['verbose'] == 0:\n",
        "                    print(print_base.format(epoch+1, format(u_l, '.20f')[:10],\n",
        "                                            format(i_l, '.20f')[:10],\n",
        "                                            format(d_l, '.20f')[:10],\n",
        "                                            format(n_l, '.20f')[:10],\n",
        "                                            format(b_i_l, '.20f')[:10],\n",
        "                                            format(b_d_l, '.20f')[:10],\n",
        "                                            format(b_n_l, '.20f')[:10],\n",
        "                                            format(b_l, '.20f')[:10],\n",
        "                                            format(p_l, '.20f')[:10]))\n",
        "                self.u_losses.append(u_l)\n",
        "                self.i_losses.append(i_l)\n",
        "                self.d_losses.append(d_l)\n",
        "                self.n_losses.append(n_l)\n",
        "                self.b_i_losses.append(b_i_l)\n",
        "                self.b_d_losses.append(b_d_l)\n",
        "                self.b_n_losses.append(b_d_l)\n",
        "                self.b_losses.append(b_l)\n",
        "                self.p_losses.append(p_l)\n",
        "                #Eventually, update wandb\n",
        "                if wandb_run != None:\n",
        "                    wandb_run.log({'Unsupervised':u_l,\n",
        "                                  'Supervised':i_l + d_l + n_l,\n",
        "                                  'Total':p_l,\n",
        "                                   'Free Boundary':b_l,\n",
        "                                   'Initial':i_l,\n",
        "                                   'Dirichlet':d_l,\n",
        "                                   'Neumann':n_l,\n",
        "                                   'FB_Init':b_i_l,\n",
        "                                   'FB_Dir':b_d_l,\n",
        "                                   'FB_Neu':b_n_l})\n",
        "                #Check for the early warning\n",
        "                if p_l <= self.early_warning['Target']:\n",
        "                    self.early_warning['Target'] = p_l\n",
        "                    self.early_warning['n_steps'] = 0\n",
        "                    self.early_warning['top_mdl'] = old_top_mdl\n",
        "                    self.early_warning['weights'] = old_weights\n",
        "                else:\n",
        "                    self.early_warning['n_steps'] += 1\n",
        "                    if self.early_warning['n_steps'] >= self.params['patience']:\n",
        "                        break\n",
        "                #Save model and weights for next step early warning\n",
        "                old_top_mdl = tf.keras.models.clone_model( self.mdl )\n",
        "                old_weights = self.mdl.get_weights()\n",
        "        else:\n",
        "            #Before freezing, both mdl and fb are training\n",
        "            for epoch in tqdm(range(self.params['fb_freezing']),\n",
        "                              desc='PINNs - Training'):\n",
        "                if epoch == 0:\n",
        "                    print(print_base.format('Epoch', 'Unsupervised', 'Initial',\n",
        "                                            'Dirichlet', 'Neumann',\n",
        "                                            'FB_Init', 'FB_Dir', 'FB_Neu',\n",
        "                                            'Free Boundary', 'Total'))\n",
        "                    print('\\n')\n",
        "                #Case 1: more mdl steps for a single fb step\n",
        "                for _ in range(self.params['steps_fb_per_pde'] -1):\n",
        "                    self.train_mdl_solo();\n",
        "                #Case 2: more fb steps for a single mdl step\n",
        "                for _ in range(0, self.params['steps_fb_per_pde'] +1, -1):\n",
        "                    self.train_fb_solo();\n",
        "                u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l =\\\n",
        "                self.train_step()\n",
        "                u_l, i_l = np.array(u_l), np.array(i_l)\n",
        "                d_l, n_l = np.array(d_l), np.array(n_l)\n",
        "                b_i_l = np.array(b_i_l)\n",
        "                b_d_l, b_n_l = np.array(b_d_l), np.array(b_n_l)\n",
        "                b_l, p_l = np.array(b_l), np.array(p_l)\n",
        "                if epoch % self.params['verbose'] == 0:\n",
        "                    print(print_base.format(epoch+1, format(u_l, '.20f')[:10],\n",
        "                                            format(i_l, '.20f')[:10],\n",
        "                                            format(d_l, '.20f')[:10],\n",
        "                                            format(n_l, '.20f')[:10],\n",
        "                                            format(b_i_l, '.20f')[:10],\n",
        "                                            format(b_d_l, '.20f')[:10],\n",
        "                                            format(b_n_l, '.20f')[:10],\n",
        "                                            format(b_l, '.20f')[:10],\n",
        "                                            format(p_l, '.20f')[:10]))\n",
        "                self.u_losses.append(u_l)\n",
        "                self.i_losses.append(i_l)\n",
        "                self.d_losses.append(d_l)\n",
        "                self.n_losses.append(n_l)\n",
        "                self.b_i_losses.append(b_i_l)\n",
        "                self.b_d_losses.append(b_d_l)\n",
        "                self.b_n_losses.append(b_d_l)\n",
        "                self.b_losses.append(b_l)\n",
        "                self.p_losses.append(p_l)\n",
        "                #Eventually, update wandb\n",
        "                if wandb_run != None:\n",
        "                    wandb_run.log({'Unsupervised':u_l,\n",
        "                                  'Supervised':i_l + d_l + n_l,\n",
        "                                  'Total':p_l,\n",
        "                                   'Free Boundary':b_l,\n",
        "                                   'Initial':i_l,\n",
        "                                   'Dirichlet':d_l,\n",
        "                                   'Neumann':n_l,\n",
        "                                   'FB_Init':b_i_l,\n",
        "                                   'FB_Dir':b_d_l,\n",
        "                                   'FB_Neu':b_n_l})\n",
        "                #Check for the early warning\n",
        "                if p_l <= self.early_warning['Target']:\n",
        "                    self.early_warning['Target'] = p_l\n",
        "                    self.early_warning['n_steps'] = 0\n",
        "                    self.early_warning['top_mdl'] = old_top_mdl\n",
        "                    self.early_warning['weights'] = old_weights\n",
        "                else:\n",
        "                    self.early_warning['n_steps'] += 1\n",
        "                    if self.early_warning['n_steps'] >= self.params['patience']:\n",
        "                        break\n",
        "                #Save model and weights for next step early warning\n",
        "                old_top_mdl = tf.keras.models.clone_model( self.mdl )\n",
        "                old_weights = self.mdl.get_weights()\n",
        "            #Now, freeze fb and train only mdl\n",
        "            for epoch in tqdm(range(self.params['fb_freezing'],\n",
        "                                    self.params['epochs']),\n",
        "                              desc='PINNs - Training; Free Boundary Fixed'):\n",
        "                u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l =\\\n",
        "                self.train_mdl_solo()\n",
        "                u_l, i_l = np.array(u_l), np.array(i_l)\n",
        "                d_l, n_l = np.array(d_l), np.array(n_l)\n",
        "                b_i_l = np.array(b_i_l)\n",
        "                b_d_l, b_n_l = np.array(b_d_l), np.array(b_n_l)\n",
        "                b_l, p_l = np.array(b_l), np.array(p_l)\n",
        "                if epoch % self.params['verbose'] == 0:\n",
        "                    print(print_base.format(epoch+1, format(u_l, '.20f')[:10],\n",
        "                                            format(i_l, '.20f')[:10],\n",
        "                                            format(d_l, '.20f')[:10],\n",
        "                                            format(n_l, '.20f')[:10],\n",
        "                                            format(b_i_l, '.20f')[:10],\n",
        "                                            format(b_d_l, '.20f')[:10],\n",
        "                                            format(b_n_l, '.20f')[:10],\n",
        "                                            format(b_l, '.20f')[:10],\n",
        "                                            format(p_l, '.20f')[:10]))\n",
        "                self.u_losses.append(u_l)\n",
        "                self.i_losses.append(i_l)\n",
        "                self.d_losses.append(d_l)\n",
        "                self.n_losses.append(n_l)\n",
        "                self.b_i_losses.append(b_i_l)\n",
        "                self.b_d_losses.append(b_d_l)\n",
        "                self.b_n_losses.append(b_d_l)\n",
        "                self.b_losses.append(b_l)\n",
        "                self.p_losses.append(p_l)\n",
        "                #Eventually, update wandb\n",
        "                if wandb_run != None:\n",
        "                    wandb_run.log({'Unsupervised':u_l,\n",
        "                                  'Supervised':i_l + d_l + n_l,\n",
        "                                  'Total':p_l,\n",
        "                                   'Free Boundary':b_l,\n",
        "                                   'Initial':i_l,\n",
        "                                   'Dirichlet':d_l,\n",
        "                                   'Neumann':n_l,\n",
        "                                   'FB_Init':b_i_l,\n",
        "                                   'FB_Dir':b_d_l,\n",
        "                                   'FB_Neu':b_n_l})\n",
        "                #Check for the early warning\n",
        "                if p_l <= self.early_warning['Target']:\n",
        "                    self.early_warning['Target'] = p_l\n",
        "                    self.early_warning['n_steps'] = 0\n",
        "                    self.early_warning['top_mdl'] = old_top_mdl\n",
        "                    self.early_warning['weights'] = old_weights\n",
        "                else:\n",
        "                    self.early_warning['n_steps'] += 1\n",
        "                    if self.early_warning['n_steps'] >= self.params['patience']:\n",
        "                        break\n",
        "                #Save model and weights for next step early warning\n",
        "                old_top_mdl = tf.keras.models.clone_model( self.mdl )\n",
        "                old_weights = self.mdl.get_weights()\n",
        "        #Recover information about optimal epoch in early warning\n",
        "        self.mdl = tf.keras.models.clone_model( self.early_warning['top_mdl'] )\n",
        "        self.mdl.set_weights(self.early_warning['weights'])\n",
        "        top_epoch = epoch+1 - self.early_warning[\"n_steps\"]\n",
        "        print(f'Best loss achieved at step {top_epoch}')\n",
        "\n",
        "    def plot_losses(self):\n",
        "        plt.figure( figsize=(12,8) )\n",
        "        plt.semilogy(self.u_losses, label='Unsupervised')\n",
        "        plt.semilogy(np.array(self.i_losses) +\\\n",
        "                     np.array(self.d_losses) +\\\n",
        "                     np.array(self.n_losses),\n",
        "                     label='Supervised')\n",
        "        plt.semilogy(self.b_losses, label='Free Boundary')\n",
        "        plt.semilogy(self.p_losses, label='PINN')\n",
        "        plt.legend()\n",
        "        plt.title('Losses')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_unsupervised_test(self, test_sampler, test_ibc_cond,\n",
        "                               test_ibc_fb_cond, to_print=True, output=False):\n",
        "        #--------------- Compute Free Boundary losses\n",
        "        #Compute Initial Free Boundary Condition\n",
        "        if test_ibc_fb_cond['Initial'] != None:\n",
        "            fb_init = self.fb(test_ibc_fb_cond['Initial'].x,\n",
        "                                  training=False)\n",
        "            if len(fb_init.shape) > 1:\n",
        "                fb_init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_init -\\\n",
        "                                  tf.expand_dims(test_ibc_fb_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                fb_init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_init -\\\n",
        "                                  test_ibc_fb_cond['Initial'].y )\n",
        "                    )\n",
        "        else:\n",
        "            fb_init_loss = 0\n",
        "        #Compute Dirichlet Free Boundary Condition\n",
        "        if test_ibc_fb_cond['Dirichlet'] != None:\n",
        "            #Compute Free Boundary values\n",
        "            s_values = self.fb(test_ibc_fb_cond['Dirichlet'].x,\n",
        "                                training=True)\n",
        "            fb_dc = self.mdl(tf.concat(\n",
        "                [s_values,\n",
        "                  test_ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                  training=True)\n",
        "            fb_dir_target = tf.nn.relu(\n",
        "                tf.ones_like(\n",
        "                    s_values\n",
        "                    ) * test_ibc_fb_cond['Dirichlet'].x[:,-1] - s_values\n",
        "            )\n",
        "            fb_dir_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(fb_dc - fb_dir_target)\n",
        "                )\n",
        "        else:\n",
        "            fb_dir_loss = 0\n",
        "        #Compute Neumann Free Boundary Condition\n",
        "        if test_ibc_fb_cond['Neumann'] != None:\n",
        "            s_values = self.fb(test_ibc_fb_cond['Neumann'].x,\n",
        "                                training=False)\n",
        "            with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                neu_fb_tape.watch(s_values)\n",
        "                pinn_nc_fb = self.mdl(\n",
        "                    tf.concat([s_values,\n",
        "                                test_ibc_fb_cond['Neumann'].x],\n",
        "                              axis=1),\n",
        "                              training=True)\n",
        "            pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                              s_values)\n",
        "            fb_neu_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(pinn_nc_fb-test_ibc_fb_cond['Neumann'].y)\n",
        "                )\n",
        "        else:\n",
        "            fb_neu_loss = 0\n",
        "        #Compute final loss\n",
        "        fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "\n",
        "        #--------------- Compute PINN losses\n",
        "        #Compute unsupervised loss\n",
        "        s_values = self.fb(tf.concat([test_sampler.t,\n",
        "                                      test_sampler.m_p], axis=-1),\n",
        "                            training=False)\n",
        "        x_f = tf.reshape(test_sampler.x[ test_sampler.x > s_values ],\n",
        "                          (-1,1) )\n",
        "        t_f = tf.reshape(test_sampler.t[ test_sampler.x > s_values ],\n",
        "                          (-1,1) )\n",
        "        m_p = test_sampler.m_p[ test_sampler.x[:, 0] > s_values[:, 0] ]\n",
        "        with G_Tape(persistent=True,\n",
        "                    watch_accessed_variables=False) as pinn_tape:\n",
        "            #Watch independet variables\n",
        "            pinn_tape.watch(x_f)\n",
        "            pinn_tape.watch(t_f)\n",
        "            #Apply u function for unsupervised\n",
        "            u_val = self.mdl(tf.concat([x_f, t_f, m_p],\n",
        "                                      axis=1),\n",
        "                            training=True)\n",
        "            u_x = pinn_tape.gradient(u_val, x_f)\n",
        "        u_xx = pinn_tape.gradient(u_x, x_f)\n",
        "        u_t = pinn_tape.gradient(u_val, t_f)\n",
        "        del(pinn_tape)\n",
        "        u_val = tf.cast(u_val, self.DTYPE)\n",
        "        f = (m_p[:,0:1] * x_f * u_x) + u_t +\\\n",
        "        (m_p[:,1:2] * x_f**2 * u_xx)/2 -\\\n",
        "        (m_p[:,0:1] * u_val)\n",
        "        unsup_loss = tf.reduce_mean(tf.square( f ))\n",
        "        #Compute Initial Condition\n",
        "        if test_ibc_cond['Initial'] != None:\n",
        "            pinn_init = self.mdl(test_ibc_cond['Initial'].x,\n",
        "                                  training=False)\n",
        "            if len(pinn_init.shape) > 1:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  tf.expand_dims(test_ibc_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  test_ibc_cond['Initial'].y )\n",
        "                    )\n",
        "        else:\n",
        "            init_loss = 0\n",
        "        #Compute Dirichlet Boundary Condition\n",
        "        if test_ibc_cond['Dirichlet'] != None:\n",
        "            pinn_dc = self.mdl(test_ibc_cond['Dirichlet'].x,\n",
        "                                training=False)\n",
        "            dir_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(pinn_dc-test_ibc_cond['Dirichlet'].y)\n",
        "                )\n",
        "        else:\n",
        "            dir_loss = 0\n",
        "        #Compute Neumann Boundary Condition\n",
        "        if test_ibc_cond['Neumann'] != None:\n",
        "            with G_Tape(watch_accessed_variables=False) as neu_tape:\n",
        "                neu_tape.watch(test_ibc_cond['Neumann'].x)\n",
        "                pinn_nc = self.mdl(tf.concat([test_ibc_cond['Neumann'].x,\n",
        "                                              test_ibc_cond['Neumann'].t,\n",
        "                                              test_ibc_cond['Neumann'].m_p],\n",
        "                                            axis=1),\n",
        "                                    training=False)\n",
        "            pinn_nc = neu_tape.gradient(pinn_nc, test_ibc_cond['Neumann'].x)\n",
        "            neu_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(pinn_nc-test_ibc_cond['Neumann'].y)\n",
        "                )\n",
        "        else:\n",
        "            neu_loss = 0\n",
        "\n",
        "        #--------------- Compute total loss\n",
        "        pinn_loss = unsup_loss + init_loss + dir_loss +\\\n",
        "        neu_loss + fb_dir_loss + fb_neu_loss\n",
        "\n",
        "        u_l, i_l = np.array(unsup_loss), np.array(init_loss)\n",
        "        d_l, n_l = np.array(dir_loss), np.array(neu_loss)\n",
        "        b_i_l = np.array(fb_init_loss)\n",
        "        b_d_l, b_n_l = np.array(fb_dir_loss), np.array(fb_neu_loss)\n",
        "        b_l, p_l = np.array(fb_loss), np.array(pinn_loss)\n",
        "\n",
        "        if to_print:\n",
        "            print_base = \"{:<10}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}\"\n",
        "            print(print_base.format('', 'Unsupervised', 'Initial',\n",
        "                                    'Dirichlet', 'Neumann', 'FB_Init', 'FB_Dir',\n",
        "                                    'FB_Neu', 'Free Boundary', 'Total'))\n",
        "            print(print_base.format('', format(u_l, '.20f')[:10],\n",
        "                                    format(i_l, '.20f')[:10],\n",
        "                                    format(d_l, '.20f')[:10],\n",
        "                                    format(n_l, '.20f')[:10],\n",
        "                                    format(b_i_l, '.20f')[:10],\n",
        "                                    format(b_d_l, '.20f')[:10],\n",
        "                                    format(b_n_l, '.20f')[:10],\n",
        "                                    format(b_l, '.20f')[:10],\n",
        "                                    format(p_l, '.20f')[:10]))\n",
        "        if output:\n",
        "            return u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vImUUiGjERuv"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sq1hK8wBERuw"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay as ex_d\n",
        "my_lr = ex_d(1e-2, 320, 0.975, staircase=False)\n",
        "fb_lr = ex_d(1e-2, 80, 0.975, staircase=False)\n",
        "\n",
        "params = {'sample_method':'sobol',\n",
        "          'layers':[n_dim+5, 20, 20, 20, 20, 20, 20, 20, 20, 1],\n",
        "          'activation':'tanh',\n",
        "          'output_act':'linear', 'initializer':'glorot_normal',\n",
        "          'fb_layers':[5, 100, 100, 100, n_dim], 'fb_activation':'tanh',\n",
        "          'fb_output_act':'linear', 'fb_initializer':'glorot_normal',\n",
        "          'lr':my_lr, 'optimizer':'rmsprop',\n",
        "          'fb_lr':fb_lr, 'fb_optimizer':'rmsprop', 'steps_fb_per_pde':4,\n",
        "          'pde_weight':1, 'epochs':15000, 'verbose':100}\n",
        "\n",
        "my_pinn = FreeBoundary_PINN(params, pinn_conditions,\n",
        "                            fb_conditions, lb, ub, N_f=80000, DTYPE=DTYPE)\n",
        "\n",
        "START = time.time()\n",
        "my_pinn.fit()\n",
        "train_time = time.time() - START\n",
        "my_pinn.plot_losses()\n",
        "\n",
        "START = time.time()\n",
        "values = my_pinn.plot_unsupervised_test(test_sampler, pinn_cond_test,\n",
        "                                        fb_cond_test, output=True)\n",
        "test_time = time.time() - START\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K3lJjOOYRIL"
      },
      "source": [
        "# 1D Parametric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZzzh7wPK1o1"
      },
      "source": [
        "### Define problem, initial and boundary conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIbVyKdUK1o2"
      },
      "outputs": [],
      "source": [
        "\n",
        "import time\n",
        "import wandb\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from math import pi, exp\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "#Define global variables\n",
        "n_dim = 1\n",
        "DTYPE = 'float32'\n",
        "\n",
        "#Fix seeds\n",
        "random_seed = 2\n",
        "np.random.seed(random_seed)\n",
        "tf.random.set_seed(random_seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpaWt-hqK1o3"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Sampler_IBC():\n",
        "    def __init__(self, lb, ub, cond=None, N_points=100,\n",
        "                 method='sobol', grid=None, this_initial_cond=False,\n",
        "                 split=False, params_split=False, DTYPE='float64'):\n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "        self.cond = cond\n",
        "        self.DTYPE = DTYPE\n",
        "        self.sample(N_points, method, grid, split,\n",
        "                    params_split, this_initial_cond)\n",
        "\n",
        "    def sample(self, N_points, method, grid,\n",
        "               split, params_split, this_initial_cond):\n",
        "        if method == 'uniform':\n",
        "            x_ibc = np.random.uniform(0, 1, size=(N_points, self.ub.shape[0]))\n",
        "            x_ibc = self.lb + (self.ub - self.lb)*x_ibc\n",
        "        elif method == 'latin':\n",
        "            from pyDOE import lhs\n",
        "            x_ibc = self.lb + (self.ub - self.lb)*lhs(self.ub.shape[0],N_points)\n",
        "        elif method == 'sobol':\n",
        "            import sobol\n",
        "            x_ibc = sobol.sample(dimension=self.ub.shape[0], n_points=N_points)\n",
        "            x_ibc = self.lb + (self.ub - self.lb)*x_ibc\n",
        "        elif method == 'equi':\n",
        "            x_ibc = np.linspace(self.lb, self.ub, N_points)\n",
        "        elif method == 'grid':\n",
        "            x_ibc = np.linspace(self.lb, self.ub, N_points).T\n",
        "            temp_final = list()\n",
        "            for val in x_ibc[0]:\n",
        "                temp_final.append( [val] )\n",
        "            dim = 1\n",
        "            while dim < x_ibc.shape[0]:\n",
        "                temp = list()\n",
        "                for t1 in range(x_ibc.shape[1]):\n",
        "                    for t2 in range(len(temp_final)):\n",
        "                        temp_val = temp_final[t2].copy()\n",
        "                        temp_val.append( x_ibc[dim, t1] )\n",
        "                        temp.append( temp_val )\n",
        "                temp_final = temp\n",
        "                dim += 1\n",
        "            x_ibc = np.array(temp_final)\n",
        "        elif method == 'grid_old':\n",
        "            idx = np.random.choice(range(grid.shape[0]),N_points,replace=False)\n",
        "            x_ibc = grid[idx]\n",
        "        #Check and, eventually, invert t and T\n",
        "        x_ibc[:,-5] = x_ibc[:,-5] * x_ibc[:,-2]\n",
        "        if this_initial_cond:\n",
        "            x_ibc[:,-5] = x_ibc[:,-2]\n",
        "        #Derfine y\n",
        "        if self.cond != None:\n",
        "            y_ibc = self.cond(x_ibc)\n",
        "            self.y = tf.cast(tf.Variable(y_ibc, trainable=False ),\n",
        "                             self.DTYPE)\n",
        "        if params_split:\n",
        "            x_ibc, m_p = x_ibc[:, :-4], x_ibc[:, -4:]\n",
        "            self.m_p = tf.cast(tf.Variable(m_p, trainable=False ),\n",
        "                             self.DTYPE)\n",
        "        if split:\n",
        "            x_ibc, t_ibc = x_ibc[:, :-1], x_ibc[:, -1:]\n",
        "            self.t = tf.cast(tf.Variable(t_ibc, trainable=False ),\n",
        "                             self.DTYPE)\n",
        "        self.x = tf.cast(tf.Variable(x_ibc, trainable=False ),\n",
        "                         self.DTYPE)\n",
        "\n",
        "#Define domain boundaries\n",
        "lb = np.array([0., 0., 0.001, 0.001, 2, 8])\n",
        "ub = np.array([30, 1., 0.03, 0.005, 5, 12])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4lRTNfSK1o4"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Problem Definition\n",
        "def h_1(inp):\n",
        "    res = list()\n",
        "    for inp_val in inp:\n",
        "        res.append( K - inp_val[0] )\n",
        "    return np.array(res)\n",
        "\n",
        "def h_2(inp):\n",
        "    res = - np.ones( inp.shape[0] )\n",
        "    return res\n",
        "\n",
        "def g(inp):\n",
        "    res = np.zeros( inp.shape[0] )\n",
        "    return np.array(res)\n",
        "\n",
        "def u_0(inp):\n",
        "    res = list()\n",
        "    for inp_val in inp:\n",
        "        res.append( np.max([0, inp_val[-1]-inp_val[0]]) )\n",
        "    return np.array(res)\n",
        "\n",
        "def s_0(inp):\n",
        "    res = list()\n",
        "    for inp_val in inp:\n",
        "        res.append( inp_val[-1] )\n",
        "    return np.array(res)\n",
        "\n",
        "#Point sampling\n",
        "N_to_sample = 10000\n",
        "N_to_sample_reduced = 5000\n",
        "\n",
        "#---------------- PDE Conditions\n",
        "print('PDE COnditions\\n')\n",
        "#Initial\n",
        "init_sampler = Sampler_IBC(np.array([0., -1, 0.001, 0.001, 2, 8]),\n",
        "                           np.array([30, -1, 0.03, 0.005, 5, 12]),\n",
        "                           u_0, N_to_sample,\n",
        "                           DTYPE=DTYPE, this_initial_cond=True )\n",
        "print( f'x: {init_sampler.x.shape}     y: {init_sampler.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "dir_sampler = Sampler_IBC(np.array([30, 0., 0.001, 0.001, 2, 8]),\n",
        "                          np.array([30, 1., 0.03, 0.005, 5, 12]),\n",
        "                          g, N_to_sample, DTYPE=DTYPE )\n",
        "print( f'x: {dir_sampler.x.shape}     y: {dir_sampler.y.shape}' )\n",
        "\n",
        "#Neumann\n",
        "print('No')\n",
        "\n",
        "#---------------- Free Boundary Conditions\n",
        "print('\\nFree Boundary COnditions\\n')\n",
        "#Initial\n",
        "init_fb_sampler = Sampler_IBC(np.array([-1, 0.001, 0.001, 2, 8]),\n",
        "                              np.array([-1, 0.03, 0.005, 5, 12]),\n",
        "                              s_0, N_to_sample_reduced,\n",
        "                              DTYPE=DTYPE, this_initial_cond=True )\n",
        "print( f't: {init_fb_sampler.x.shape}     y: {init_fb_sampler.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "dir_fb_sampler = Sampler_IBC(np.array([0., 0.001, 0.001, 2, 8]),\n",
        "                             np.array([1., 0.03, 0.005, 5, 12]),\n",
        "                             None, N_to_sample, DTYPE=DTYPE )\n",
        "print( f't: {dir_fb_sampler.x.shape}' )\n",
        "\n",
        "#Neumann\n",
        "neu_fb_sampler = Sampler_IBC(np.array([0., 0.001, 0.001, 2, 8]),\n",
        "                             np.array([1., 0.03, 0.005, 5, 12]),\n",
        "                             h_2, N_to_sample, DTYPE=DTYPE )\n",
        "print( f't: {neu_fb_sampler.x.shape}     y: {neu_fb_sampler.y.shape}' )\n",
        "\n",
        "#---------------- Conditions passed to PINN\n",
        "pinn_conditions = {'Initial':init_sampler,\n",
        "                   'Dirichlet':dir_sampler,\n",
        "                   'Neumann':None}\n",
        "fb_conditions = {'Initial':init_fb_sampler,\n",
        "                 'Dirichlet':dir_fb_sampler,\n",
        "                 'Neumann':neu_fb_sampler}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nxhs7ZPbK1o7"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_sampler = Sampler_IBC(lb, ub, cond=None, DTYPE=DTYPE,\n",
        "                           N_points=10000, method='uniform', split=True, params_split=True)\n",
        "\n",
        "\n",
        "#Point sampling\n",
        "sample_to_test = 1000\n",
        "\n",
        "#---------------- PDE Conditions\n",
        "print('PDE COnditions\\n')\n",
        "#Initial\n",
        "test_init_sampler = Sampler_IBC(np.array([0., 0, 0.001, 0.001, 2, 8]),\n",
        "                           np.array([30, 0, 0.03, 0.005, 5, 12]),\n",
        "                           u_0, sample_to_test, method='uniform',\n",
        "                           DTYPE=DTYPE, this_initial_cond=True )\n",
        "print( f'x: {test_init_sampler.x.shape}     y: {test_init_sampler.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "test_dir_sampler = Sampler_IBC(np.array([30, 0., 0.001, 0.001, 2, 8]),\n",
        "                          np.array([30, 1., 0.03, 0.005, 5, 12]),\n",
        "                          g, sample_to_test, DTYPE=DTYPE, method='uniform' )\n",
        "print( f'x: {test_dir_sampler.x.shape}     y: {test_dir_sampler.y.shape}' )\n",
        "\n",
        "#Neumann\n",
        "print('No')\n",
        "\n",
        "#---------------- Free Boundary Conditions\n",
        "print('\\nFree Boundary COnditions\\n')\n",
        "#Initial\n",
        "test_init_fb_sampler = Sampler_IBC(np.array([0, 0.001, 0.001, 2, 8]),\n",
        "                              np.array([0, 0.03, 0.005, 5, 12]),\n",
        "                              s_0, sample_to_test, method='uniform',\n",
        "                              DTYPE=DTYPE, this_initial_cond=True )\n",
        "print( f't: {test_init_fb_sampler.x.shape}     y: {test_init_fb_sampler.y.shape}' )\n",
        "\n",
        "#Dirichlet\n",
        "test_dir_fb_sampler = Sampler_IBC(np.array([0., 0.001, 0.001, 2, 8]),\n",
        "                             np.array([1., 0.03, 0.005, 5, 12]),\n",
        "                             None, sample_to_test, method='uniform', DTYPE=DTYPE )\n",
        "print( f't: {test_dir_fb_sampler.x.shape}' )\n",
        "\n",
        "#Neumann\n",
        "test_neu_fb_sampler = Sampler_IBC(np.array([0., 0.001, 0.001, 2, 8]),\n",
        "                             np.array([1., 0.03, 0.005, 5, 12]),\n",
        "                             h_2, sample_to_test, DTYPE=DTYPE, method='uniform' )\n",
        "print( f't: {test_neu_fb_sampler.x.shape}     y: {test_neu_fb_sampler.y.shape}' )\n",
        "\n",
        "#---------------- Conditions passed to PINN\n",
        "pinn_cond_test = {'Initial':test_init_sampler,\n",
        "                  'Dirichlet':test_dir_sampler,\n",
        "                  'Neumann':None}\n",
        "fb_cond_test = {'Initial':test_init_fb_sampler,\n",
        "                'Dirichlet':test_dir_fb_sampler,\n",
        "                'Neumann':test_neu_fb_sampler}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIMoj2LrK1o9"
      },
      "source": [
        "### PINN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZYi8FqdK1o9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow import GradientTape as G_Tape\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.initializers import GlorotNormal, GlorotUniform\n",
        "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
        "from tensorflow.keras.metrics import mean_squared_error\n",
        "\n",
        "class FreeBoundary_PINN():\n",
        "    def __init__(self, params, ibc_cond, ibc_fb_cond, lb, ub,\n",
        "                 N_f=10000, N_fb_ibc=150, DTYPE='float64', coll_points=None):\n",
        "        self.params = params\n",
        "        self.DTYPE = DTYPE\n",
        "        self.Default_Params()\n",
        "        #Set seed\n",
        "        if self.params['seed'] != None:\n",
        "            tf.keras.utils.set_random_seed( self.params['seed'] )\n",
        "            tf.config.experimental.enable_op_determinism()\n",
        "        #Define intial and boundary conditions\n",
        "        self.ibc_cond = ibc_cond\n",
        "        self.ibc_fb_cond = ibc_fb_cond\n",
        "        #All needed for points sampling\n",
        "        self.lb = tf.Variable(lb, trainable=False)\n",
        "        self.ub = tf.Variable(ub, trainable=False)\n",
        "        if coll_points == None:\n",
        "            self.N_f = N_f\n",
        "            self.Sample_Points()\n",
        "        else:\n",
        "            self.x_f = coll_points[0]\n",
        "            self.t_f = coll_points[1]\n",
        "            self.m_p = coll_points[2:]\n",
        "        self.N_fb_ibc = N_fb_ibc\n",
        "        #Initialize the class: define the network\n",
        "        self.Define_Regularizer()\n",
        "        self.Define_Initializer()\n",
        "        self.Define_Optimizer()\n",
        "        self.Create_Network()\n",
        "        self.Create_FB_Network()\n",
        "\n",
        "    def Default_Params(self):\n",
        "        target = self.params.keys()\n",
        "        if 'seed' not in target:\n",
        "            self.params['seed'] = None\n",
        "        if 'optimizer' not in target:\n",
        "            self.params['optimizer'] = 'Adam'\n",
        "        if 'fb_optimizer' not in target:\n",
        "            self.params['fb_optimizer'] = 'Adam'\n",
        "        if 'reg_type' not in target:\n",
        "            self.params['reg_type'] = None\n",
        "        if 'initializer' not in target:\n",
        "            self.params['initializer'] = 'glorot_normal'\n",
        "        if 'activation' not in target:\n",
        "            self.params['activation'] = 'tanh'\n",
        "        if 'output_act' not in target:\n",
        "            self.params['output_act'] = 'linear'\n",
        "        if 'pde_weight' not in target:\n",
        "            self.params['pde_weight'] = 1.\n",
        "        if 'sup_weight' not in target:\n",
        "            self.params['sup_weight'] = [1., 1., 1.]\n",
        "        if 'fb_weight' not in target:\n",
        "            self.params['fb_weight'] = [1., 1., 1.]\n",
        "        if 'patience' not in target:\n",
        "            self.params['patience'] = np.inf\n",
        "        if 'sample_method' not in target:\n",
        "            self.params['sample_method'] = 'uniform'\n",
        "        if 'fb_output_act' not in target:\n",
        "            self.params['fb_output_act'] = 'linear'\n",
        "        if 'fb_activation' not in target:\n",
        "            self.params['fb_activation'] = 'tanh'\n",
        "        if 'verbose' not in target:\n",
        "            self.params['verbose'] = 1\n",
        "        if 'steps_fb_per_pde' not in target:\n",
        "            self.params['steps_fb_per_pde'] = 1\n",
        "        if 'fb_freezing' not in target:\n",
        "            self.params['fb_freezing'] = None\n",
        "\n",
        "    def Define_Regularizer(self):\n",
        "        if self.params['reg_type'] == 'l1':\n",
        "            self.regularizer = l1( self.params['reg'] )\n",
        "        elif self.params['reg_type'] == 'l2':\n",
        "            self.regularizer = l2( self.params['reg'] )\n",
        "        elif self.params['reg_type'] == 'l1_l2':\n",
        "            self.regularizer = l1_l2( self.params['reg'][0],\n",
        "                                      self.params['reg'][1] )\n",
        "        else:\n",
        "            self.regularizer = None\n",
        "\n",
        "    def Define_Initializer(self):\n",
        "        if self.params['initializer'] == 'glorot_normal':\n",
        "            self.initializer = GlorotNormal()\n",
        "        elif self.params['initializer'] == 'glorot_uniform':\n",
        "            self.initializer = GlorotUniform()\n",
        "        else:\n",
        "            self.initializer = None\n",
        "\n",
        "    def Define_Optimizer(self):\n",
        "        temp = self.params['optimizer']\n",
        "        if temp.lower() == 'adam':\n",
        "            self.opt = Adam( self.params['lr'] )\n",
        "        elif temp.lower() == 'rmsprop':\n",
        "            self.opt = RMSprop( self.params['lr'] )\n",
        "        else:\n",
        "            raise ValueError(f\"Optimizer {temp} not recognized\")\n",
        "\n",
        "        temp = self.params['fb_optimizer']\n",
        "        if temp.lower() == 'adam':\n",
        "            self.fb_opt = Adam( self.params['fb_lr'] )\n",
        "        elif temp.lower() == 'rmsprop':\n",
        "            self.fb_opt = RMSprop( self.params['fb_lr'] )\n",
        "        else:\n",
        "            raise ValueError(f\"fb_Optimizer {temp} not recognized\")\n",
        "\n",
        "    def Create_Network(self):\n",
        "        input_layer = Input(shape=self.params['layers'][0],\n",
        "                            name = 'Input')\n",
        "        x = Dense(units=self.params['layers'][1],\n",
        "                  activation=self.params['activation'],\n",
        "                  kernel_initializer=self.initializer,\n",
        "                  kernel_regularizer=self.regularizer,\n",
        "                  name='Dense_1')(input_layer)\n",
        "        for layer in range(2, len(self.params['layers'])-1):\n",
        "            x = Dense(units=self.params['layers'][layer],\n",
        "                      activation=self.params['activation'],\n",
        "                      kernel_initializer=self.initializer,\n",
        "                      kernel_regularizer=self.regularizer,\n",
        "                      name=f'Dense_{layer}')(x)\n",
        "        output = Dense(units=self.params['layers'][-1],\n",
        "                       activation=self.params['output_act'],\n",
        "                       kernel_initializer=self.initializer,\n",
        "                       kernel_regularizer=self.regularizer,\n",
        "                       name='Output')(x)\n",
        "        self.mdl = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "    def Create_FB_Network(self):\n",
        "        input_layer = Input(shape=self.params['fb_layers'][0],\n",
        "                            name = 'Input')\n",
        "        x = Dense(units=self.params['fb_layers'][1],\n",
        "                  activation=self.params['fb_activation'],\n",
        "                  kernel_initializer=self.initializer,\n",
        "                  kernel_regularizer=self.regularizer,\n",
        "                  name='Dense_1')(input_layer)\n",
        "        for layer in range(2, len(self.params['fb_layers'])-1):\n",
        "            x = Dense(units=self.params['fb_layers'][layer],\n",
        "                      activation=self.params['fb_activation'],\n",
        "                      kernel_initializer=self.initializer,\n",
        "                      kernel_regularizer=self.regularizer,\n",
        "                      name=f'Dense_{layer}')(x)\n",
        "        output = Dense(units=self.params['fb_layers'][-1],\n",
        "                       activation=self.params['fb_output_act'],\n",
        "                       kernel_initializer=self.initializer,\n",
        "                       kernel_regularizer=self.regularizer,\n",
        "                       name='Output')(x)\n",
        "        self.fb = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "    def Sample_Points(self):\n",
        "        #According to the selected method, sample collocation points\n",
        "        method = self.params['sample_method']\n",
        "        if method == 'latin':\n",
        "            from pyDOE import lhs\n",
        "            cps = self.lb + (self.ub - self.lb)*lhs(self.ub.shape[0], self.N_f)\n",
        "        elif method == 'uniform':\n",
        "            cps = np.random.uniform(0, 1, size=(self.N_f, self.ub.shape[0]))\n",
        "            cps = self.lb + (self.ub - self.lb)*cps\n",
        "        elif method == 'sobol':\n",
        "            import sobol\n",
        "            cps = sobol.sample(dimension=self.ub.shape[0], n_points=self.N_f)\n",
        "            cps = self.lb + (self.ub - self.lb)*cps\n",
        "        else:\n",
        "            raise ValueError(f'Sampling method {method} not recognized')\n",
        "        #Check and, eventually, invert t and T\n",
        "        cps = np.array(cps)\n",
        "        cps[:,-5] = cps[:,-5] * cps[:,-2]\n",
        "        #Return collocation points as tf tensors\n",
        "        self.x_f_total = tf.cast(tf.Variable(cps[:, :-5], trainable=False),\n",
        "                           self.DTYPE)\n",
        "        self.t_f_total = tf.cast(tf.Variable(cps[:, -5:-4], trainable=False),\n",
        "                           self.DTYPE)\n",
        "        self.m_p_total = tf.cast(tf.Variable(cps[:, -4:], trainable=False),\n",
        "                           self.DTYPE)\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self):\n",
        "        with G_Tape(watch_accessed_variables=False) as mdl_tape:\n",
        "            #Watch solution weights\n",
        "            mdl_tape.watch(self.mdl.trainable_variables)\n",
        "            #--------------- Compute Free Boundary losses\n",
        "            with G_Tape(watch_accessed_variables=False) as fb_tape:\n",
        "                #Watch free boundary weights\n",
        "                fb_tape.watch(self.fb.trainable_variables)\n",
        "                #Compute Initial Free Boundary Condition\n",
        "                fb_init = self.fb(self.ibc_fb_cond['Initial'].x,\n",
        "                                      training=True)\n",
        "                if len(fb_init.shape) > 1:\n",
        "                    fb_init_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(fb_init -\\\n",
        "                                      tf.expand_dims(self.ibc_fb_cond['Initial'].y,\n",
        "                                                     axis=-1 ) )\n",
        "                        )\n",
        "                else:\n",
        "                    fb_init_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(fb_init -\\\n",
        "                                      self.ibc_fb_cond['Initial'].y )\n",
        "                        )\n",
        "                #Compute Dirichlet Free Boundary Condition\n",
        "                if self.ibc_fb_cond['Dirichlet'] != None:\n",
        "                    #Compute Free Boundary values\n",
        "                    s_values = self.fb(self.ibc_fb_cond['Dirichlet'].x,\n",
        "                                       training=True)\n",
        "                    fb_dc = self.mdl(tf.concat(\n",
        "                        [s_values,\n",
        "                         self.ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                         training=True)\n",
        "                    fb_dir_target = tf.nn.relu(\n",
        "                        tf.ones_like(\n",
        "                            s_values\n",
        "                            ) * self.ibc_fb_cond['Dirichlet'].x[:,-1] - s_values\n",
        "                    )\n",
        "                    fb_dir_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(fb_dc - fb_dir_target)\n",
        "                        )\n",
        "                else:\n",
        "                    fb_dir_loss = 0\n",
        "                #Compute Neumann Free Boundary Condition\n",
        "                if self.ibc_fb_cond['Neumann'] != None:\n",
        "                    s_values = self.fb(self.ibc_fb_cond['Neumann'].x,\n",
        "                                       training=True)\n",
        "                    with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                        neu_fb_tape.watch(s_values)\n",
        "                        pinn_nc_fb = self.mdl(\n",
        "                            tf.concat([s_values,\n",
        "                                       self.ibc_fb_cond['Neumann'].x],\n",
        "                                      axis=1),\n",
        "                                      training=True)\n",
        "                    pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                                      s_values)\n",
        "                    fb_neu_loss = tf.math.reduce_mean(\n",
        "                        tf.math.square(pinn_nc_fb-self.ibc_fb_cond['Neumann'].y)\n",
        "                        )\n",
        "                else:\n",
        "                    fb_neu_loss = 0\n",
        "                #Compute final loss\n",
        "                fb_loss = self.params['fb_weight'][0] * fb_init_loss +\\\n",
        "                self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "                self.params['fb_weight'][2] * fb_neu_loss\n",
        "            #Compute gradient and apply optimizers for free boundary\n",
        "            gradient_fb = fb_tape.gradient(fb_loss,\n",
        "                                          self.fb.trainable_variables)\n",
        "            self.fb_opt.apply_gradients( zip(gradient_fb,\n",
        "                                          self.fb.trainable_variables) )\n",
        "\n",
        "            #--------------- Compute PINN losses\n",
        "            #Compute unsupervised loss\n",
        "            s_values = self.fb(tf.concat([self.t_f_total,\n",
        "                                          self.m_p_total], axis=-1),\n",
        "                               training=False)\n",
        "            x_f = tf.reshape(self.x_f_total[ self.x_f_total > s_values ],\n",
        "                             (-1,1) )\n",
        "            t_f = tf.reshape(self.t_f_total[ self.x_f_total > s_values ],\n",
        "                             (-1,1) )\n",
        "            m_p = self.m_p_total[ self.x_f_total[:, 0] > s_values[:, 0] ]\n",
        "            with G_Tape(persistent=True,\n",
        "                        watch_accessed_variables=False) as pinn_tape:\n",
        "                #Watch independet variables\n",
        "                pinn_tape.watch(x_f)\n",
        "                pinn_tape.watch(t_f)\n",
        "                #Apply u function for unsupervised\n",
        "                u_val = self.mdl(tf.concat([x_f, t_f, m_p],\n",
        "                                          axis=1),\n",
        "                                training=True)\n",
        "                u_x = pinn_tape.gradient(u_val, x_f)\n",
        "            u_xx = pinn_tape.gradient(u_x, x_f)\n",
        "            u_t = pinn_tape.gradient(u_val, t_f)\n",
        "            del(pinn_tape)\n",
        "            u_val = tf.cast(u_val, self.DTYPE)\n",
        "            f = (m_p[:,0:1] * x_f * u_x) + u_t +\\\n",
        "            (m_p[:,1:2] * x_f**2 * u_xx)/2 -\\\n",
        "            (m_p[:,0:1] * u_val)\n",
        "            unsup_loss = tf.reduce_mean(tf.square( f ))\n",
        "            #Compute Initial Condition\n",
        "            pinn_init = self.mdl(self.ibc_cond['Initial'].x, training=True)\n",
        "            if len(pinn_init.shape) > 1:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  tf.expand_dims(self.ibc_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  self.ibc_cond['Initial'].y )\n",
        "                    )\n",
        "            #Compute Dirichlet Boundary Condition\n",
        "            if self.ibc_cond['Dirichlet'] != None:\n",
        "                pinn_dc = self.mdl(self.ibc_cond['Dirichlet'].x, training=True)\n",
        "                dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_dc-self.ibc_cond['Dirichlet'].y)\n",
        "                    )\n",
        "            else:\n",
        "                dir_loss = 0\n",
        "            #Compute Neumann Boundary Condition\n",
        "            if self.ibc_cond['Neumann'] != None:\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_tape:\n",
        "                    neu_tape.watch(self.ibc_cond['Neumann'].x)\n",
        "                    pinn_nc = self.mdl(tf.concat([self.ibc_cond['Neumann'].x,\n",
        "                                                  self.ibc_cond['Neumann'].t,\n",
        "                                                  self.ibc_cond['Neumann'].m_p],\n",
        "                                                axis=1),\n",
        "                                        training=True)\n",
        "                pinn_nc = neu_tape.gradient(pinn_nc, self.ibc_cond['Neumann'].x)\n",
        "                neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc-self.ibc_cond['Neumann'].y)\n",
        "                    )\n",
        "            else:\n",
        "                neu_loss = 0\n",
        "\n",
        "            #--------------- Compute total loss\n",
        "            pinn_loss = (self.params['pde_weight'] * unsup_loss) +\\\n",
        "            self.params['sup_weight'][0] * init_loss +\\\n",
        "            self.params['sup_weight'][1] * dir_loss +\\\n",
        "            self.params['sup_weight'][2] * neu_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "        #Compute gradient and apply optimizers\n",
        "        gradient = mdl_tape.gradient(pinn_loss,self.mdl.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(gradient,self.mdl.trainable_variables))\n",
        "        #Return losses\n",
        "        final_fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "        final_pinn_loss = unsup_loss + init_loss +\\\n",
        "         dir_loss + neu_loss + fb_dir_loss + fb_neu_loss\n",
        "        return (unsup_loss, init_loss, dir_loss, neu_loss, fb_init_loss,\n",
        "                fb_dir_loss, fb_neu_loss, final_fb_loss, final_pinn_loss)\n",
        "\n",
        "    @tf.function\n",
        "    def train_mdl_solo(self):\n",
        "        with G_Tape(watch_accessed_variables=False) as mdl_tape:\n",
        "            #Watch solution weights\n",
        "            mdl_tape.watch(self.mdl.trainable_variables)\n",
        "            #--------------- Compute Free Boundary losses\n",
        "            #Compute Initial Free Boundary Condition\n",
        "            fb_init = self.fb(self.ibc_fb_cond['Initial'].x,\n",
        "                                  training=True)\n",
        "            if len(fb_init.shape) > 1:\n",
        "                fb_init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_init -\\\n",
        "                                  tf.expand_dims(self.ibc_fb_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                fb_init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_init -\\\n",
        "                                  self.ibc_fb_cond['Initial'].y )\n",
        "                    )\n",
        "            #Compute Dirichlet Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Dirichlet'] != None:\n",
        "                #Compute Free Boundary values\n",
        "                s_values = self.fb(self.ibc_fb_cond['Dirichlet'].x,\n",
        "                                    training=True)\n",
        "                fb_dc = self.mdl(tf.concat(\n",
        "                    [s_values,\n",
        "                      self.ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                      training=True)\n",
        "                fb_dir_target = tf.nn.relu(\n",
        "                    tf.ones_like(\n",
        "                        s_values\n",
        "                        ) * self.ibc_fb_cond['Dirichlet'].x[:,-1] - s_values\n",
        "                )\n",
        "                fb_dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_dc - fb_dir_target)\n",
        "                    )\n",
        "            else:\n",
        "                fb_dir_loss = 0\n",
        "            #Compute Neumann Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Neumann'] != None:\n",
        "                s_values = self.fb(self.ibc_fb_cond['Neumann'].x,\n",
        "                                    training=True)\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                    neu_fb_tape.watch(s_values)\n",
        "                    pinn_nc_fb = self.mdl(\n",
        "                        tf.concat([s_values,\n",
        "                                    self.ibc_fb_cond['Neumann'].x],\n",
        "                                  axis=1),\n",
        "                                  training=True)\n",
        "                pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                                  s_values)\n",
        "                fb_neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc_fb-self.ibc_fb_cond['Neumann'].y)\n",
        "                    )\n",
        "            else:\n",
        "                fb_neu_loss = 0\n",
        "            #Compute final loss\n",
        "            fb_loss = self.params['fb_weight'][0] * fb_init_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "\n",
        "            #--------------- Compute PINN losses\n",
        "            #Compute unsupervised loss\n",
        "            s_values = self.fb(tf.concat([self.t_f_total,\n",
        "                                          self.m_p_total], axis=-1),\n",
        "                               training=False)\n",
        "            x_f = tf.reshape(self.x_f_total[ self.x_f_total > s_values ],\n",
        "                             (-1,1) )\n",
        "            t_f = tf.reshape(self.t_f_total[ self.x_f_total > s_values ],\n",
        "                             (-1,1) )\n",
        "            m_p = self.m_p_total[ self.x_f_total[:, 0] > s_values[:, 0] ]\n",
        "            with G_Tape(persistent=True,\n",
        "                        watch_accessed_variables=False) as pinn_tape:\n",
        "                #Watch independet variables\n",
        "                pinn_tape.watch(x_f)\n",
        "                pinn_tape.watch(t_f)\n",
        "                #Apply u function for unsupervised\n",
        "                u_val = self.mdl(tf.concat([x_f, t_f, m_p],\n",
        "                                          axis=1),\n",
        "                                training=True)\n",
        "                u_x = pinn_tape.gradient(u_val, x_f)\n",
        "            u_xx = pinn_tape.gradient(u_x, x_f)\n",
        "            u_t = pinn_tape.gradient(u_val, t_f)\n",
        "            del(pinn_tape)\n",
        "            u_val = tf.cast(u_val, self.DTYPE)\n",
        "            f = (m_p[:,0:1] * x_f * u_x) + u_t +\\\n",
        "            (m_p[:,1:2] * x_f**2 * u_xx)/2 -\\\n",
        "            (m_p[:,0:1] * u_val)\n",
        "            unsup_loss = tf.reduce_mean(tf.square( f ))\n",
        "            #Compute Initial Condition\n",
        "            pinn_init = self.mdl(self.ibc_cond['Initial'].x,\n",
        "                                training=True)\n",
        "            if len(pinn_init.shape) > 1:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  tf.expand_dims(self.ibc_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  self.ibc_cond['Initial'].y )\n",
        "                    )\n",
        "            #Compute Dirichlet Boundary Condition\n",
        "            if self.ibc_cond['Dirichlet'] != None:\n",
        "                pinn_dc = self.mdl(self.ibc_cond['Dirichlet'].x,\n",
        "                                    training=True)\n",
        "                dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_dc-self.ibc_cond['Dirichlet'].y)\n",
        "                    )\n",
        "            else:\n",
        "                dir_loss = 0\n",
        "            #Compute Neumann Boundary Condition\n",
        "            if self.ibc_cond['Neumann'] != None:\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_tape:\n",
        "                    neu_tape.watch(self.ibc_cond['Neumann'].x)\n",
        "                    pinn_nc = self.mdl(tf.concat([self.ibc_cond['Neumann'].x,\n",
        "                                                  self.ibc_cond['Neumann'].t,\n",
        "                                                  self.ibc_cond['Neumann'].m_p],\n",
        "                                                axis=1),\n",
        "                                        training=True)\n",
        "                pinn_nc = neu_tape.gradient(pinn_nc, self.ibc_cond['Neumann'].x)\n",
        "                neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc-self.ibc_cond['Neumann'].y)\n",
        "                    )\n",
        "            else:\n",
        "                neu_loss = 0\n",
        "\n",
        "            #--------------- Compute total loss\n",
        "            pinn_loss = (self.params['pde_weight'] * unsup_loss) +\\\n",
        "            self.params['sup_weight'][0] * init_loss +\\\n",
        "            self.params['sup_weight'][1] * dir_loss +\\\n",
        "            self.params['sup_weight'][2] * neu_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "        #Compute gradient and apply optimizers\n",
        "        gradient = mdl_tape.gradient(pinn_loss,self.mdl.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(gradient,self.mdl.trainable_variables))\n",
        "        #Return losses\n",
        "        final_fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "        final_pinn_loss = unsup_loss + init_loss +\\\n",
        "         dir_loss + neu_loss + fb_dir_loss + fb_neu_loss\n",
        "        return (unsup_loss, init_loss, dir_loss, neu_loss, fb_init_loss,\n",
        "                fb_dir_loss, fb_neu_loss, final_fb_loss, final_pinn_loss)\n",
        "\n",
        "    @tf.function\n",
        "    def train_fb_solo(self):\n",
        "        #--------------- Compute Free Boundary losses\n",
        "        with G_Tape(watch_accessed_variables=False) as fb_tape:\n",
        "            #Watch free boundary weights\n",
        "            fb_tape.watch(self.fb.trainable_variables)\n",
        "            #Compute Initial Free Boundary Condition\n",
        "            fb_init = self.fb(self.ibc_fb_cond['Initial'].x,\n",
        "                                  training=True)\n",
        "            if len(fb_init.shape) > 1:\n",
        "                fb_init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_init -\\\n",
        "                                  tf.expand_dims(self.ibc_fb_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                fb_init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_init -\\\n",
        "                                  self.ibc_fb_cond['Initial'].y )\n",
        "                    )\n",
        "            #Compute Dirichlet Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Dirichlet'] != None:\n",
        "                #Compute Free Boundary values\n",
        "                s_values = self.fb(self.ibc_fb_cond['Dirichlet'].x,\n",
        "                                    training=True)\n",
        "                fb_dc = self.mdl(tf.concat(\n",
        "                    [s_values,\n",
        "                      self.ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                      training=True)\n",
        "                fb_dir_target = tf.nn.relu(\n",
        "                    tf.ones_like(\n",
        "                        s_values\n",
        "                        ) * self.ibc_fb_cond['Dirichlet'].x[:,-1] - s_values\n",
        "                )\n",
        "                fb_dir_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_dc - fb_dir_target)\n",
        "                    )\n",
        "            else:\n",
        "                fb_dir_loss = 0\n",
        "            #Compute Neumann Free Boundary Condition\n",
        "            if self.ibc_fb_cond['Neumann'] != None:\n",
        "                s_values = self.fb(self.ibc_fb_cond['Neumann'].x,\n",
        "                                    training=True)\n",
        "                with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                    neu_fb_tape.watch(s_values)\n",
        "                    pinn_nc_fb = self.mdl(\n",
        "                        tf.concat([s_values,\n",
        "                                    self.ibc_fb_cond['Neumann'].x],\n",
        "                                  axis=1),\n",
        "                                  training=True)\n",
        "                pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                                  s_values)\n",
        "                fb_neu_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_nc_fb-self.ibc_fb_cond['Neumann'].y)\n",
        "                    )\n",
        "            else:\n",
        "                fb_neu_loss = 0\n",
        "            #Compute final loss\n",
        "            fb_loss = self.params['fb_weight'][0] * fb_init_loss +\\\n",
        "            self.params['fb_weight'][1] * fb_dir_loss +\\\n",
        "            self.params['fb_weight'][2] * fb_neu_loss\n",
        "        #Compute gradient and apply optimizers for free boundary\n",
        "        gradient_fb = fb_tape.gradient(fb_loss,\n",
        "                                      self.fb.trainable_variables)\n",
        "        self.fb_opt.apply_gradients( zip(gradient_fb,\n",
        "                                      self.fb.trainable_variables) )\n",
        "        #Return losses\n",
        "        final_fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "        return fb_init_loss, fb_dir_loss, fb_neu_loss, final_fb_loss\n",
        "\n",
        "    def fit(self, wandb_run=None):\n",
        "        #Early warning initialization\n",
        "        self.early_warning = {'Target':np.inf,\n",
        "                              'n_steps':0,\n",
        "                              'top_mdl':None,\n",
        "                              'weights':None}\n",
        "        old_top_mdl, old_weights = None, None\n",
        "        #Training\n",
        "        self.u_losses, self.i_losses = list(), list()\n",
        "        self.d_losses, self.n_losses = list(), list()\n",
        "        self.b_i_losses = list()\n",
        "        self.b_d_losses, self.b_n_losses = list(), list()\n",
        "        self.b_losses, self.p_losses = list(), list()\n",
        "        print_base = \"{:<10}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}\"\n",
        "        if self.params['fb_freezing'] == None:\n",
        "            for epoch in tqdm(range(self.params['epochs']),\n",
        "                              desc='PINNs - Training'):\n",
        "                if epoch == 0:\n",
        "                    print(print_base.format('Epoch', 'Unsupervised', 'Initial',\n",
        "                                            'Dirichlet', 'Neumann',\n",
        "                                            'FB_Init', 'FB_Dir', 'FB_Neu',\n",
        "                                            'Free Boundary', 'Total'))\n",
        "                    print('\\n')\n",
        "                #Case 1: more mdl steps for a single fb step\n",
        "                for _ in range(self.params['steps_fb_per_pde'] -1):\n",
        "                    self.train_mdl_solo();\n",
        "                #Case 2: more fb steps for a single mdl step\n",
        "                for _ in range(0, self.params['steps_fb_per_pde'] +1, -1):\n",
        "                    self.train_fb_solo();\n",
        "                u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l =\\\n",
        "                self.train_step()\n",
        "                u_l, i_l = np.array(u_l), np.array(i_l)\n",
        "                d_l, n_l = np.array(d_l), np.array(n_l)\n",
        "                b_i_l = np.array(b_i_l)\n",
        "                b_d_l, b_n_l = np.array(b_d_l), np.array(b_n_l)\n",
        "                b_l, p_l = np.array(b_l), np.array(p_l)\n",
        "                if epoch % self.params['verbose'] == 0:\n",
        "                    print(print_base.format(epoch+1, format(u_l, '.20f')[:10],\n",
        "                                            format(i_l, '.20f')[:10],\n",
        "                                            format(d_l, '.20f')[:10],\n",
        "                                            format(n_l, '.20f')[:10],\n",
        "                                            format(b_i_l, '.20f')[:10],\n",
        "                                            format(b_d_l, '.20f')[:10],\n",
        "                                            format(b_n_l, '.20f')[:10],\n",
        "                                            format(b_l, '.20f')[:10],\n",
        "                                            format(p_l, '.20f')[:10]))\n",
        "                self.u_losses.append(u_l)\n",
        "                self.i_losses.append(i_l)\n",
        "                self.d_losses.append(d_l)\n",
        "                self.n_losses.append(n_l)\n",
        "                self.b_i_losses.append(b_i_l)\n",
        "                self.b_d_losses.append(b_d_l)\n",
        "                self.b_n_losses.append(b_d_l)\n",
        "                self.b_losses.append(b_l)\n",
        "                self.p_losses.append(p_l)\n",
        "                #Eventually, update wandb\n",
        "                if wandb_run != None:\n",
        "                    wandb_run.log({'Unsupervised':u_l,\n",
        "                                  'Supervised':i_l + d_l + n_l,\n",
        "                                  'Total':p_l,\n",
        "                                   'Free Boundary':b_l,\n",
        "                                   'Initial':i_l,\n",
        "                                   'Dirichlet':d_l,\n",
        "                                   'Neumann':n_l,\n",
        "                                   'FB_Init':b_i_l,\n",
        "                                   'FB_Dir':b_d_l,\n",
        "                                   'FB_Neu':b_n_l})\n",
        "                #Check for the early warning\n",
        "                if p_l <= self.early_warning['Target']:\n",
        "                    self.early_warning['Target'] = p_l\n",
        "                    self.early_warning['n_steps'] = 0\n",
        "                    self.early_warning['top_mdl'] = old_top_mdl\n",
        "                    self.early_warning['weights'] = old_weights\n",
        "                else:\n",
        "                    self.early_warning['n_steps'] += 1\n",
        "                    if self.early_warning['n_steps'] >= self.params['patience']:\n",
        "                        break\n",
        "                #Save model and weights for next step early warning\n",
        "                old_top_mdl = tf.keras.models.clone_model( self.mdl )\n",
        "                old_weights = self.mdl.get_weights()\n",
        "        else:\n",
        "            #Before freezing, both mdl and fb are training\n",
        "            for epoch in tqdm(range(self.params['fb_freezing']), desc='PINNs - Training'):\n",
        "                if epoch == 0:\n",
        "                    print(print_base.format('Epoch', 'Unsupervised', 'Initial',\n",
        "                                            'Dirichlet', 'Neumann',\n",
        "                                            'FB_Init', 'FB_Dir', 'FB_Neu',\n",
        "                                            'Free Boundary', 'Total'))\n",
        "                    print('\\n')\n",
        "                #Case 1: more mdl steps for a single fb step\n",
        "                for _ in range(self.params['steps_fb_per_pde'] -1):\n",
        "                    self.train_mdl_solo();\n",
        "                #Case 2: more fb steps for a single mdl step\n",
        "                for _ in range(0, self.params['steps_fb_per_pde'] +1, -1):\n",
        "                    self.train_fb_solo();\n",
        "                u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l =\\\n",
        "                self.train_step()\n",
        "                u_l, i_l = np.array(u_l), np.array(i_l)\n",
        "                d_l, n_l = np.array(d_l), np.array(n_l)\n",
        "                b_i_l = np.array(b_i_l)\n",
        "                b_d_l, b_n_l = np.array(b_d_l), np.array(b_n_l)\n",
        "                b_l, p_l = np.array(b_l), np.array(p_l)\n",
        "                if epoch % self.params['verbose'] == 0:\n",
        "                    print(print_base.format(epoch+1, format(u_l, '.20f')[:10],\n",
        "                                            format(i_l, '.20f')[:10],\n",
        "                                            format(d_l, '.20f')[:10],\n",
        "                                            format(n_l, '.20f')[:10],\n",
        "                                            format(b_i_l, '.20f')[:10],\n",
        "                                            format(b_d_l, '.20f')[:10],\n",
        "                                            format(b_n_l, '.20f')[:10],\n",
        "                                            format(b_l, '.20f')[:10],\n",
        "                                            format(p_l, '.20f')[:10]))\n",
        "                self.u_losses.append(u_l)\n",
        "                self.i_losses.append(i_l)\n",
        "                self.d_losses.append(d_l)\n",
        "                self.n_losses.append(n_l)\n",
        "                self.b_i_losses.append(b_i_l)\n",
        "                self.b_d_losses.append(b_d_l)\n",
        "                self.b_n_losses.append(b_d_l)\n",
        "                self.b_losses.append(b_l)\n",
        "                self.p_losses.append(p_l)\n",
        "                #Eventually, update wandb\n",
        "                if wandb_run != None:\n",
        "                    wandb_run.log({'Unsupervised':u_l,\n",
        "                                  'Supervised':i_l + d_l + n_l,\n",
        "                                  'Total':p_l,\n",
        "                                   'Free Boundary':b_l,\n",
        "                                   'Initial':i_l,\n",
        "                                   'Dirichlet':d_l,\n",
        "                                   'Neumann':n_l,\n",
        "                                   'FB_Init':b_i_l,\n",
        "                                   'FB_Dir':b_d_l,\n",
        "                                   'FB_Neu':b_n_l})\n",
        "                #Check for the early warning\n",
        "                if p_l <= self.early_warning['Target']:\n",
        "                    self.early_warning['Target'] = p_l\n",
        "                    self.early_warning['n_steps'] = 0\n",
        "                    self.early_warning['top_mdl'] = old_top_mdl\n",
        "                    self.early_warning['weights'] = old_weights\n",
        "                else:\n",
        "                    self.early_warning['n_steps'] += 1\n",
        "                    if self.early_warning['n_steps'] >= self.params['patience']:\n",
        "                        break\n",
        "                #Save model and weights for next step early warning\n",
        "                old_top_mdl = tf.keras.models.clone_model( self.mdl )\n",
        "                old_weights = self.mdl.get_weights()\n",
        "            #Now, freeze fb and train only mdl\n",
        "            for epoch in tqdm(range(self.params['fb_freezing'],\n",
        "                                    self.params['epochs']),\n",
        "                              desc='PINNs - Training; Free Boundary Fixed'):\n",
        "                u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l =\\\n",
        "                self.train_mdl_solo()\n",
        "                u_l, i_l = np.array(u_l), np.array(i_l)\n",
        "                d_l, n_l = np.array(d_l), np.array(n_l)\n",
        "                b_i_l = np.array(b_i_l)\n",
        "                b_d_l, b_n_l = np.array(b_d_l), np.array(b_n_l)\n",
        "                b_l, p_l = np.array(b_l), np.array(p_l)\n",
        "                if epoch % self.params['verbose'] == 0:\n",
        "                    print(print_base.format(epoch+1, format(u_l, '.20f')[:10],\n",
        "                                            format(i_l, '.20f')[:10],\n",
        "                                            format(d_l, '.20f')[:10],\n",
        "                                            format(n_l, '.20f')[:10],\n",
        "                                            format(b_i_l, '.20f')[:10],\n",
        "                                            format(b_d_l, '.20f')[:10],\n",
        "                                            format(b_n_l, '.20f')[:10],\n",
        "                                            format(b_l, '.20f')[:10],\n",
        "                                            format(p_l, '.20f')[:10]))\n",
        "                self.u_losses.append(u_l)\n",
        "                self.i_losses.append(i_l)\n",
        "                self.d_losses.append(d_l)\n",
        "                self.n_losses.append(n_l)\n",
        "                self.b_i_losses.append(b_i_l)\n",
        "                self.b_d_losses.append(b_d_l)\n",
        "                self.b_n_losses.append(b_d_l)\n",
        "                self.b_losses.append(b_l)\n",
        "                self.p_losses.append(p_l)\n",
        "                #Eventually, update wandb\n",
        "                if wandb_run != None:\n",
        "                    wandb_run.log({'Unsupervised':u_l,\n",
        "                                  'Supervised':i_l + d_l + n_l,\n",
        "                                  'Total':p_l,\n",
        "                                   'Free Boundary':b_l,\n",
        "                                   'Initial':i_l,\n",
        "                                   'Dirichlet':d_l,\n",
        "                                   'Neumann':n_l,\n",
        "                                   'FB_Init':b_i_l,\n",
        "                                   'FB_Dir':b_d_l,\n",
        "                                   'FB_Neu':b_n_l})\n",
        "                #Check for the early warning\n",
        "                if p_l <= self.early_warning['Target']:\n",
        "                    self.early_warning['Target'] = p_l\n",
        "                    self.early_warning['n_steps'] = 0\n",
        "                    self.early_warning['top_mdl'] = old_top_mdl\n",
        "                    self.early_warning['weights'] = old_weights\n",
        "                else:\n",
        "                    self.early_warning['n_steps'] += 1\n",
        "                    if self.early_warning['n_steps'] >= self.params['patience']:\n",
        "                        break\n",
        "                #Save model and weights for next step early warning\n",
        "                old_top_mdl = tf.keras.models.clone_model( self.mdl )\n",
        "                old_weights = self.mdl.get_weights()\n",
        "        #Recover information about optimal epoch in early warning\n",
        "        self.mdl = tf.keras.models.clone_model( self.early_warning['top_mdl'] )\n",
        "        self.mdl.set_weights(self.early_warning['weights'])\n",
        "        top_epoch = epoch+1 - self.early_warning[\"n_steps\"]\n",
        "        print(f'Best loss achieved at step {top_epoch}')\n",
        "\n",
        "    def plot_losses(self):\n",
        "        plt.figure( figsize=(12,8) )\n",
        "        plt.semilogy(self.u_losses, label='Unsupervised')\n",
        "        plt.semilogy(np.array(self.i_losses) +\\\n",
        "                     np.array(self.d_losses) +\\\n",
        "                     np.array(self.n_losses),\n",
        "                     label='Supervised')\n",
        "        plt.semilogy(self.b_losses, label='Free Boundary')\n",
        "        plt.semilogy(self.p_losses, label='PINN')\n",
        "        plt.legend()\n",
        "        plt.title('Losses')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_unsupervised_test(self, test_sampler, test_ibc_cond,\n",
        "                               test_ibc_fb_cond, to_print=True, output=False):\n",
        "        #--------------- Compute Free Boundary losses\n",
        "        #Compute Initial Free Boundary Condition\n",
        "        if test_ibc_fb_cond['Initial'] != None:\n",
        "            fb_init = self.fb(test_ibc_fb_cond['Initial'].x,\n",
        "                                  training=False)\n",
        "            if len(fb_init.shape) > 1:\n",
        "                fb_init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_init -\\\n",
        "                                  tf.expand_dims(test_ibc_fb_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                fb_init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(fb_init -\\\n",
        "                                  test_ibc_fb_cond['Initial'].y )\n",
        "                    )\n",
        "        else:\n",
        "            fb_init_loss = 0\n",
        "        #Compute Dirichlet Free Boundary Condition\n",
        "        if test_ibc_fb_cond['Dirichlet'] != None:\n",
        "            #Compute Free Boundary values\n",
        "            s_values = self.fb(test_ibc_fb_cond['Dirichlet'].x,\n",
        "                                training=True)\n",
        "            fb_dc = self.mdl(tf.concat(\n",
        "                [s_values,\n",
        "                  test_ibc_fb_cond['Dirichlet'].x], axis=1),\n",
        "                  training=True)\n",
        "            fb_dir_target = tf.nn.relu(\n",
        "                tf.ones_like(\n",
        "                    s_values\n",
        "                    ) * test_ibc_fb_cond['Dirichlet'].x[:,-1] - s_values\n",
        "            )\n",
        "            fb_dir_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(fb_dc - fb_dir_target)\n",
        "                )\n",
        "        else:\n",
        "            fb_dir_loss = 0\n",
        "        #Compute Neumann Free Boundary Condition\n",
        "        if test_ibc_fb_cond['Neumann'] != None:\n",
        "            s_values = self.fb(test_ibc_fb_cond['Neumann'].x,\n",
        "                                training=False)\n",
        "            with G_Tape(watch_accessed_variables=False) as neu_fb_tape:\n",
        "                neu_fb_tape.watch(s_values)\n",
        "                pinn_nc_fb = self.mdl(\n",
        "                    tf.concat([s_values,\n",
        "                                test_ibc_fb_cond['Neumann'].x],\n",
        "                              axis=1),\n",
        "                              training=True)\n",
        "            pinn_nc_fb = neu_fb_tape.gradient(pinn_nc_fb,\n",
        "                                              s_values)\n",
        "            fb_neu_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(pinn_nc_fb-test_ibc_fb_cond['Neumann'].y)\n",
        "                )\n",
        "        else:\n",
        "            fb_neu_loss = 0\n",
        "        #Compute final loss\n",
        "        fb_loss = fb_init_loss + fb_dir_loss + fb_neu_loss\n",
        "\n",
        "        #--------------- Compute PINN losses\n",
        "        #Compute unsupervised loss\n",
        "        s_values = self.fb(tf.concat([test_sampler.t,\n",
        "                                      test_sampler.m_p], axis=-1),\n",
        "                            training=False)\n",
        "        x_f = tf.reshape(test_sampler.x[ test_sampler.x > s_values ],\n",
        "                          (-1,1) )\n",
        "        t_f = tf.reshape(test_sampler.t[ test_sampler.x > s_values ],\n",
        "                          (-1,1) )\n",
        "        m_p = test_sampler.m_p[ test_sampler.x[:, 0] > s_values[:, 0] ]\n",
        "        with G_Tape(persistent=True,\n",
        "                    watch_accessed_variables=False) as pinn_tape:\n",
        "            #Watch independet variables\n",
        "            pinn_tape.watch(x_f)\n",
        "            pinn_tape.watch(t_f)\n",
        "            #Apply u function for unsupervised\n",
        "            u_val = self.mdl(tf.concat([x_f, t_f, m_p],\n",
        "                                      axis=1),\n",
        "                            training=True)\n",
        "            u_x = pinn_tape.gradient(u_val, x_f)\n",
        "        u_xx = pinn_tape.gradient(u_x, x_f)\n",
        "        u_t = pinn_tape.gradient(u_val, t_f)\n",
        "        del(pinn_tape)\n",
        "        u_val = tf.cast(u_val, self.DTYPE)\n",
        "        f = (m_p[:,0:1] * x_f * u_x) + u_t +\\\n",
        "        (m_p[:,1:2] * x_f**2 * u_xx)/2 -\\\n",
        "        (m_p[:,0:1] * u_val)\n",
        "        unsup_loss = tf.reduce_mean(tf.square( f ))\n",
        "        #Compute Initial Condition\n",
        "        if test_ibc_cond['Initial'] != None:\n",
        "            pinn_init = self.mdl(test_ibc_cond['Initial'].x,\n",
        "                                  training=False)\n",
        "            if len(pinn_init.shape) > 1:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  tf.expand_dims(test_ibc_cond['Initial'].y,\n",
        "                                                 axis=-1 ) )\n",
        "                    )\n",
        "            else:\n",
        "                init_loss = tf.math.reduce_mean(\n",
        "                    tf.math.square(pinn_init -\\\n",
        "                                  test_ibc_cond['Initial'].y )\n",
        "                    )\n",
        "        else:\n",
        "            init_loss = 0\n",
        "        #Compute Dirichlet Boundary Condition\n",
        "        if test_ibc_cond['Dirichlet'] != None:\n",
        "            pinn_dc = self.mdl(test_ibc_cond['Dirichlet'].x,\n",
        "                                training=False)\n",
        "            dir_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(pinn_dc-test_ibc_cond['Dirichlet'].y)\n",
        "                )\n",
        "        else:\n",
        "            dir_loss = 0\n",
        "        #Compute Neumann Boundary Condition\n",
        "        if test_ibc_cond['Neumann'] != None:\n",
        "            with G_Tape(watch_accessed_variables=False) as neu_tape:\n",
        "                neu_tape.watch(test_ibc_cond['Neumann'].x)\n",
        "                pinn_nc = self.mdl(tf.concat([test_ibc_cond['Neumann'].x,\n",
        "                                              test_ibc_cond['Neumann'].t,\n",
        "                                              test_ibc_cond['Neumann'].m_p],\n",
        "                                            axis=1),\n",
        "                                    training=False)\n",
        "            pinn_nc = neu_tape.gradient(pinn_nc, test_ibc_cond['Neumann'].x)\n",
        "            neu_loss = tf.math.reduce_mean(\n",
        "                tf.math.square(pinn_nc-test_ibc_cond['Neumann'].y)\n",
        "                )\n",
        "        else:\n",
        "            neu_loss = 0\n",
        "\n",
        "        #--------------- Compute total loss\n",
        "        pinn_loss = unsup_loss + init_loss + dir_loss +\\\n",
        "        neu_loss + fb_dir_loss + fb_neu_loss\n",
        "\n",
        "        u_l, i_l = np.array(unsup_loss), np.array(init_loss)\n",
        "        d_l, n_l = np.array(dir_loss), np.array(neu_loss)\n",
        "        b_i_l = np.array(fb_init_loss)\n",
        "        b_d_l, b_n_l = np.array(fb_dir_loss), np.array(fb_neu_loss)\n",
        "        b_l, p_l = np.array(fb_loss), np.array(pinn_loss)\n",
        "\n",
        "        if to_print:\n",
        "            print_base = \"{:<10}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}\"\n",
        "            print(print_base.format('', 'Unsupervised', 'Initial',\n",
        "                                    'Dirichlet', 'Neumann', 'FB_Init', 'FB_Dir',\n",
        "                                    'FB_Neu', 'Free Boundary', 'Total'))\n",
        "            print(print_base.format('', format(u_l, '.20f')[:10],\n",
        "                                    format(i_l, '.20f')[:10],\n",
        "                                    format(d_l, '.20f')[:10],\n",
        "                                    format(n_l, '.20f')[:10],\n",
        "                                    format(b_i_l, '.20f')[:10],\n",
        "                                    format(b_d_l, '.20f')[:10],\n",
        "                                    format(b_n_l, '.20f')[:10],\n",
        "                                    format(b_l, '.20f')[:10],\n",
        "                                    format(p_l, '.20f')[:10]))\n",
        "        if output:\n",
        "            return u_l, i_l, d_l, n_l, b_i_l, b_d_l, b_n_l, b_l, p_l\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blNdgkEKYRIv"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1ZKZquOuUpx3"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay as ex_d\n",
        "my_lr = ex_d(1e-2, 320, 0.975, staircase=False)\n",
        "fb_lr = ex_d(1e-2, 80, 0.975, staircase=False)\n",
        "\n",
        "params = {'sample_method':'sobol',\n",
        "          'layers':[n_dim+5, 20, 20, 20, 20, 20, 20, 20, 20, 1],\n",
        "          'activation':'tanh',\n",
        "          'output_act':'linear', 'initializer':'glorot_normal',\n",
        "          'fb_layers':[5, 100, 100, 100, n_dim], 'fb_activation':'tanh',\n",
        "          'fb_output_act':'linear', 'fb_initializer':'glorot_normal',\n",
        "          'lr':my_lr, 'optimizer':'rmsprop',\n",
        "          'fb_lr':fb_lr, 'fb_optimizer':'rmsprop', 'steps_fb_per_pde':4,\n",
        "          'pde_weight':1, 'epochs':15000, 'verbose':100}\n",
        "\n",
        "my_pinn = FreeBoundary_PINN(params, pinn_conditions,\n",
        "                            fb_conditions, lb, ub, N_f=100000, DTYPE=DTYPE)\n",
        "\n",
        "START = time.time()\n",
        "my_pinn.fit()\n",
        "train_time = time.time() - START\n",
        "my_pinn.plot_losses()\n",
        "\n",
        "START = time.time()\n",
        "values = my_pinn.plot_unsupervised_test(test_sampler, pinn_cond_test,\n",
        "                                        fb_cond_test, output=True)\n",
        "test_time = time.time() - START\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5ftpBs1PXXrn"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
